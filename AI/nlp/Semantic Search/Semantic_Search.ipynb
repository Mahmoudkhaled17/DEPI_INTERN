{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c6dddc7676314387aa52aa7cd5025358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_104d0fa848b84711bf120318a61683e0",
              "IPY_MODEL_15246c3d280443c78f2c36be6b2a1f28",
              "IPY_MODEL_c8a9f9d1bb074efcab99914591ab80c4"
            ],
            "layout": "IPY_MODEL_2fa7ee4ed1ca4a4ba500f9a6c9301423"
          }
        },
        "104d0fa848b84711bf120318a61683e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2265caa74d4db88932f69ed3699268",
            "placeholder": "​",
            "style": "IPY_MODEL_055fc54c2c674deb859f7373407d6e82",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "15246c3d280443c78f2c36be6b2a1f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a9e04879c374cb888ffd57efe5f8c56",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15682f76630942c7abd2d28b08b20ece",
            "value": 48
          }
        },
        "c8a9f9d1bb074efcab99914591ab80c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56bce7aa62da4562a67d3816bac80e2e",
            "placeholder": "​",
            "style": "IPY_MODEL_b17c8075539c49e4b57790caa9b6aee5",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.12kB/s]"
          }
        },
        "2fa7ee4ed1ca4a4ba500f9a6c9301423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c2265caa74d4db88932f69ed3699268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "055fc54c2c674deb859f7373407d6e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a9e04879c374cb888ffd57efe5f8c56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15682f76630942c7abd2d28b08b20ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56bce7aa62da4562a67d3816bac80e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17c8075539c49e4b57790caa9b6aee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7de8a5d6229e45f38d6da27396e700db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b65cefeb4db43399f96341d06591137",
              "IPY_MODEL_73882d55571946b296b1f5641d70331a",
              "IPY_MODEL_88dbff06d73d4e108804aee6fcaa2366"
            ],
            "layout": "IPY_MODEL_e78936dd25954d59aedf419e3092f39d"
          }
        },
        "8b65cefeb4db43399f96341d06591137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5afe2623dc3452fba3c424c220e77c0",
            "placeholder": "​",
            "style": "IPY_MODEL_736beb7ffb2b4914a9ed3f9bf5c939e7",
            "value": "config.json: 100%"
          }
        },
        "73882d55571946b296b1f5641d70331a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5efe96e7dcca47a18eb22495521eb1ad",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b879793a2d94d628a5027e5b10cd82c",
            "value": 570
          }
        },
        "88dbff06d73d4e108804aee6fcaa2366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54bc5bf458334b888f587e67d4f9773d",
            "placeholder": "​",
            "style": "IPY_MODEL_2d0a65a50e1b4e5a9898e71fc63d1b75",
            "value": " 570/570 [00:00&lt;00:00, 8.74kB/s]"
          }
        },
        "e78936dd25954d59aedf419e3092f39d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5afe2623dc3452fba3c424c220e77c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "736beb7ffb2b4914a9ed3f9bf5c939e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5efe96e7dcca47a18eb22495521eb1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b879793a2d94d628a5027e5b10cd82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54bc5bf458334b888f587e67d4f9773d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0a65a50e1b4e5a9898e71fc63d1b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12d4a730e826493b96285a859b361529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a65cdc9641034a04b807e48276655918",
              "IPY_MODEL_ef252531af0640f3bf277fecc52f430d",
              "IPY_MODEL_5c338a1fd90c411a9a1513958948b61f"
            ],
            "layout": "IPY_MODEL_3da0b639df384e66ac4b1c52fd48eee8"
          }
        },
        "a65cdc9641034a04b807e48276655918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47bc132f360e4e37b4af1e3c28ec3089",
            "placeholder": "​",
            "style": "IPY_MODEL_1ce40e0749034d1db9826536eb84a8bd",
            "value": "vocab.txt: 100%"
          }
        },
        "ef252531af0640f3bf277fecc52f430d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5993b5ebcf440879c50d517ef7f8539",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce1fa5ebd4644966af6f45a5607337f3",
            "value": 231508
          }
        },
        "5c338a1fd90c411a9a1513958948b61f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0fdd0d961ed460f827e597646e85fb7",
            "placeholder": "​",
            "style": "IPY_MODEL_1571014f9cf740f78fea960dd0ffaa9e",
            "value": " 232k/232k [00:00&lt;00:00, 1.81MB/s]"
          }
        },
        "3da0b639df384e66ac4b1c52fd48eee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47bc132f360e4e37b4af1e3c28ec3089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce40e0749034d1db9826536eb84a8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5993b5ebcf440879c50d517ef7f8539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1fa5ebd4644966af6f45a5607337f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0fdd0d961ed460f827e597646e85fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1571014f9cf740f78fea960dd0ffaa9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b895216a560e430797cb020f3c2f40d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27ade560f8a145f58b608756ec35794b",
              "IPY_MODEL_cdf6adf5d9214d6ea71d0ebc47729178",
              "IPY_MODEL_dd3ed42d9f2a49a294172aa22457d8f8"
            ],
            "layout": "IPY_MODEL_4f28930e930847a282496af9dcc09e19"
          }
        },
        "27ade560f8a145f58b608756ec35794b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59706b5e7cb340b2aaa0c97ddd94159b",
            "placeholder": "​",
            "style": "IPY_MODEL_438df17a1d824292a9b80f831293b2c8",
            "value": "tokenizer.json: 100%"
          }
        },
        "cdf6adf5d9214d6ea71d0ebc47729178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fba7865e88546dd94e5a2900eb0c2fa",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26ddf46a0b0d40f98e306ed9e4c0f2aa",
            "value": 466062
          }
        },
        "dd3ed42d9f2a49a294172aa22457d8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26d215612c84818aab14bffb5c3ca2a",
            "placeholder": "​",
            "style": "IPY_MODEL_cd7fe51a4ccb4b84bb66bc49e10221ce",
            "value": " 466k/466k [00:00&lt;00:00, 2.51MB/s]"
          }
        },
        "4f28930e930847a282496af9dcc09e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59706b5e7cb340b2aaa0c97ddd94159b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438df17a1d824292a9b80f831293b2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fba7865e88546dd94e5a2900eb0c2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ddf46a0b0d40f98e306ed9e4c0f2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f26d215612c84818aab14bffb5c3ca2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd7fe51a4ccb4b84bb66bc49e10221ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3198d01d8e44d27b5f5061a826c23e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc3cff2301ed44328dd9bfe99b9ba739",
              "IPY_MODEL_79b4b3c27e834f4f98f503f5ecf935aa",
              "IPY_MODEL_f15d9ce711e24b2b9b02d0545359115b"
            ],
            "layout": "IPY_MODEL_7376aec80d494915938ade3b70560f9c"
          }
        },
        "bc3cff2301ed44328dd9bfe99b9ba739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a563e45aff54c81bcd9d1bc2902ee60",
            "placeholder": "​",
            "style": "IPY_MODEL_03743e5c3d2f49c0a88b96171cc5e39f",
            "value": "model.safetensors: 100%"
          }
        },
        "79b4b3c27e834f4f98f503f5ecf935aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da625a7abd6493687794883541ba5fa",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe11454a24a74fa4b5bedb7d9b3c7bd8",
            "value": 440449768
          }
        },
        "f15d9ce711e24b2b9b02d0545359115b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93acdac3337d4064872e259646e959cc",
            "placeholder": "​",
            "style": "IPY_MODEL_0fb0800995644cd69011f3a4fc46919c",
            "value": " 440M/440M [00:03&lt;00:00, 111MB/s]"
          }
        },
        "7376aec80d494915938ade3b70560f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a563e45aff54c81bcd9d1bc2902ee60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03743e5c3d2f49c0a88b96171cc5e39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0da625a7abd6493687794883541ba5fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe11454a24a74fa4b5bedb7d9b3c7bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93acdac3337d4064872e259646e959cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fb0800995644cd69011f3a4fc46919c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "!pip install PyPDF2\n",
        "\n",
        "\n",
        "def read_pdf(file_path):\n",
        "  with open(file_path, 'rb') as fileobj:\n",
        "    pdfreader = PyPDF2.PdfReader(fileobj)\n",
        "    text = ''\n",
        "    for page in pdfreader.pages:\n",
        "      page_text = page.extract_text()\n",
        "      # Remove non-text characters using regular expressions\n",
        "      cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', page_text)\n",
        "      text += cleaned_text\n",
        "  return text\n",
        "\n",
        "\n",
        "file_path = '/content/Aurelien_Geron_Hands_On_Machine_Learning_with_Scikit_Learn_Keras.pdf'\n",
        "text = read_pdf(file_path)\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enNJrU0EmIBT",
        "outputId": "15e93b67-565d-41e6-978e-af1d28e66671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Layers and patterns try training the default neural network by clicking the run\n",
            "button top left Notice how it quickly finds a good solution for the classifica\n",
            "tion task Notice that the neurons in the first hidden layer have learned simple\n",
            "patterns while the neurons in the second hidden layer have learned to com\n",
            "bine the simple patterns of the first hidden layer into more complex patterns\n",
            "In general the more layers the more complex the patterns can be\n",
            "Activation function try replacing the Tanh activation function with the ReLU\n",
            "activation function and train the network again Notice that it finds a solution\n",
            "even faster but this time the boundaries are linear This is due to the shape of\n",
            "the ReLU function\n",
            "Local minima modify the network architecture to have just one hidden layer\n",
            "with three neurons Train it multiple times to reset the network weights click\n",
            "the reset button next to the play button Notice that the training time varies a\n",
            "lot and sometimes it even gets stuck in a local minimum\n",
            "Too small now remove one neuron to keep just 2 Notice that the neural net\n",
            "work is now incapable of finding a good solution even if you try multiple\n",
            "times The model has too few parameters and it systematically underfits the\n",
            "training set\n",
            "Large enough next set the number of neurons to 8 and train the network sev\n",
            "eral times Notice that it is now consistently fast and never gets stuck This\n",
            "highlights an important finding in neural network theory large neural net\n",
            "works almost never get stuck in local minima and even when they do these\n",
            "local optima are almost as good as the global optimum However they can still\n",
            "get stuck on long plateaus for a long time\n",
            "Deep net and vanishing gradients now change the dataset to be the spiral bot\n",
            "tom right dataset under DATA  Change the network architecture to have 4\n",
            "hidden layers with 8 neurons each Notice that training takes much longer and\n",
            "often gets stuck on plateaus for long periods of time Also notice that the neu\n",
            "rons in the highest layers ie on the right tend to evolve faster than the neu\n",
            "rons in the lowest layers ie on the left This problem called the vanishing\n",
            "gradients problem can be alleviated using better weight initialization and\n",
            "322  Chapter 10 Introduction to Artificial  Neural Networks with Kerasother techniques better optimizers such as AdaGrad or Adam or using\n",
            "Batch Normalization\n",
            "More go ahead and play with the other parameters to get a feel of what they\n",
            "do In fact you should definitely play with this UI for at least one hour it will\n",
            "grow your intuitions about neural networks significantly\n",
            "2Draw an ANN using the original artificial neurons like the ones in Figure 103 \n",
            "that computes A  B where  represents the XOR operation Hint A  B  A\n",
            "  B   A  B\n",
            "3Why is it generally preferable to use a Logistic Regression classifier rather than a\n",
            "classical Perceptron ie a single layer of threshold logic units trained using the\n",
            "Perceptron training algorithm How can you tweak a Perceptron to make it\n",
            "equivalent to a Logistic Regression classifier\n",
            "4Why was the logistic activation function a key ingredient in training the first\n",
            "MLPs\n",
            "5Name three popular activation functions Can you draw them\n",
            "6Suppose you have an MLP composed of one input layer with 10 passthrough\n",
            "neurons followed by one hidden layer with 50 artificial neurons and finally one\n",
            "output layer with 3 artificial neurons All artificial neurons use the ReLU activa\n",
            "tion function\n",
            "What is the shape of the input matrix X\n",
            "What about the shape of the hidden layers weight vector Wh and the shape of\n",
            "its bias vector bh\n",
            "What is the shape of the output layers weight vector Wo and its bias vector bo\n",
            "What is the shape of the networks output matrix Y\n",
            "Write the equation that computes the networks output matrix Y as a function\n",
            "of X Wh bh Wo and bo\n",
            "7How many neurons do you need in the output layer if you want to classify email\n",
            "into spam or ham What activation function should you use in the output layer\n",
            "If instead you want to tackle MNIST how many neurons do you need in the out\n",
            "put layer using what activation function Answer the same questions for getting\n",
            "your network to predict housing prices as in Chapter 2 \n",
            "8What is backpropagation and how does it work What is the difference between\n",
            "backpropagation and reversemode autodiff\n",
            "9Can you list all the hyperparameters you can tweak in an MLP If the MLP over\n",
            "fits the training data how could you tweak these hyperparameters to try to solve\n",
            "the problem\n",
            "Exercises  32310Train a deep MLP on the MNIST dataset and see if you can get over 98 preci\n",
            "sion Try adding all the bells and whistles ie save checkpoints use early stop\n",
            "ping plot learning curves using TensorBoard and so on\n",
            "Solutions to these exercises are available in \n",
            "324  Chapter 10 Introduction to Artificial  Neural Networks with KerasCHAPTER 11\n",
            "Training Deep Neural Networks\n",
            "With Early Release ebooks you get books in their earliest form\n",
            "the authors raw and unedited content as he or she writesso you\n",
            "can take advantage of these technologies long before the official\n",
            "release of these titles The following will be Chapter 11 in the final\n",
            "release of the book\n",
            "In Chapter 10  we introduced artificial neural networks and trained our first deep\n",
            "neural networks But they were very shallow nets with just a few hidden layers What\n",
            "if you need to tackle a very complex problem such as detecting hundreds of types of\n",
            "objects in highresolution images Y ou may need to train a much deeper DNN per\n",
            "haps with 10 layers or much more each containing hundreds of neurons connected\n",
            "by hundreds of thousands of connections This would not be a walk in the park\n",
            "First you would be faced with the tricky vanishing gradients  problem or the\n",
            "related exploding gradients  problem that affects deep neural networks and makes\n",
            "lower layers very hard to train\n",
            "Second you might not have enough training data for such a large network or it\n",
            "might be too costly to label\n",
            "Third training may be extremely slow\n",
            "Fourth a model with millions of parameters would severely risk overfitting the\n",
            "training set especially if there are not enough training instances or they are too\n",
            "noisy\n",
            "In this chapter we will go through each of these problems in turn and present techni\n",
            "ques to solve them We will start by explaining the vanishing gradients problem and\n",
            "exploring some of the most popular solutions to this problem Next we will look at\n",
            "transfer learning and unsupervised pretraining which can help you tackle complex\n",
            "3251Understanding the Difficulty of Training Deep Feedforward Neural Networks  X Glorot Y Bengio 2010tasks even when you have little labeled data Then we will discuss various optimizers\n",
            "that can speed up training large models tremendously compared to plain Gradient\n",
            "Descent Finally we will go through a few popular regularization techniques for large\n",
            "neural networks\n",
            "With these tools you will be able to train very deep nets welcome to Deep Learning\n",
            "VanishingExploding Gradients Problems\n",
            "As we discussed in Chapter 10  the backpropagation algorithm works by going from\n",
            "the output layer to the input layer propagating the error gradient on the way Once\n",
            "the algorithm has computed the gradient of the cost function with regards to each\n",
            "parameter in the network it uses these gradients to update each parameter with a\n",
            "Gradient Descent step\n",
            "Unfortunately gradients often get smaller and smaller as the algorithm progresses\n",
            "down to the lower layers As a result the Gradient Descent update leaves the lower\n",
            "layer connection weights virtually unchanged and training never converges to a good\n",
            "solution This is called the vanishing gradients  problem In some cases the opposite\n",
            "can happen the gradients can grow bigger and bigger so many layers get insanely\n",
            "large weight updates and the algorithm diverges This is the exploding gradients  prob\n",
            "lem which is mostly encountered in recurrent neural networks see  More gener\n",
            "ally deep neural networks suffer from unstable gradients different layers may learn at\n",
            "widely different speeds\n",
            "Although this unfortunate behavior has been empirically observed for quite a while\n",
            "it was one of the reasons why deep neural networks were mostly abandoned for a\n",
            "long time it is only around 2010 that significant progress was made in understand\n",
            "ing it A paper titled Understanding the Difficulty of Training Deep Feedforward\n",
            "Neural Networks  by Xavier Glorot and Y oshua Bengio1 found a few suspects includ\n",
            "ing the combination of the popular logistic sigmoid activation function and the\n",
            "weight initialization technique that was most popular at the time namely random ini\n",
            "tialization using a normal distribution with a mean of 0 and a standard deviation of 1\n",
            "In short they showed that with this activation function and this initialization scheme\n",
            "the variance of the outputs of each layer is much greater than the variance of its\n",
            "inputs Going forward in the network the variance keeps increasing after each layer\n",
            "until the activation function saturates at the top layers This is actually made worse by\n",
            "the fact that the logistic function has a mean of 05 not 0 the hyperbolic tangent\n",
            "function has a mean of 0 and behaves slightly better than the logistic function in deep\n",
            "networks\n",
            "326  Chapter 11 Training Deep Neural Networks2Heres an analogy if you set a microphone amplifiers knob too close to zero people wont hear your voice but\n",
            "if you set it too close to the max your voice will be saturated and people wont understand what you are say\n",
            "ing Now imagine a chain of such amplifiers they all need to be set properly in order for your voice to come\n",
            "out loud and clear at the end of the chain Y our voice has to come out of each amplifier at the same amplitude\n",
            "as it came inLooking at the logistic activation function see Figure 111  you can see that when\n",
            "inputs become large negative or positive the function saturates at 0 or 1 with a\n",
            "derivative extremely close to 0 Thus when backpropagation kicks in it has virtually\n",
            "no gradient to propagate back through the network and what little gradient exists\n",
            "keeps getting diluted as backpropagation progresses down through the top layers so\n",
            "there is really nothing left for the lower layers\n",
            "Figure 111 Logistic activation function saturation\n",
            "Glorot and He Initialization\n",
            "In their paper Glorot and Bengio propose a way to significantly alleviate this prob\n",
            "lem We need the signal to flow properly in both directions in the forward direction\n",
            "when making predictions and in the reverse direction when backpropagating gradi\n",
            "ents We dont want the signal to die out nor do we want it to explode and saturate\n",
            "For the signal to flow properly the authors argue that we need the variance of the\n",
            "outputs of each layer to be equal to the variance of its inputs2 and we also need the\n",
            "gradients to have equal variance before and after flowing through a layer in the\n",
            "reverse direction please check out the paper if you are interested in the mathematical\n",
            "details It is actually not possible to guarantee both unless the layer has an equal\n",
            "number of inputs and neurons these numbers are called the fanin  and fanout  of the\n",
            "layer but they proposed a good compromise that has proven to work very well in\n",
            "practice the connection weights of each layer must be initialized randomly as\n",
            "VanishingExploding Gradients Problems  3273Such as Delving Deep into Rectifiers Surpassing HumanLevel Performance on ImageNet Classification  K\n",
            "He et al 2015described in Equation 111  where f anavgf aninf anout2 This initialization\n",
            "strategy is called Xavier initialization  after the authors first name or Glorot initiali\n",
            "zation  after his last name\n",
            "Equation 111 Glorot initialization when using the logistic activation function\n",
            "Normal distribution with mean 0 and variance  21\n",
            "fanavg\n",
            "Or a uniform distribution between  r and   r with  r3\n",
            "fanavg\n",
            "If you just replace fanavg with fanin in Equation 111  you get an initialization strategy\n",
            "that was actually already proposed by Y ann LeCun in the 1990s called LeCun initiali\n",
            "zation  which was even recommended in the 1998 book Neural Networks Tricks of the\n",
            "Trade  by Genevieve Orr and KlausRobert Mller Springer It is equivalent to\n",
            "Glorot initialization when fanin  fanout It took over a decade for researchers to realize\n",
            "just how important this trick really is Using Glorot initialization can speed up train\n",
            "ing considerably and it is one of the tricks that led to the current success of Deep\n",
            "Learning\n",
            "Some papers3 have provided similar strategies for different activation functions\n",
            "These strategies differ only by the scale of the variance and whether they use fanavg or\n",
            "fanin as shown in Table 111  for the uniform distribution just compute r32\n",
            "The initialization strategy for the ReLU activation function and its variants includ\n",
            "ing the ELU activation described shortly is sometimes called He initialization  after\n",
            "the last name of its author The SELU activation function will be explained later in\n",
            "this chapter It should be used with LeCun initialization preferably with a normal\n",
            "distribution as we will see\n",
            "Table 111 Initialization parameters for each type of activation function\n",
            "Initialization Activation functions  Normal\n",
            "Glorot None Tanh Logistic Softmax 1  fan avg\n",
            "He ReLU  variants 2  fan in\n",
            "LeCun SELU 1  fan in\n",
            "By default Keras uses Glorot initialization with a uniform distribution Y ou can\n",
            "change this to He initialization by setting kernelinitializerheuniform  or ker\n",
            "nelinitializerhenormal  when creating a layer like this\n",
            "328  Chapter 11 Training Deep Neural Networks4Unless it is part of the first hidden layer a dead neuron may sometimes come back to life gradient descent\n",
            "may indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neurons\n",
            "inputs is positive again\n",
            "5Empirical Evaluation of Rectified Activations in Convolution Network  B Xu et al 2015keraslayersDense10 activation relu kernelinitializer henormal \n",
            "If you want He initialization with a uniform distribution but based on fanavg rather\n",
            "than fanin you can use the VarianceScaling  initializer like this\n",
            "heavginit   kerasinitializers VarianceScaling scale2 modefanavg \n",
            "                                                 distribution uniform \n",
            "keraslayersDense10 activation sigmoid  kernelinitializer heavginit \n",
            "Nonsaturating Activation Functions\n",
            "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing\n",
            "exploding gradients problems were in part due to a poor choice of activation func\n",
            "tion Until then most people had assumed that if Mother Nature had chosen to use\n",
            "roughly sigmoid activation functions in biological neurons they must be an excellent\n",
            "choice But it turns out that other activation functions behave much better in deep\n",
            "neural networks in particular the ReLU activation function mostly because it does\n",
            "not saturate for positive values and also because it is quite fast to compute\n",
            "Unfortunately the ReLU activation function is not perfect It suffers from a problem\n",
            "known as the dying ReLUs  during training some neurons effectively die meaning\n",
            "they stop outputting anything other than 0 In some cases you may find that half of\n",
            "your networks neurons are dead especially if you used a large learning rate A neu\n",
            "ron dies when its weights get tweaked in such a way that the weighted sum of its\n",
            "inputs are negative for all instances in the training set When this happens it just\n",
            "keeps outputting 0s and gradient descent does not affect it anymore since the gradi\n",
            "ent of the ReLU function is 0 when its input is negative4\n",
            "To solve this problem you may want to use a variant of the ReLU function such as\n",
            "the leaky ReLU  This function is defined as LeakyReLUz  max z z see\n",
            "Figure 112  The hyperparameter  defines how much the function leaks it is the\n",
            "slope of the function for z  0 and is typically set to 001 This small slope ensures\n",
            "that leaky ReLUs never die they can go into a long coma but they have a chance to\n",
            "eventually wake up A 2015 paper5 compared several variants of the ReLU activation\n",
            "function and one of its conclusions was that the leaky variants always outperformed\n",
            "the strict ReLU activation function In fact setting   02 huge leak seemed to\n",
            "result in better performance than   001 small leak They also evaluated the\n",
            "randomized leaky ReLU  RReLU where  is picked randomly in a given range during\n",
            "training and it is fixed to an average value during testing It also performed fairly well\n",
            "and seemed to act as a regularizer reducing the risk of overfitting the training set\n",
            "VanishingExploding Gradients Problems  3296Fast and Accurate Deep Network Learning by Exponential Linear Units ELUs  D Clevert T Unterthiner\n",
            "S Hochreiter 2015Finally they also evaluated the parametric leaky ReLU  PReLU where  is authorized\n",
            "to be learned during training instead of being a hyperparameter it becomes a\n",
            "parameter that can be modified by backpropagation like any other parameter This\n",
            "was reported to strongly outperform ReLU on large image datasets but on smaller\n",
            "datasets it runs the risk of overfitting the training set\n",
            "Figure 112 Leaky ReLU\n",
            "Last but not least a 2015 paper  by DjorkArn Clevert et al6 proposed a new activa\n",
            "tion function called the exponential linear unit  ELU that outperformed all the ReLU\n",
            "variants in their experiments training time was reduced and the neural network per\n",
            "formed better on the test set It is represented in Figure 113  and Equation 112\n",
            "shows its definition\n",
            "Equation 112 ELU activation function\n",
            "ELUzexp z 1 ifz 0\n",
            "z ifz 0\n",
            "330  Chapter 11 Training Deep Neural Networks7SelfNormalizing Neural Networks  G Klambauer T Unterthiner and A Mayr 2017\n",
            "Figure 113 ELU activation function\n",
            "It looks a lot like the ReLU function with a few major differences\n",
            "First it takes on negative values when z  0 which allows the unit to have an\n",
            "average output closer to 0 This helps alleviate the vanishing gradients problem\n",
            "as discussed earlier The hyperparameter  defines the value that the ELU func\n",
            "tion approaches when z is a large negative number It is usually set to 1 but you\n",
            "can tweak it like any other hyperparameter if you want\n",
            "Second it has a nonzero gradient for z  0 which avoids the dead neurons prob\n",
            "lem\n",
            "Third if  is equal to 1 then the function is smooth everywhere including\n",
            "around z  0 which helps speed up Gradient Descent since it does not bounce as\n",
            "much left and right of z  0\n",
            "The main drawback of the ELU activation function is that it is slower to compute\n",
            "than the ReLU and its variants due to the use of the exponential function but dur\n",
            "ing training this is compensated by the faster convergence rate However at test time\n",
            "an ELU network will be slower than a ReLU network\n",
            "Moreover in a 2017 paper7 by Gnter Klambauer et al called SelfNormalizing\n",
            "Neural Networks  the authors showed that if you build a neural network composed\n",
            "exclusively of a stack of dense layers and if all hidden layers use the SELU activation\n",
            "function which is just a scaled version of the ELU activation function as its name\n",
            "suggests then the network will selfnormalize  the output of each layer will tend to\n",
            "preserve mean 0 and standard deviation 1 during training which solves the vanish\n",
            "ingexploding gradients problem As a result this activation function often outper\n",
            "VanishingExploding Gradients Problems  331forms other activation functions very significantly for such neural nets especially\n",
            "deep ones However there are a few conditions for selfnormalization to happen\n",
            "The input features must be standardized mean 0 and standard deviation 1\n",
            "Every hidden layers weights must also be initialized using LeCun normal initiali\n",
            "zation In Keras this means setting kernelinitializerlecunnormal \n",
            "The networks architecture must be sequential Unfortunately if you try to use\n",
            "SELU in nonsequential architectures such as recurrent networks see  or\n",
            "networks with skip connections  ie connections that skip layers such as in wide\n",
            " deep nets selfnormalization will not be guaranteed so SELU will not neces\n",
            "sarily outperform other activation functions\n",
            "The paper only guarantees selfnormalization if all layers are dense However in\n",
            "practice the SELU activation function seems to work great with convolutional\n",
            "neural nets as well see Chapter 14 \n",
            "So which activation function should you use for the hidden layers\n",
            "of your deep neural networks Although your mileage will vary in\n",
            "general SELU  ELU  leaky ReLU and its variants  ReLU  tanh\n",
            " logistic If the networks architecture prevents it from self\n",
            "normalizing then ELU may perform better than SELU since SELU\n",
            "is not smooth at z  0 If you care a lot about runtime latency then\n",
            "you may prefer leaky ReLU If you dont want to tweak yet another\n",
            "hyperparameter you may just use the default  values used by\n",
            "Keras eg 03 for the leaky ReLU If you have spare time and\n",
            "computing power you can use crossvalidation to evaluate other\n",
            "activation functions in particular RReLU if your network is over\n",
            "fitting or PReLU if you have a huge training set\n",
            "To use the leaky ReLU activation function you must create a LeakyReLU  instance like\n",
            "this\n",
            "leakyrelu   keraslayersLeakyReLU alpha02\n",
            "layer  keraslayersDense10 activation leakyrelu \n",
            "                           kernelinitializer henormal \n",
            "For PReLU just replace LeakyRelualpha02  with PReLU  There is currently no\n",
            "official implementation of RReLU in Keras but you can fairly easily implement your\n",
            "own see the exercises at the end of Chapter 12 \n",
            "For SELU activation just set activationselu  and kernelinitial\n",
            "izerlecunnormal  when creating a layer\n",
            "layer  keraslayersDense10 activation selu\n",
            "                           kernelinitializer lecunnormal \n",
            "332  Chapter 11 Training Deep Neural Networks8Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift  S Ioffe\n",
            "and C Szegedy 2015Batch Normalization\n",
            "Although using He initialization along with ELU or any variant of ReLU can signifi\n",
            "cantly reduce the vanishingexploding gradients problems at the beginning of train\n",
            "ing it doesnt guarantee that they wont come back during training\n",
            "In a 2015 paper 8 Sergey Ioffe and Christian Szegedy proposed a technique called\n",
            "Batch Normalization  BN to address the vanishingexploding gradients problems\n",
            "The technique consists of adding an operation in the model just before or after the\n",
            "activation function of each hidden layer simply zerocentering and normalizing each\n",
            "input then scaling and shifting the result using two new parameter vectors per layer\n",
            "one for scaling the other for shifting In other words this operation lets the model\n",
            "learn the optimal scale and mean of each of the layers inputs In many cases if you\n",
            "add a BN layer as the very first layer of your neural network you do not need to\n",
            "standardize your training set eg using a StandardScaler  the BN layer will do it\n",
            "for you well approximately since it only looks at one batch at a time and it can also\n",
            "rescale and shift each input feature\n",
            "In order to zerocenter and normalize the inputs the algorithm needs to estimate\n",
            "each inputs mean and standard deviation It does so by evaluating the mean and stan\n",
            "dard deviation of each input over the current minibatch hence the name Batch\n",
            "Normalization The whole operation is summarized in Equation 113 \n",
            "Equation 113 Batch Normalization algorithm\n",
            "1  B1\n",
            "mB\n",
            "i 1mB\n",
            "xi\n",
            "2  B21\n",
            "mB\n",
            "i 1mB\n",
            "xiB2\n",
            "3  xixiB\n",
            "B2\n",
            "4  zixi\n",
            "B is the vector of input means evaluated over the whole minibatch B it con\n",
            "tains one mean per input\n",
            "VanishingExploding Gradients Problems  333B is the vector of input standard deviations also evaluated over the whole mini\n",
            "batch it contains one standard deviation per input\n",
            "mB is the number of instances in the minibatch\n",
            "xi is the vector of zerocentered and normalized inputs for instance i\n",
            " is the output scale parameter vector for the layer it contains one scale parame\n",
            "ter per input\n",
            " represents elementwise multiplication each input is multiplied by its corre\n",
            "sponding output scale parameter\n",
            " is the output shift offset parameter vector for the layer it contains one offset\n",
            "parameter per input Each input is offset by its corresponding shift parameter\n",
            " is a tiny number to avoid division by zero typically 105 This is called a\n",
            "smoothing term \n",
            "zi is the output of the BN operation it is a rescaled and shifted version of the\n",
            "inputs\n",
            "So during training BN just standardizes its inputs then rescales and offsets them\n",
            "Good What about at test time Well it is not that simple Indeed we may need to\n",
            "make predictions for individual instances rather than for batches of instances in this\n",
            "case we will have no way to compute each inputs mean and standard deviation\n",
            "Moreover even if we do have a batch of instances it may be too small or the instan\n",
            "ces may not be independent and identically distributed IID so computing statistics\n",
            "over the batch instances would be unreliable during training the batches should not\n",
            "be too small if possible more than 30 instances and all instances should be IID as we\n",
            "saw in Chapter 4  One solution could be to wait until the end of training then run\n",
            "the whole training set through the neural network and compute the mean and stan\n",
            "dard deviation of each input of the BN layer These final input means and standard\n",
            "deviations can then be used instead of the batch input means and standard deviations\n",
            "when making predictions However it is often preferred to estimate these final statis\n",
            "tics during training using a moving average of the layers input means and standard\n",
            "deviations To sum up four parameter vectors are learned in each batchnormalized\n",
            "layer  the ouput scale vector and  the output offset vector are learned through\n",
            "regular backpropagation and  the final input mean vector and  the final input\n",
            "standard deviation vector are estimated using an exponential moving average Note\n",
            "that  and  are estimated during training but they are not used at all during train\n",
            "ing only after training to replace the batch input means and standard deviations in\n",
            "Equation 113 \n",
            "The authors demonstrated that this technique considerably improved all the deep\n",
            "neural networks they experimented with leading to a huge improvement in the\n",
            "ImageNet classification task ImageNet is a large database of images classified into\n",
            "many classes and commonly used to evaluate computer vision systems The vanish\n",
            "334  Chapter 11 Training Deep Neural Networksing gradients problem was strongly reduced to the point that they could use saturat\n",
            "ing activation functions such as the tanh and even the logistic activation function\n",
            "The networks were also much less sensitive to the weight initialization They were\n",
            "able to use much larger learning rates significantly speeding up the learning process\n",
            "Specifically they note that  Applied to a stateoftheart image classification model\n",
            "Batch Normalization achieves the same accuracy with 14 times fewer training steps\n",
            "and beats the original model by a significant margin  Using an ensemble of\n",
            "batchnormalized networks we improve upon the best published result on ImageNet\n",
            "classification reaching 49 top5 validation error and 48 test error exceeding\n",
            "the accuracy of human raters  Finally like a gift that keeps on giving Batch Normal\n",
            "ization also acts like a regularizer reducing the need for other regularization techni\n",
            "ques such as dropout described later in this chapter\n",
            "Batch Normalization does however add some complexity to the model although it\n",
            "can remove the need for normalizing the input data as we discussed earlier More\n",
            "over there is a runtime penalty the neural network makes slower predictions due to\n",
            "the extra computations required at each layer So if you need predictions to be\n",
            "lightningfast you may want to check how well plain ELU  He initialization perform\n",
            "before playing with Batch Normalization\n",
            "Y ou may find that training is rather slow because each epoch takes\n",
            "much more time when you use batch normalization However this\n",
            "is usually counterbalanced by the fact that convergence is much\n",
            "faster with BN so it will take fewer epochs to reach the same per\n",
            "formance All in all wall time  will usually be smaller this is the\n",
            "time measured by the clock on your wall\n",
            "Implementing Batch Normalization with Keras\n",
            "As with most things with Keras implementing Batch Normalization is quite simple\n",
            "Just add a BatchNormalization  layer before or after each hidden layers activation\n",
            "function and optionally add a BN layer as well as the first layer in your model For\n",
            "example this model applies BN after every hidden layer and as the first layer in the\n",
            "model after flattening the input images\n",
            "model  kerasmodelsSequential \n",
            "    keraslayersFlatteninputshape 28 28\n",
            "    keraslayersBatchNormalization \n",
            "    keraslayersDense300 activation elu kernelinitializer henormal \n",
            "    keraslayersBatchNormalization \n",
            "    keraslayersDense100 activation elu kernelinitializer henormal \n",
            "    keraslayersBatchNormalization \n",
            "    keraslayersDense10 activation softmax \n",
            "\n",
            "VanishingExploding Gradients Problems  3359However they are estimated during training based on the training data so arguably they are trainable In\n",
            "Keras Nontrainable really means untouched by backpropagation Thats all In this tiny example with just two hidden layers its unlikely that Batch\n",
            "Normalization will have a very positive impact but for deeper networks it can make a\n",
            "tremendous difference\n",
            "Lets zoom in a bit If you display the model summary you can see that each BN layer\n",
            "adds 4 parameters per input    and  for example the first BN layer adds 3136\n",
            "parameters which is 4 times 784 The last two parameters  and  are the moving\n",
            "averages they are not affected by backpropagation so Keras calls them Non\n",
            "trainable9 if you count the total number of BN parameters 3136  1200  400 and\n",
            "divide by two you get 2368 which is the total number of nontrainable params in\n",
            "this model\n",
            " modelsummary\n",
            "Model sequential3\n",
            "\n",
            "Layer type                 Output Shape              Param \n",
            "\n",
            "flatten3 Flatten          None 784               0\n",
            "\n",
            "batchnormalizationv2 Batc None 784               3136\n",
            "\n",
            "dense50 Dense             None 300               235500\n",
            "\n",
            "batchnormalizationv21 Ba None 300               1200\n",
            "\n",
            "dense51 Dense             None 100               30100\n",
            "\n",
            "batchnormalizationv22 Ba None 100               400\n",
            "\n",
            "dense52 Dense             None 10                1010\n",
            "\n",
            "Total params 271346\n",
            "Trainable params 268978\n",
            "Nontrainable params 2368\n",
            "Lets look at the parameters of the first BN layer Two are trainable by backprop and\n",
            "two are not\n",
            " varname vartrainable  for var in modellayers1variables \n",
            "batchnormalizationv2gamma0 True\n",
            " batchnormalizationv2beta0 True\n",
            " batchnormalizationv2movingmean0 False\n",
            " batchnormalizationv2movingvariance0 False\n",
            "Now when you create a BN layer in Keras it also creates two operations that will be\n",
            "called by Keras at each iteration during training These operations will update the\n",
            "336  Chapter 11 Training Deep Neural Networksmoving averages Since we are using the TensorFlow backend these operations are\n",
            "TensorFlow operations we will discuss TF operations in Chapter 12 \n",
            " modellayers1updates\n",
            "tfOperation cond2Identity typeIdentity\n",
            " tfOperation cond3Identity typeIdentity\n",
            "The authors of the BN paper argued in favor of adding the BN layers before the acti\n",
            "vation functions rather than after as we just did There is some debate about this as\n",
            "it seems to depend on the task So thats one more thing you can experiment with to\n",
            "see which option works best on your dataset To add the BN layers before the activa\n",
            "tion functions we must remove the activation function from the hidden layers and\n",
            "add them as separate layers after the BN layers Moreover since a Batch Normaliza\n",
            "tion layer includes one offset parameter per input you can remove the bias term from\n",
            "the previous layer just pass usebiasFalse  when creating it\n",
            "model  kerasmodelsSequential \n",
            "    keraslayersFlatteninputshape 28 28\n",
            "    keraslayersBatchNormalization \n",
            "    keraslayersDense300 kernelinitializer henormal  usebias False\n",
            "    keraslayersBatchNormalization \n",
            "    keraslayersActivation elu\n",
            "    keraslayersDense100 kernelinitializer henormal  usebias False\n",
            "    keraslayersActivation elu\n",
            "    keraslayersBatchNormalization \n",
            "    keraslayersDense10 activation softmax \n",
            "\n",
            "The BatchNormalization  class has quite a few hyperparameters you can tweak The\n",
            "defaults will usually be fine but you may occasionally need to tweak the momentum \n",
            "This hyperparameter is used when updating the exponential moving averages given a\n",
            "new value v ie a new vector of input means or standard deviations computed over\n",
            "the current batch the running average  is updated using the following equation\n",
            "v v momentum  v1  momentum\n",
            "A good momentum value is typically close to 1for example 09 099 or 0999 you\n",
            "want more 9s for larger datasets and smaller minibatches\n",
            "Another important hyperparameter is axis  it determines which axis should be nor\n",
            "malized It defaults to 1 meaning that by default it will normalize the last axis using\n",
            "the means and standard deviations computed across the other  axes For example\n",
            "when the input batch is 2D ie the batch shape is batch size features this means\n",
            "that each input feature will be normalized based on the mean and standard deviation\n",
            "computed across all the instances in the batch For example the first BN layer in the\n",
            "previous code example will independently normalize and rescale and shift each of\n",
            "the 784 input features However if we move the first BN layer before the Flatten\n",
            "VanishingExploding Gradients Problems  33710Fixup Initialization Residual Learning Without Normalization  Hongyi Zhang Y ann N Dauphin Tengyu\n",
            "Ma 2019\n",
            "11On the difficulty of training recurrent neural networks  R Pascanu et al 2013layer then the input batches will be 3D with shape batch size height width there\n",
            "fore the BN layer will compute 28 means and 28 standard deviations one per column\n",
            "of pixels computed across all instances in the batch and all rows in the column and\n",
            "it will normalize all pixels in a given column using the same mean and standard devi\n",
            "ation There will also be just 28 scale parameters and 28 shift parameters If instead\n",
            "you still want to treat each of the 784 pixels independently then you should set\n",
            "axis1 2 \n",
            "Notice that the BN layer does not perform the same computation during training and\n",
            "after training it uses batch statistics during training and the final statistics after\n",
            "training ie the final value of the moving averages Lets take a peek at the source\n",
            "code of this class to see how this is handled\n",
            "class BatchNormalization Layer\n",
            "    \n",
            "    def callself inputs training None\n",
            "        if training  is None\n",
            "            training   kerasbackendlearningphase \n",
            "        \n",
            "The call  method is the one that actually performs the computations and as you\n",
            "can see it has an extra training  argument if it is None  it falls back to kerasback\n",
            "endlearningphase  which returns 1 during training the fit  method ensures\n",
            "that Otherwise it returns 0 If you ever need to write a custom layer and it needs to\n",
            "behave differently during training and testing simply use the same pattern we will\n",
            "discuss custom layers in Chapter 12 \n",
            "Batch Normalization has become one of the most used layers in deep neural net\n",
            "works to the point that it is often omitted in the diagrams as it is assumed that BN is\n",
            "added after every layer However a very recent paper10 by Hongyi Zhang et al may\n",
            "well change this the authors show that by using a novel fixedupdate fixup weight\n",
            "initialization technique they manage to train a very deep neural network 10000 lay\n",
            "ers without BN achieving stateoftheart performance on complex image classifi\n",
            "cation tasks\n",
            "Gradient Clipping\n",
            "Another popular technique to lessen the exploding gradients problem is to simply\n",
            "clip the gradients during backpropagation so that they never exceed some threshold\n",
            "This is called Gradient Clipping 11 This technique is most often used in recurrent neu\n",
            "338  Chapter 11 Training Deep Neural Networksral networks as Batch Normalization is tricky to use in RNNs as we will see in \n",
            "For other types of networks BN is usually sufficient\n",
            "In Keras implementing Gradient Clipping is just a matter of setting the clipvalue  or\n",
            "clipnorm  argument when creating an optimizer For example\n",
            "optimizer   kerasoptimizers SGDclipvalue 10\n",
            "modelcompilelossmse optimizer optimizer \n",
            "This will clip every component of the gradient vector to a value between 10 and 10\n",
            "This means that all the partial derivatives of the loss with regards to each and every\n",
            "trainable parameter will be clipped between 10 and 10 The threshold is a hyper\n",
            "parameter you can tune Note that it may change the orientation of the gradient vec\n",
            "tor for example if the original gradient vector is 09 1000 it points mostly in the\n",
            "direction of the second axis but once you clip it by value you get 09 10 which\n",
            "points roughly in the diagonal between the two axes In practice however this\n",
            "approach works well If you want to ensure that Gradient Clipping does not change\n",
            "the direction of the gradient vector you should clip by norm by setting clipnorm\n",
            "instead of clipvalue  This will clip the whole gradient if its 2 norm is greater than\n",
            "the threshold you picked For example if you set clipnorm10  then the vector 09\n",
            "1000 will be clipped to 000899964 09999595 preserving its orientation but\n",
            "almost eliminating the first component If you observe that the gradients explode\n",
            "during training you can track the size of the gradients using TensorBoard you may\n",
            "want to try both clipping by value and clipping by norm with different threshold\n",
            "and see which option performs best on the validation set\n",
            "Reusing Pretrained Layers\n",
            "It is generally not a good idea to train a very large DNN from scratch instead you\n",
            "should always try to find an existing neural network that accomplishes a similar task\n",
            "to the one you are trying to tackle we will discuss how to find them in Chapter 14 \n",
            "then just reuse the lower layers of this network this is called transfer learning  It will\n",
            "not only speed up training considerably but will also require much less training data\n",
            "For example suppose that you have access to a DNN that was trained to classify pic\n",
            "tures into 100 different categories including animals plants vehicles and everyday\n",
            "objects Y ou now want to train a DNN to classify specific types of vehicles These\n",
            "tasks are very similar even partly overlapping so you should try to reuse parts of the\n",
            "first network see Figure 114 \n",
            "Reusing Pretrained Layers  339Figure 114 Reusing pretrained layers\n",
            "If the input pictures of your new task dont have the same size as\n",
            "the ones used in the original task you will usually have to add a\n",
            "preprocessing step to resize them to the size expected by the origi\n",
            "nal model More generally transfer learning will work best when\n",
            "the inputs have similar lowlevel features\n",
            "The output layer of the original model should usually be replaced since it is most\n",
            "likely not useful at all for the new task and it may not even have the right number of\n",
            "outputs for the new task\n",
            "Similarly the upper hidden layers of the original model are less likely to be as useful\n",
            "as the lower layers since the highlevel features that are most useful for the new task\n",
            "may differ significantly from the ones that were most useful for the original task Y ou\n",
            "want to find the right number of layers to reuse\n",
            "The more similar the tasks are the more layers you want to reuse\n",
            "starting with the lower layers For very similar tasks you can try\n",
            "keeping all the hidden layers and just replace the output layer\n",
            "Try freezing all the reused layers first ie make their weights nontrainable so gradi\n",
            "ent descent wont modify them then train your model and see how it performs\n",
            "Then try unfreezing one or two of the top hidden layers to let backpropagation tweak\n",
            "them and see if performance improves The more training data you have the more\n",
            "340  Chapter 11 Training Deep Neural Networkslayers you can unfreeze It is also useful to reduce the learning rate when you unfreeze\n",
            "reused layers this will avoid wrecking their finetuned weights\n",
            "If you still cannot get good performance and you have little training data try drop\n",
            "ping the top hidden layers and freeze all remaining hidden layers again Y ou can\n",
            "iterate until you find the right number of layers to reuse If you have plenty of train\n",
            "ing data you may try replacing the top hidden layers instead of dropping them and\n",
            "even add more hidden layers\n",
            "Transfer Learning With Keras\n",
            "Lets look at an example Suppose the fashion MNIST dataset only contained 8 classes\n",
            "for example all classes except for sandals and shirts Someone built and trained a\n",
            "Keras model on that set and got reasonably good performance 90 accuracy Lets\n",
            "call this model A Y ou now want to tackle a different task you have images of sandals\n",
            "and shirts and you want to train a binary classifier positiveshirts negativesan\n",
            "dals However your dataset is quite small you only have 200 labeled images When\n",
            "you train a new model for this task lets call it model B with the same architecture\n",
            "as model A it performs reasonably well 972 accuracy but since its a much easier\n",
            "task there are just 2 classes you were hoping for more While drinking your morn\n",
            "ing coffee you realize that your task is quite similar to task A so perhaps transfer\n",
            "learning can help Lets find out\n",
            "First you need to load model A and create a new model based on the model A s lay\n",
            "ers Lets reuse all layers except for the output layer\n",
            "modelA  kerasmodelsloadmodel mymodelAh5 \n",
            "modelBonA   kerasmodelsSequential modelAlayers1\n",
            "modelBonA addkeraslayersDense1 activation sigmoid \n",
            "Note that modelA  and modelBonA  now share some layers When you train\n",
            "modelBonA  it will also affect modelA  If you want to avoid that you need to clone\n",
            "modelA  before you reuse its layers To do this you must clone model A s architecture\n",
            "then copy its weights since clonemodel  does not clone the weights\n",
            "modelAclone   kerasmodelsclonemodel modelA\n",
            "modelAclone setweights modelAgetweights \n",
            "Now we could just train modelBonA  for task B but since the new output layer was\n",
            "initialized randomly it will make large errors at least during the first few epochs so\n",
            "there will be large error gradients that may wreck the reused weights To avoid this\n",
            "one approach is to freeze the reused layers during the first few epochs giving the new\n",
            "layer some time to learn reasonable weights To do this simply set every layers train\n",
            "able  attribute to False  and compile the model\n",
            "for layer in modelBonA layers1\n",
            "    layertrainable   False\n",
            "Reusing Pretrained Layers  341modelBonA compilelossbinarycrossentropy  optimizer sgd\n",
            "                     metricsaccuracy \n",
            "Y ou must always compile your model after you freeze or unfreeze\n",
            "layers\n",
            "Next we can train the model for a few epochs then unfreeze the reused layers which\n",
            "requires compiling the model again and continue training to finetune the reused\n",
            "layers for task B After unfreezing the reused layers it is usually a good idea to reduce\n",
            "the learning rate once again to avoid damaging the reused weights\n",
            "history  modelBonA fitXtrainB  ytrainB  epochs4\n",
            "                           validationdata XvalidB  yvalidB \n",
            "for layer in modelBonA layers1\n",
            "    layertrainable   True\n",
            "optimizer   kerasoptimizers SGDlr1e4  the default lr is 1e3\n",
            "modelBonA compilelossbinarycrossentropy  optimizer optimizer \n",
            "                     metricsaccuracy \n",
            "history  modelBonA fitXtrainB  ytrainB  epochs16\n",
            "                           validationdata XvalidB  yvalidB \n",
            "So whats the final verdict Well this models test accuracy is 9925 which means\n",
            "that transfer learning reduced the error rate from 28 down to almost 07 Thats a\n",
            "factor of 4\n",
            " modelBonA evaluate XtestB  ytestB \n",
            "006887910133600235 09925\n",
            "Are you convinced Well you shouldnt be I cheated  I tried many configurations\n",
            "until I found one that demonstrated a strong improvement If you try to change the\n",
            "classes or the random seed you will see that the improvement generally drops or\n",
            "even vanishes or reverses What I did is called torturing the data until it confesses \n",
            "When a paper just looks too positive you should be suspicious perhaps the flashy\n",
            "new technique does not help much in fact it may even degrade performance but\n",
            "the authors tried many variants and reported only the best results which may be due\n",
            "to shear luck without mentioning how many failures they encountered on the way\n",
            "Most of the time this is not malicious at all but it is part of the reason why so many\n",
            "results in Science can never be reproduced\n",
            "So why did I cheat Well it turns out that transfer learning does not work very well\n",
            "with small dense networks it works best with deep convolutional neural networks so\n",
            "we will revisit transfer learning in Chapter 14  using the same techniques and this\n",
            "time there will be no cheating I promise\n",
            "342  Chapter 11 Training Deep Neural NetworksUnsupervised Pretraining\n",
            "Suppose you want to tackle a complex task for which you dont have much labeled\n",
            "training data but unfortunately you cannot find a model trained on a similar task\n",
            "Dont lose all hope First you should of course try to gather more labeled training\n",
            "data but if this is too hard or too expensive you may still be able to perform unsuper\n",
            "vised pretraining  see Figure 115  It is often rather cheap to gather unlabeled train\n",
            "ing examples but quite expensive to label them If you can gather plenty of unlabeled\n",
            "training data you can try to train the layers one by one starting with the lowest layer\n",
            "and then going up using an unsupervised feature detector algorithm such as Restric\n",
            "ted Boltzmann Machines  RBMs see  or autoencoders see  Each layer is\n",
            "trained on the output of the previously trained layers all layers except the one being\n",
            "trained are frozen Once all layers have been trained this way you can add the output\n",
            "layer for your task and finetune the final network using supervised learning ie\n",
            "with the labeled training examples At this point you can unfreeze all the pretrained\n",
            "layers or just some of the upper ones\n",
            "Figure 115 Unsupervised pretraining\n",
            "This is a rather long and tedious process but it often works well in fact it is this\n",
            "technique that Geoffrey Hinton and his team used in 2006 and which led to the\n",
            "revival of neural networks and the success of Deep Learning Until 2010 unsuper\n",
            "vised pretraining typically using RBMs was the norm for deep nets and it was only\n",
            "after the vanishing gradients problem was alleviated that it became much more com\n",
            "Reusing Pretrained Layers  343mon to train DNNs purely using supervised learning However unsupervised pre\n",
            "training today typically using autoencoders rather than RBMs is still a good option\n",
            "when you have a complex task to solve no similar model you can reuse and little\n",
            "labeled training data but plenty of unlabeled training data\n",
            "Pretraining on an Auxiliary Task\n",
            "If you do not have much labeled training data one last option is to train a first neural\n",
            "network on an auxiliary task for which you can easily obtain or generate labeled\n",
            "training data then reuse the lower layers of that network for your actual task The\n",
            "first neural networks lower layers will learn feature detectors that will likely be reusa\n",
            "ble by the second neural network\n",
            "For example if you want to build a system to recognize faces you may only have a\n",
            "few pictures of each individualclearly not enough to train a good classifier Gather\n",
            "ing hundreds of pictures of each person would not be practical However you could\n",
            "gather a lot of pictures of random people on the web and train a first neural network\n",
            "to detect whether or not two different pictures feature the same person Such a net\n",
            "work would learn good feature detectors for faces so reusing its lower layers would\n",
            "allow you to train a good face classifier using little training data\n",
            "For natural language processing  NLP applications you can easily download millions\n",
            "of text documents and automatically generate labeled data from it For example you\n",
            "could randomly mask out some words and train a model to predict what the missing\n",
            "words are eg it should predict that the missing word in the sentence What \n",
            "you saying is probably are or were If you can train a model to reach good per\n",
            "formance on this task then it will already know quite a lot about language and you\n",
            "can certainly reuse it for your actual task and finetune it on your labeled data we\n",
            "will discuss more pretraining tasks in \n",
            "Selfsupervised learning  is when you automatically generate the\n",
            "labels from the data itself then you train a model on the resulting\n",
            "labeled dataset using supervised learning techniques Since this\n",
            "approach requires no human labeling whatsoever it is best classi\n",
            "fied as a form of unsupervised learning\n",
            "Faster Optimizers\n",
            "Training a very large deep neural network can be painfully slow So far we have seen\n",
            "four ways to speed up training and reach a better solution applying a good initiali\n",
            "zation strategy for the connection weights using a good activation function using\n",
            "Batch Normalization and reusing parts of a pretrained network possibly built on an\n",
            "auxiliary task or using unsupervised learning Another huge speed boost comes from\n",
            "using a faster optimizer than the regular Gradient Descent optimizer In this section\n",
            "344  Chapter 11 Training Deep Neural Networks12Some methods of speeding up the convergence of iteration methods  B Polyak 1964we will present the most popular ones Momentum optimization Nesterov Acceler\n",
            "ated Gradient AdaGrad RMSProp and finally Adam and Nadam optimization\n",
            "Momentum Optimization\n",
            "Imagine a bowling ball rolling down a gentle slope on a smooth surface it will start\n",
            "out slowly but it will quickly pick up momentum until it eventually reaches terminal\n",
            "velocity if there is some friction or air resistance This is the very simple idea behind\n",
            "Momentum optimization  proposed by Boris Polyak in 1964 12 In contrast regular\n",
            "Gradient Descent will simply take small regular steps down the slope so it will take\n",
            "much more time to reach the bottom\n",
            "Recall that Gradient Descent simply updates the weights  by directly subtracting the\n",
            "gradient of the cost function J with regards to the weights  J multiplied by\n",
            "the learning rate  The equation is     J It does not care about what the\n",
            "earlier gradients were If the local gradient is tiny it goes very slowly\n",
            "Momentum optimization cares a great deal about what previous gradients were at\n",
            "each iteration it subtracts the local gradient from the momentum vector  m multi\n",
            "plied by the learning rate  and it updates the weights by simply adding this\n",
            "momentum vector see Equation 114  In other words the gradient is used for accel\n",
            "eration not for speed To simulate some sort of friction mechanism and prevent the\n",
            "momentum from growing too large the algorithm introduces a new hyperparameter\n",
            " simply called the momentum  which must be set between 0 high friction and 1\n",
            "no friction A typical momentum value is 09\n",
            "Equation 114 Momentum algorithm\n",
            "1  m mJ\n",
            "2   m\n",
            "Y ou can easily verify that if the gradient remains constant the terminal velocity ie\n",
            "the maximum size of the weight updates is equal to that gradient multiplied by the\n",
            "learning rate  multiplied by 1\n",
            "1  ignoring the sign For example if   09 then the\n",
            "terminal velocity is equal to 10 times the gradient times the learning rate so Momen\n",
            "tum optimization ends up going 10 times faster than Gradient Descent This allows\n",
            "Momentum optimization to escape from plateaus much faster than Gradient Descent\n",
            "In particular we saw in Chapter 4  that when the inputs have very different scales the \n",
            "cost function will look like an elongated bowl see Figure 47  Gradient Descent goes\n",
            "down the steep slope quite fast but then it takes a very long time to go down the val\n",
            "Faster Optimizers  34513 A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O1k2  Yurii\n",
            "Nesterov 1983\n",
            "ley In contrast Momentum optimization will roll down the valley faster and faster\n",
            "until it reaches the bottom the optimum In deep neural networks that dont use\n",
            "Batch Normalization the upper layers will often end up having inputs with very dif\n",
            "ferent scales so using Momentum optimization helps a lot It can also help roll past\n",
            "local optima\n",
            "Due to the momentum the optimizer may overshoot a bit then\n",
            "come back overshoot again and oscillate like this many times\n",
            "before stabilizing at the minimum This is one of the reasons why it\n",
            "is good to have a bit of friction in the system it gets rid of these\n",
            "oscillations and thus speeds up convergence\n",
            "Implementing Momentum optimization in Keras is a nobrainer just use the SGD\n",
            "optimizer and set its momentum  hyperparameter then lie back and profit\n",
            "optimizer   kerasoptimizers SGDlr0001 momentum 09\n",
            "The one drawback of Momentum optimization is that it adds yet another hyperpara\n",
            "meter to tune However the momentum value of 09 usually works well in practice\n",
            "and almost always goes faster than regular Gradient Descent\n",
            "Nesterov Accelerated Gradient\n",
            "One small variant to Momentum optimization proposed by Yurii Nesterov in 1983 13\n",
            "is almost always faster than vanilla Momentum optimization The idea of Nesterov\n",
            "Momentum optimization  or Nesterov Accelerated Gradient  NAG is to measure the\n",
            "gradient of the cost function not at the local position but slightly ahead in the direc\n",
            "tion of the momentum see Equation 115  The only difference from vanilla\n",
            "Momentum optimization is that the gradient is measured at   m rather than at \n",
            "Equation 115 Nesterov Accelerated Gradient algorithm\n",
            "1  m mJm\n",
            "2   m\n",
            "This small tweak works because in general the momentum vector will be pointing in\n",
            "the right direction ie toward the optimum so it will be slightly more accurate to\n",
            "use the gradient measured a bit farther in that direction rather than using the gradi\n",
            "ent at the original position as you can see in Figure 116  where 1 represents the\n",
            "gradient of the cost function measured at the starting point  and 2 represents the\n",
            "346  Chapter 11 Training Deep Neural Networks14 Adaptive Subgradient Methods for Online Learning and Stochastic Optimization  J Duchi et al 2011gradient at the point located at   m As you can see the Nesterov update ends up\n",
            "slightly closer to the optimum After a while these small improvements add up and\n",
            "NAG ends up being significantly faster than regular Momentum optimization More\n",
            "over note that when the momentum pushes the weights across a valley 1 continues\n",
            "to push further across the valley while 2 pushes back toward the bottom of the val\n",
            "ley This helps reduce oscillations and thus converges faster\n",
            "NAG will almost always speed up training compared to regular Momentum optimi\n",
            "zation To use it simply set nesterovTrue  when creating the SGD optimizer\n",
            "optimizer   kerasoptimizers SGDlr0001 momentum 09 nesterov True\n",
            "Figure 116 Regular versus Nesterov Momentum optimization\n",
            "AdaGrad\n",
            "Consider the elongated bowl problem again Gradient Descent starts by quickly going\n",
            "down the steepest slope then slowly goes down the bottom of the valley It would be\n",
            "nice if the algorithm could detect this early on and correct its direction to point a bit\n",
            "more toward the global optimum\n",
            "The AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\n",
            "steepest dimensions see Equation 116 \n",
            "Equation 116 AdaGrad algorithm\n",
            "1  s sJJ\n",
            "2   Js\n",
            "Faster Optimizers  347The first step accumulates the square of the gradients into the vector s recall that the\n",
            " symbol represents the elementwise multiplication This vectorized form is equiv\n",
            "alent to computing si  si   J   i2 for each element si of the vector s in other\n",
            "words each si accumulates the squares of the partial derivative of the cost function\n",
            "with regards to parameter i If the cost function is steep along the ith dimension then\n",
            "si will get larger and larger at each iteration\n",
            "The second step is almost identical to Gradient Descent but with one big difference\n",
            "the gradient vector is scaled down by a factor of  the  symbol represents the\n",
            "elementwise division and  is a smoothing term to avoid division by zero typically\n",
            "set to 1010 This vectorized form is equivalent to computing\n",
            "iiJ isi for all parameters i simultaneously\n",
            "In short this algorithm decays the learning rate but it does so faster for steep dimen\n",
            "sions than for dimensions with gentler slopes This is called an adaptive learning rate  \n",
            "It helps point the resulting updates more directly toward the global optimum see\n",
            "Figure 117  One additional benefit is that it requires much less tuning of the learn\n",
            "ing rate hyperparameter \n",
            "Figure 117 AdaGrad versus Gradient Descent\n",
            "AdaGrad often performs well for simple quadratic problems but unfortunately it\n",
            "often stops too early when training neural networks The learning rate gets scaled\n",
            "down so much that the algorithm ends up stopping entirely before reaching the\n",
            "global optimum So even though Keras has an Adagrad  optimizer you should not use\n",
            "it to train deep neural networks it may be efficient for simpler tasks such as Linear\n",
            "Regression though However understanding Adagrad is helpful to grasp the other\n",
            "adaptive learning rate optimizers\n",
            "348  Chapter 11 Training Deep Neural Networks15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey\n",
            "Hinton in his Coursera class on neural networks slides httpshomlinfo57  video httpshomlinfo58 \n",
            "Amusingly since the authors did not write a paper to describe it researchers often cite slide 29 in lecture 6\n",
            "in their papers\n",
            "16 Adam A Method for Stochastic Optimization  D Kingma J Ba 2015\n",
            "17These are estimations of the mean and uncentered variance of the gradients The mean is often called the\n",
            "first moment  while the variance is often called the second moment  hence the name of the algorithmRMSProp\n",
            "Although AdaGrad slows down a bit too fast and ends up never converging to the\n",
            "global optimum the RMSProp  algorithm15 fixes this by accumulating only the gradi\n",
            "ents from the most recent iterations as opposed to all the gradients since the begin\n",
            "ning of training It does so by using exponential decay in the first step see Equation\n",
            "117 \n",
            "Equation 117 RMSProp algorithm\n",
            "1  s s1 JJ\n",
            "2   Js\n",
            "The decay rate  is typically set to 09 Y es it is once again a new hyperparameter but\n",
            "this default value often works well so you may not need to tune it at all\n",
            "As you might expect Keras has an RMSProp  optimizer\n",
            "optimizer   kerasoptimizers RMSproplr0001 rho09\n",
            "Except on very simple problems this optimizer almost always performs much better\n",
            "than AdaGrad In fact it was the preferred optimization algorithm of many research\n",
            "ers until Adam optimization came around\n",
            "Adam and Nadam Optimization\n",
            "Adam 16 which stands for adaptive moment estimation  combines the ideas of Momen\n",
            "tum optimization and RMSProp just like Momentum optimization it keeps track of\n",
            "an exponentially decaying average of past gradients and just like RMSProp it keeps\n",
            "track of an exponentially decaying average of past squared gradients see Equation\n",
            "118 17\n",
            "Faster Optimizers  349Equation 118 Adam algorithm\n",
            "1  m 1m1 1J\n",
            "2  s 2s1 2JJ\n",
            "3  mm\n",
            "1 1t\n",
            "4  ss\n",
            "1 2t\n",
            "5   m s\n",
            "t represents the iteration number starting at 1\n",
            "If you just look at steps 1 2 and 5 you will notice Adams close similarity to both\n",
            "Momentum optimization and RMSProp The only difference is that step 1 computes\n",
            "an exponentially decaying average rather than an exponentially decaying sum but\n",
            "these are actually equivalent except for a constant factor the decaying average is just\n",
            "1  1 times the decaying sum Steps 3 and 4 are somewhat of a technical detail since\n",
            "m and s are initialized at 0 they will be biased toward 0 at the beginning of training\n",
            "so these two steps will help boost m and s at the beginning of training\n",
            "The momentum decay hyperparameter 1 is typically initialized to 09 while the scal\n",
            "ing decay hyperparameter 2 is often initialized to 0999 As earlier the smoothing\n",
            "term  is usually initialized to a tiny number such as 107 These are the default values\n",
            "for the Adam  class to be precise epsilon  defaults to None  which tells Keras to use\n",
            "kerasbackendepsilon  which defaults to 107 you can change it using\n",
            "kerasbackendsetepsilon \n",
            "optimizer   kerasoptimizers Adamlr0001 beta109 beta20999\n",
            "Since Adam is an adaptive learning rate algorithm like AdaGrad and RMSProp it\n",
            "requires less tuning of the learning rate hyperparameter  Y ou can often use the\n",
            "default value   0001 making Adam even easier to use than Gradient Descent\n",
            "If you are starting to feel overwhelmed by all these different techni\n",
            "ques and wondering how to choose the right ones for your task\n",
            "dont worry some practical guidelines are provided at the end of\n",
            "this chapter\n",
            "Finally two variants of Adam are worth mentioning\n",
            "350  Chapter 11 Training Deep Neural Networks18Incorporating Nesterov Momentum into Adam  Timothy Dozat 2015\n",
            "19The Marginal Value of Adaptive Gradient Methods in Machine Learning  A C Wilson et al 2017\n",
            "Adamax introduced in the same paper as Adam notice that in step 2 of Equation\n",
            "118  Adam accumulates the squares of the gradients in s with a greater weight\n",
            "for more recent weights In step 5 if we ignore  and steps 3 and 4 which are\n",
            "technical details anyway Adam just scales down the parameter updates by the\n",
            "square root of s In short Adam scales down the parameter updates by the 2\n",
            "norm of the timedecayed gradients recall that the 2 norm is the square root of\n",
            "the sum of squares Adamax just replaces the 2 norm with the  norm a fancy\n",
            "way of saying the max Specifically it replaces step 2 in Equation 118  with\n",
            " max 2J it drops step 4 and in step 5 it scales down the gradient\n",
            "updates by a factor of s which is just the max of the timedecayed gradients In\n",
            "practice this can make Adamax more stable than Adam but this really depends\n",
            "on the dataset and in general Adam actually performs better So its just one\n",
            "more optimizer you can try if you experience problems with Adam on some task\n",
            "Nadam optimization18 is more important it is simply Adam optimization plus\n",
            "the Nesterov trick so it will often converge slightly faster than Adam In his\n",
            "report Timothy Dozat compares many different optimizers on various tasks and\n",
            "finds that Nadam generally outperforms Adam but is sometimes outperformed\n",
            "by RMSProp\n",
            "Adaptive optimization methods including RMSProp Adam and\n",
            "Nadam optimization are often great converging fast to a good sol\n",
            "ution However a 2017 paper19 by Ashia C Wilson et al showed\n",
            "that they can lead to solutions that generalize poorly on some data\n",
            "sets So when you are disappointed by your models performance\n",
            "try using plain Nesterov Accelerated Gradient instead your dataset\n",
            "may just be allergic to adaptive gradients Also check out the latest\n",
            "research it is moving fast eg AdaBound\n",
            "All the optimization techniques discussed so far only rely on the firstorder  partial\n",
            "derivatives  Jacobians  The optimization literature contains amazing algorithms\n",
            "based on the secondorder partial derivatives  the Hessians  which are the partial\n",
            "derivatives of the Jacobians Unfortunately these algorithms are very hard to apply\n",
            "to deep neural networks because there are n2 Hessians per output where n is the\n",
            "number of parameters as opposed to just n Jacobians per output Since DNNs typi\n",
            "cally have tens of thousands of parameters the secondorder optimization algorithms\n",
            "Faster Optimizers  35120PrimalDual Subgradient Methods for Convex Problems  Yurii Nesterov 2005\n",
            "21 Ad Click Prediction a View from the Trenches  H McMahan et al 2013often dont even fit in memory and even when they do computing the Hessians is \n",
            "just too slow\n",
            "Training Sparse Models\n",
            "All the optimization algorithms just presented produce dense models meaning that\n",
            "most parameters will be nonzero If you need a blazingly fast model at runtime or if\n",
            "you need it to take up less memory you may prefer to end up with a sparse model\n",
            "instead\n",
            "One trivial way to achieve this is to train the model as usual then get rid of the tiny\n",
            "weights set them to 0 However this will typically not lead to a very sparse model\n",
            "and it may degrade the models performance\n",
            "A better option is to apply strong 1 regularization during training as it pushes the\n",
            "optimizer to zero out as many weights as it can as discussed in Chapter 4  about Lasso\n",
            "Regression\n",
            "However in some cases these techniques may remain insufficient One last option is\n",
            "to apply Dual Averaging  often called Follow The Regularized Leader  FTRL a techni\n",
            "que proposed by Yurii Nesterov 20 When used with 1 regularization this technique\n",
            "often leads to very sparse models Keras implements a variant of FTRL called FTRL\n",
            "Proximal21 in the FTRL  optimizer\n",
            "Learning Rate Scheduling\n",
            "Finding a good learning rate can be tricky If you set it way too high training may\n",
            "actually diverge as we discussed in Chapter 4  If you set it too low training will\n",
            "eventually converge to the optimum but it will take a very long time If you set it\n",
            "slightly too high it will make progress very quickly at first but it will end up dancing\n",
            "around the optimum never really settling down If you have a limited computing\n",
            "budget you may have to interrupt training before it has converged properly yielding\n",
            "a suboptimal solution see Figure 118 \n",
            "352  Chapter 11 Training Deep Neural NetworksFigure 118 Learning curves for various learning rates \n",
            "As we discussed in Chapter 10  one approach is to start with a large learning rate and\n",
            "divide it by 3 until the training algorithm stops diverging Y ou will not be too far\n",
            "from the optimal learning rate which will learn quickly and converge to good solu\n",
            "tion\n",
            "However you can do better than a constant learning rate if you start with a high\n",
            "learning rate and then reduce it once it stops making fast progress you can reach a\n",
            "good solution faster than with the optimal constant learning rate There are many dif\n",
            "ferent strategies to reduce the learning rate during training These strategies are called\n",
            "learning schedules  we briefly introduced this concept in Chapter 4  the most com\n",
            "mon of which are\n",
            "Power scheduling\n",
            "Set the learning rate to a function of the iteration number t t  0  1  tkc\n",
            "The initial learning rate 0 the power c typically set to 1 and the steps s are\n",
            "hyperparameters The learning rate drops at each step and after s steps it is down\n",
            "to 0  2 After s more steps it is down to 0  3 Then down to 0  4 then 0  5\n",
            "and so on As you can see this schedule first drops quickly then more and more\n",
            "slowly Of course this requires tuning 0 s and possibly c\n",
            "Exponential scheduling\n",
            "Set the learning rate to t  0 01ts The learning rate will gradually drop by a\n",
            "factor of 10 every s steps While power scheduling reduces the learning rate more\n",
            "and more slowly exponential scheduling keeps slashing it by a factor of 10 every\n",
            "s steps\n",
            "Piecewise constant scheduling\n",
            "Use a constant learning rate for a number of epochs eg 0  01 for 5 epochs\n",
            "then a smaller learning rate for another number of epochs eg 1  0001 for 50\n",
            "epochs and so on Although this solution can work very well it requires fid\n",
            "Faster Optimizers  35322 An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition  A Senior et al\n",
            "2013dling around to figure out the right sequence of learning rates and how long to\n",
            "use each of them\n",
            "Performance scheduling\n",
            "Measure the validation error every N steps just like for early stopping and\n",
            "reduce the learning rate by a factor of  when the error stops dropping\n",
            "A 2013 paper22 by Andrew Senior et al compared the performance of some of the\n",
            "most popular learning schedules when training deep neural networks for speech rec\n",
            "ognition using Momentum optimization The authors concluded that in this setting\n",
            "both performance scheduling and exponential scheduling performed well They\n",
            "favored exponential scheduling because it was easy to tune and it converged slightly\n",
            "faster to the optimal solution they also mentioned that it was easier to implement\n",
            "than performance scheduling but in Keras both options are easy\n",
            "Implementing power scheduling in Keras is the easiest option just set the decay\n",
            "hyperparameter when creating an optimizer The decay  is the inverse of s the num\n",
            "ber of steps it takes to divide the learning rate by one more unit and Keras assumes\n",
            "that c is equal to 1 For example\n",
            "optimizer   kerasoptimizers SGDlr001 decay1e4\n",
            "Exponential scheduling and piecewise scheduling are quite simple too Y ou first need\n",
            "to define a function that takes the current epoch and returns the learning rate For\n",
            "example lets implement exponential scheduling\n",
            "def exponentialdecayfn epoch\n",
            "    return 001  01epoch  20\n",
            "If you do not want to hardcode 0 and s you can create a function that returns a\n",
            "configured function\n",
            "def exponentialdecay lr0 s\n",
            "    def exponentialdecayfn epoch\n",
            "        return lr0  01epoch  s\n",
            "    return exponentialdecayfn\n",
            "exponentialdecayfn   exponentialdecay lr0001 s20\n",
            "Next just create a LearningRateScheduler  callback giving it the schedule function\n",
            "and pass this callback to the fit  method\n",
            "lrscheduler   kerascallbacks LearningRateScheduler exponentialdecayfn \n",
            "history  modelfitXtrainscaled  ytrain  callbacks lrscheduler \n",
            "354  Chapter 11 Training Deep Neural NetworksThe LearningRateScheduler  will update the optimizers learningrate  attribute at\n",
            "the beginning of each epoch Updating the learning rate just once per epoch is usually\n",
            "enough but if you want it to be updated more often for example at every step you\n",
            "need to write your own callback see the notebook for an example This can make\n",
            "sense if there are many steps per epoch\n",
            "The schedule function can optionally take the current learning rate as a second argu\n",
            "ment For example the following schedule function just multiplies the previous\n",
            "learning rate by 01120 which results in the same exponential decay except the decay\n",
            "now starts at the beginning of epoch 0 instead of 1 This implementation relies on\n",
            "the optimizers initial learning rate contrary to the previous implementation so\n",
            "make sure to set it appropriately\n",
            "def exponentialdecayfn epoch lr\n",
            "    return lr  011  20\n",
            "When you save a model the optimizer and its learning rate get saved along with it\n",
            "This means that with this new schedule function you could just load a trained model\n",
            "and continue training where it left off no problem However things are not so simple\n",
            "if your schedule function uses the epoch  argument indeed the epoch does not get\n",
            "saved and it gets reset to 0 every time you call the fit  method This could lead to a\n",
            "very large learning rate when you continue training a model where it left off which\n",
            "would likely damage your models weights One solution is to manually set the fit\n",
            "methods initialepoch  argument so the epoch  starts at the right value\n",
            "For piecewise constant scheduling you can use a schedule function like the following\n",
            "one as earlier you can define a more general function if you want see the notebook\n",
            "for an example then create a LearningRateScheduler  callback with this function\n",
            "and pass it to the fit  method just like we did for exponential scheduling\n",
            "def piecewiseconstantfn epoch\n",
            "    if epoch  5\n",
            "        return 001\n",
            "    elif epoch  15\n",
            "        return 0005\n",
            "    else\n",
            "        return 0001\n",
            "For performance scheduling simply use the ReduceLROnPlateau  callback For exam\n",
            "ple if you pass the following callback to the fit  method it will multiply the learn\n",
            "ing rate by 05 whenever the best validation loss does not improve for 5 consecutive\n",
            "epochs other options are available please check the documentation for more\n",
            "details\n",
            "lrscheduler   kerascallbacks ReduceLROnPlateau factor05 patience 5\n",
            "Lastly tfkeras offers an alternative way to implement learning rate scheduling just\n",
            "define the learning rate using one of the schedules available in kerasoptimiz\n",
            "Faster Optimizers  355ersschedules  then pass this learning rate to any optimizer This approach updates\n",
            "the learning rate at each step rather than at each epoch For example here is how to\n",
            "implement the same exponential schedule as earlier\n",
            "s  20  lenXtrain  32  number of steps in 20 epochs batch size  32\n",
            "learningrate   kerasoptimizers schedules ExponentialDecay 001 s 01\n",
            "optimizer   kerasoptimizers SGDlearningrate \n",
            "This is nice and simple plus when you save the model the learning rate and its\n",
            "schedule including its state get saved as well However this approach is not part of\n",
            "the Keras API it is specific to tfkeras\n",
            "To sum up exponential decay or performance scheduling can considerably speed up\n",
            "convergence so give them a try\n",
            "Avoiding Overfitting  Through Regularization\n",
            "With four parameters I can fit an elephant and with five I can make him wiggle his\n",
            "trunk\n",
            "John von Neumann cited by Enrico Fermi in Nature 427\n",
            "With thousands of parameters you can fit the whole zoo Deep neural networks typi\n",
            "cally have tens of thousands of parameters sometimes even millions With so many\n",
            "parameters the network has an incredible amount of freedom and can fit a huge vari\n",
            "ety of complex datasets But this great flexibility also means that it is prone to overfit\n",
            "ting the training set We need regularization\n",
            "We already implemented one of the best regularization techniques in Chapter 10 \n",
            "early stopping Moreover even though Batch Normalization was designed to solve\n",
            "the vanishingexploding gradients problems is also acts like a pretty good regularizer\n",
            "In this section we will present other popular regularization techniques for neural net\n",
            "works 1 and 2 regularization dropout and maxnorm regularization\n",
            "1 and 2 Regularization\n",
            "Just like you did in Chapter 4  for simple linear models you can use 1 and 2 regulari\n",
            "zation to constrain a neural networks connection weights but typically not its bia\n",
            "ses Here is how to apply 2 regularization to a Keras layers connection weights\n",
            "using a regularization factor of 001\n",
            "layer  keraslayersDense100 activation elu\n",
            "                           kernelinitializer henormal \n",
            "                           kernelregularizer kerasregularizers l2001\n",
            "The l2  function returns a regularizer that will be called to compute the regulariza\n",
            "tion loss at each step during training This regularization loss is then added to the\n",
            "final loss As you might expect you can just use kerasregularizersl1  if you\n",
            "356  Chapter 11 Training Deep Neural Networks23Improving neural networks by preventing coadaptation of feature detectors  G Hinton et al 2012\n",
            "24Dropout A Simple Way to Prevent Neural Networks from Overfitting  N Srivastava et al 2014want 1 regularization and if you want both 1 and 2 regularization use kerasregu\n",
            "larizersl1l2  specifying both regularization factors\n",
            "Since you will typically want to apply the same regularizer to all layers in your net\n",
            "work as well as the same activation function and the same initialization strategy in all\n",
            "hidden layers you may find yourself repeating the same arguments over and over\n",
            "This makes it ugly and errorprone To avoid this you can try refactoring your code\n",
            "to use loops Another option is to use Pythons functoolspartial  function it lets\n",
            "you create a thin wrapper for any callable with some default argument values For\n",
            "example\n",
            "from functools  import partial\n",
            "RegularizedDense   partialkeraslayersDense\n",
            "                           activation elu\n",
            "                           kernelinitializer henormal \n",
            "                           kernelregularizer kerasregularizers l2001\n",
            "model  kerasmodelsSequential \n",
            "    keraslayersFlatteninputshape 28 28\n",
            "    RegularizedDense 300\n",
            "    RegularizedDense 100\n",
            "    RegularizedDense 10 activation softmax \n",
            "                     kernelinitializer glorotuniform \n",
            "\n",
            "Dropout\n",
            "Dropout  is one of the most popular regularization techniques for deep neural net\n",
            "works It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\n",
            "by Nitish Srivastava et al and it has proven to be highly successful even the stateof\n",
            "theart neural networks got a 12 accuracy boost simply by adding dropout This\n",
            "may not sound like a lot but when a model already has 95 accuracy getting a 2\n",
            "accuracy boost means dropping the error rate by almost 40 going from 5 error to\n",
            "roughly 3\n",
            "It is a fairly simple algorithm at every training step every neuron including the\n",
            "input neurons but always excluding the output neurons has a probability p of being\n",
            "temporarily dropped out  meaning it will be entirely ignored during this training\n",
            "step but it may be active during the next step see Figure 119  The hyperparameter\n",
            "p is called the dropout rate  and it is typically set to 50 After training neurons dont\n",
            "get dropped anymore And thats all except for a technical detail we will discuss\n",
            "momentarily\n",
            "Avoiding Overfitting  Through Regularization  357Figure 119 Dropout regularization\n",
            "It is quite surprising at first that this rather brutal technique works at all Would a\n",
            "company perform better if its employees were told to toss a coin every morning to\n",
            "decide whether or not to go to work Well who knows perhaps it would The com\n",
            "pany would obviously be forced to adapt its organization it could not rely on any sin\n",
            "gle person to fill in the coffee machine or perform any other critical tasks so this\n",
            "expertise would have to be spread across several people Employees would have to\n",
            "learn to cooperate with many of their coworkers not just a handful of them The\n",
            "company would become much more resilient If one person quit it wouldnt make\n",
            "much of a difference Its unclear whether this idea would actually work for compa\n",
            "nies but it certainly does for neural networks Neurons trained with dropout cannot\n",
            "coadapt with their neighboring neurons they have to be as useful as possible on\n",
            "their own They also cannot rely excessively on just a few input neurons they must\n",
            "pay attention to each of their input neurons They end up being less sensitive to slight\n",
            "changes in the inputs In the end you get a more robust network that generalizes bet\n",
            "ter\n",
            "Another way to understand the power of dropout is to realize that a unique neural\n",
            "network is generated at each training step Since each neuron can be either present or\n",
            "absent there is a total of 2N possible networks where N is the total number of drop\n",
            "pable neurons This is such a huge number that it is virtually impossible for the same\n",
            "neural network to be sampled twice Once you have run a 10000 training steps you\n",
            "have essentially trained 10000 different neural networks each with just one training\n",
            "instance These neural networks are obviously not independent since they share\n",
            "many of their weights but they are nevertheless all different The resulting neural\n",
            "network can be seen as an averaging ensemble of all these smaller neural networks\n",
            "There is one small but important technical detail Suppose p  50 in which case\n",
            "during testing a neuron will be connected to twice as many input neurons as it was\n",
            "on average during training To compensate for this fact we need to multiply each\n",
            "358  Chapter 11 Training Deep Neural Networks25This is specific to tfkeras so you may prefer to use kerasbackendsetlearningphase1  before calling\n",
            "the fit  method and set it back to 0 right after\n",
            "neurons input connection weights by 05 after training If we dont each neuron will\n",
            "get a total input signal roughly twice as large as what the network was trained on and\n",
            "it is unlikely to perform well More generally we need to multiply each input connec\n",
            "tion weight by the keep probability  1  p after training Alternatively we can divide\n",
            "each neurons output by the keep probability during training these alternatives are\n",
            "not perfectly equivalent but they work equally well\n",
            "To implement dropout using Keras you can use the keraslayersDropout  layer\n",
            "During training it randomly drops some inputs setting them to 0 and divides the\n",
            "remaining inputs by the keep probability After training it does nothing at all it just\n",
            "passes the inputs to the next layer For example the following code applies dropout\n",
            "regularization before every Dense  layer using a dropout rate of 02\n",
            "model  kerasmodelsSequential \n",
            "    keraslayersFlatteninputshape 28 28\n",
            "    keraslayersDropoutrate02\n",
            "    keraslayersDense300 activation elu kernelinitializer henormal \n",
            "    keraslayersDropoutrate02\n",
            "    keraslayersDense100 activation elu kernelinitializer henormal \n",
            "    keraslayersDropoutrate02\n",
            "    keraslayersDense10 activation softmax \n",
            "\n",
            "Since dropout is only active during training the training loss is\n",
            "penalized compared to the validation loss so comparing the two\n",
            "can be misleading In particular a model may be overfitting the\n",
            "training set and yet have similar training and validation losses So\n",
            "make sure to evaluate the training loss without dropout eg after\n",
            "training Alternatively you can call the fit  method inside a\n",
            "with kerasbackendlearningphasescope1  block this will\n",
            "force dropout to be active during both training and validation25\n",
            "If you observe that the model is overfitting you can increase the dropout rate Con\n",
            "versely you should try decreasing the dropout rate if the model underfits the training\n",
            "set It can also help to increase the dropout rate for large layers and reduce it for\n",
            "small ones Moreover many stateoftheart architectures only use dropout after the\n",
            "last hidden layer so you may want to try this if full dropout is too strong\n",
            "Dropout does tend to significantly slow down convergence but it usually results in a\n",
            "much better model when tuned properly So it is generally well worth the extra time\n",
            "and effort\n",
            "Avoiding Overfitting  Through Regularization  35926Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning  Y  Gal and Z\n",
            "Ghahramani 2016\n",
            "27Specifically they show that training a dropout network is mathematically equivalent to approximate Bayesian\n",
            "inference in a specific type of probabilistic model called a deep Gaussian Process \n",
            "If you want to regularize a selfnormalizing network based on the\n",
            "SELU activation function as discussed earlier you should use\n",
            "AlphaDropout  this is a variant of dropout that preserves the mean\n",
            "and standard deviation of its inputs it was introduced in the same\n",
            "paper as SELU as regular dropout would break selfnormalization\n",
            "MonteCarlo MC Dropout\n",
            "In 2016 a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\n",
            "use dropout\n",
            "First the paper establishes a profound connection between dropout networks\n",
            "ie neural networks containing a dropout layer before every weight layer and\n",
            "approximate Bayesian inference27 giving dropout a solid mathematical justifica\n",
            "tion\n",
            "Second they introduce a powerful technique called MC Dropout  which can\n",
            "boost the performance of any trained dropout model without having to retrain it\n",
            "or even modify it at all\n",
            "Moreover MC Dropout also provides a much better measure of the models\n",
            "uncertainty\n",
            "Finally it is also amazingly simple to implement If this all sounds like a one\n",
            "weird trick advertisement then take a look at the following code It is the full\n",
            "implementation of MC Dropout  boosting the dropout model we trained earlier\n",
            "without retraining it\n",
            "with kerasbackendlearningphasescope 1  force training mode  dropout on\n",
            "    yprobas   npstackmodelpredictXtestscaled \n",
            "                         for sample in range100\n",
            "yproba  yprobas meanaxis0\n",
            "We first force training mode on using a learningphasescope1  context This\n",
            "turns dropout on within the with  block Then we make 100 predictions over the test\n",
            "set and we stack them Since dropout is on all predictions will be different Recall\n",
            "that predict  returns a matrix with one row per instance and one column per class\n",
            "Since there are 10000 instances in the test set and 10 classes this is a matrix of shape\n",
            "10000 10 We stack 100 such matrices so yprobas  is an array of shape 100 10000\n",
            "10 Once we average over the first dimension  axis0  we get yproba  an array of\n",
            "shape 10000 10 like we would get with a single prediction Thats all Averaging\n",
            "360  Chapter 11 Training Deep Neural Networksover multiple predictions with dropout on gives us a Monte Carlo estimate that is\n",
            "generally more reliable than the result of a single prediction with dropout off For\n",
            "example lets look at the models prediction for the first instance in the test set with\n",
            "dropout off\n",
            " nproundmodelpredictXtestscaled 1 2\n",
            "array0   0   0   0   0   0   0   001 0   099\n",
            "      dtypefloat32\n",
            "The model seems almost certain that this image belongs to class 9 ankle boot\n",
            "Should you trust it Is there really so little room for doubt Compare this with the\n",
            "predictions made when dropout is activated\n",
            " nproundyprobas  1 2\n",
            "array0   0   0   0   0   014 0   017 0   068\n",
            "       0   0   0   0   0   016 0   02  0   064\n",
            "       0   0   0   0   0   002 0   001 0   097\n",
            "       \n",
            "This tells a very different story apparently when we activate dropout the model is\n",
            "not sure anymore It still seems to prefer class 9 but sometimes it hesitates with\n",
            "classes 5 sandal and 7 sneaker which makes sense given theyre all footwear Once\n",
            "we average over the first dimension we get the following MC dropout predictions\n",
            " nproundyproba1 2\n",
            "array0   0   0   0   0   022 0   016 0   062\n",
            "      dtypefloat32\n",
            "The model still thinks this image belongs to class 9 but only with a 62 confidence\n",
            "which seems much more reasonable than 99 Plus its useful to know exactly which\n",
            "other classes it thinks are likely And you can also take a look at the standard devia\n",
            "tion of the probability estimates \n",
            " ystd  yprobas stdaxis0\n",
            " nproundystd1 2\n",
            "array0   0   0   0   0   028 0   021 002 032\n",
            "      dtypefloat32\n",
            "Apparently theres quite a lot of variance in the probability estimates if you were\n",
            "building a risksensitive system eg a medical or financial system you should prob\n",
            "ably treat such an uncertain prediction with extreme caution Y ou definitely would\n",
            "not treat it like a 99 confident prediction Moreover the models accuracy got a\n",
            "small boost from 868 to 869\n",
            " accuracy   npsumypred  ytest  lenytest\n",
            " accuracy\n",
            "08694\n",
            "Avoiding Overfitting  Through Regularization  361The number of Monte Carlo samples you use 100 in this example\n",
            "is a hyperparameter you can tweak The higher it is the more accu\n",
            "rate the predictions and their uncertainty estimates will be How\n",
            "ever it you double it inference time will also be doubled\n",
            "Moreover above a certain number of samples you will notice little\n",
            "improvement So your job is to find the right tradeoff between\n",
            "latency and accuracy depending on your application\n",
            "If your model contains other layers that behave in a special way during training such\n",
            "as Batch Normalization layers then you should not force training mode like we just\n",
            "did Instead you should replace the Dropout  layers with the following MCDropout\n",
            "class\n",
            "class MCDropout keraslayersDropout\n",
            "    def callself inputs\n",
            "        return supercallinputs training True\n",
            "We just sublass the Dropout  layer and override the call  method to force its train\n",
            "ing argument to True  see Chapter 12  Similarly you could define an MCAlphaDrop\n",
            "out class by subclassing AlphaDropout  instead If you are creating a model from\n",
            "scratch its just a matter of using MCDropout  rather than Dropout  But if you have a\n",
            "model that was already trained using Dropout  you need to create a new model iden\n",
            "tical to the existing model except replacing the Dropout  layers with MCDropout  then\n",
            "copy the existing models weights to your new model\n",
            "In short MC Dropout is a fantastic technique that boosts dropout models and pro\n",
            "vides better uncertainty estimates And of course since it is just regular dropout dur\n",
            "ing training it also acts like a regularizer\n",
            "MaxNorm Regularization\n",
            "Another regularization technique that is quite popular for neural networks is called\n",
            "maxnorm regularization  for each neuron it constrains the weights w of the incom\n",
            "ing connections such that  w 2  r where r is the maxnorm hyperparameter\n",
            "and   2 is the 2 norm\n",
            "Maxnorm regularization does not add a regularization loss term to the overall loss\n",
            "function Instead it is typically implemented by computing w2 after each training\n",
            "step and clipping w if needed  w wr\n",
            "w2\n",
            "Reducing r increases the amount of regularization and helps reduce overfitting Max\n",
            "norm regularization can also help alleviate the vanishingexploding gradients prob\n",
            "lems if you are not using Batch Normalization\n",
            "362  Chapter 11 Training Deep Neural NetworksTo implement maxnorm regularization in Keras just set every hidden layers ker\n",
            "nelconstraint  argument to a maxnorm  constraint with the appropriate max\n",
            "value for example\n",
            "keraslayersDense100 activation elu kernelinitializer henormal \n",
            "                   kernelconstraint kerasconstraints maxnorm 1\n",
            "After each training iteration the models fit  method will call the object returned\n",
            "by maxnorm  passing it the layers weights and getting clipped weights in return\n",
            "which then replace the layers weights As we will see in Chapter 12  you can define\n",
            "your own custom constraint function if you ever need to and use it as the ker\n",
            "nelconstraint  Y ou can also constrain the bias terms by setting the biascon\n",
            "straint  argument\n",
            "The maxnorm  function has an axis  argument that defaults to 0 A Dense  layer usu\n",
            "ally has weights of shape number of inputs number of neurons so using axis0\n",
            "means that the max norm constraint will apply independently to each neurons weight\n",
            "vector If you want to use maxnorm with convolutional layers see Chapter 14 \n",
            "make sure to set the maxnorm  constraints axis  argument appropriately usually\n",
            "axis0 1 2 \n",
            "Summary and Practical Guidelines\n",
            "In this chapter we have covered a wide range of techniques and you may be wonder\n",
            "ing which ones you should use The configuration in Table 112  will work fine in\n",
            "most cases without requiring much hyperparameter tuning\n",
            "Table 112 Default DNN configuration\n",
            "Hyperparameter Default value\n",
            "Kernel initializer LeCun initialization\n",
            "Activation function SELU\n",
            "Normalization None selfnormalization\n",
            "Regularization Early stopping\n",
            "Optimizer Nadam\n",
            "Learning rate schedule Performance scheduling\n",
            "Dont forget to standardize the input features Of course you should also try to reuse\n",
            "parts of a pretrained neural network if you can find one that solves a similar problem\n",
            "or use unsupervised pretraining if you have a lot of unlabeled data or pretraining on\n",
            "an auxiliary task if you have a lot of labeled data for a similar task\n",
            "The default configuration in Table 112  may need to be tweaked\n",
            "Summary and Practical Guidelines  363If your model selfnormalizes\n",
            "If it overfits the training set then you should add alpha dropout and always\n",
            "use early stopping as well Do not use other regularization methods or else\n",
            "they would break selfnormalization\n",
            "If your model cannot selfnormalize eg it is a recurrent net or it contains skip\n",
            "connections\n",
            "Y ou can try using ELU or another activation function instead of SELU it\n",
            "may perform better Make sure to change the initialization method accord\n",
            "ingly eg He init for ELU or ReLU\n",
            "If it is a deep network you should use Batch Normalization after every hidden\n",
            "layer If it overfits the training set you can also try using maxnorm or 2 reg\n",
            "ularization\n",
            "If you need a sparse model you can use 1 regularization and optionally zero out\n",
            "the tiny weights after training If you need an even sparser model you can try\n",
            "using FTRL instead of Nadam optimization along with 1 regularization In any\n",
            "case this will break selfnormalization so you will need to switch to BN if your\n",
            "model is deep\n",
            "If you need a lowlatency model one that performs lightningfast predictions\n",
            "you may need to use less layers avoid Batch Normalization and possibly replace\n",
            "the SELU activation function with the leaky ReLU Having a sparse model will\n",
            "also help Y ou may also want to reduce the float precision from 32bits to 16bit\n",
            "or even 8bits see \n",
            "If you are building a risksensitive application or inference latency is not very\n",
            "important in your application you can use MC Dropout to boost performance\n",
            "and get more reliable probability estimates along with uncertainty estimates\n",
            "With these guidelines you are now ready to train very deep nets I hope you are now\n",
            "convinced that you can go a very long way using just Keras However there may\n",
            "come a time when you need to have even more control for example to write a custom\n",
            "loss function or to tweak the training algorithm For such cases you will need to use\n",
            "TensorFlows lowerlevel API as we will see in the next chapter\n",
            "Exercises\n",
            "1Is it okay to initialize all the weights to the same value as long as that value is\n",
            "selected randomly using He initialization\n",
            "2Is it okay to initialize the bias terms to 0\n",
            "3Name three advantages of the SELU activation function over ReLU\n",
            "364  Chapter 11 Training Deep Neural Networks4In which cases would you want to use each of the following activation functions\n",
            "SELU leaky ReLU and its variants ReLU tanh logistic and softmax\n",
            "5What may happen if you set the momentum  hyperparameter too close to 1 eg\n",
            "099999 when using an SGD optimizer\n",
            "6Name three ways you can produce a sparse model\n",
            "7Does dropout slow down training Does it slow down inference ie making\n",
            "predictions on new instances What are about MC dropout\n",
            "8Deep Learning\n",
            "aBuild a DNN with five hidden layers of 100 neurons each He initialization\n",
            "and the ELU activation function\n",
            "bUsing Adam optimization and early stopping try training it on MNIST but\n",
            "only on digits 0 to 4 as we will use transfer learning for digits 5 to 9 in the\n",
            "next exercise Y ou will need a softmax output layer with five neurons and as\n",
            "always make sure to save checkpoints at regular intervals and save the final\n",
            "model so you can reuse it later\n",
            "cTune the hyperparameters using crossvalidation and see what precision you\n",
            "can achieve\n",
            "dNow try adding Batch Normalization and compare the learning curves is it\n",
            "converging faster than before Does it produce a better model\n",
            "eIs the model overfitting the training set Try adding dropout to every layer\n",
            "and try again Does it help\n",
            "9Transfer learning\n",
            "aCreate a new DNN that reuses all the pretrained hidden layers of the previous\n",
            "model freezes them and replaces the softmax output layer with a new one\n",
            "bTrain this new DNN on digits 5 to 9 using only 100 images per digit and time\n",
            "how long it takes Despite this small number of examples can you achieve\n",
            "high precision\n",
            "cTry caching the frozen layers and train the model again how much faster is it\n",
            "now\n",
            "dTry again reusing just four hidden layers instead of five Can you achieve a\n",
            "higher precision\n",
            "eNow unfreeze the top two hidden layers and continue training can you get\n",
            "the model to perform even better\n",
            "10Pretraining on an auxiliary task\n",
            "aIn this exercise you will build a DNN that compares two MNIST digit images\n",
            "and predicts whether they represent the same digit or not Then you will reuse\n",
            "the lower layers of this network to train an MNIST classifier using very little\n",
            "Exercises  365training data Start by building two DNNs lets call them DNN A and B both\n",
            "similar to the one you built earlier but without the output layer each DNN\n",
            "should have five hidden layers of 100 neurons each He initialization and ELU\n",
            "activation Next add one more hidden layer with 10 units on top of both\n",
            "DNNs To do this you should use a keraslayersConcatenate  layer to con\n",
            "catenate the outputs of both DNNs for each instance then feed the result to\n",
            "the hidden layer Finally add an output layer with a single neuron using the\n",
            "logistic activation function\n",
            "bSplit the MNIST training set in two sets split 1 should containing 55000\n",
            "images and split 2 should contain contain 5000 images Create a function\n",
            "that generates a training batch where each instance is a pair of MNIST images\n",
            "picked from split 1 Half of the training instances should be pairs of images\n",
            "that belong to the same class while the other half should be images from dif\n",
            "ferent classes For each pair the training label should be 0 if the images are\n",
            "from the same class or 1 if they are from different classes\n",
            "cTrain the DNN on this training set For each image pair you can simultane\n",
            "ously feed the first image to DNN A and the second image to DNN B The\n",
            "whole network will gradually learn to tell whether two images belong to the\n",
            "same class or not\n",
            "dNow create a new DNN by reusing and freezing the hidden layers of DNN A\n",
            "and adding a softmax output layer on top with 10 neurons Train this network\n",
            "on split 2 and see if you can achieve high performance despite having only\n",
            "500 images per class\n",
            "Solutions to these exercises are available in \n",
            "366  Chapter 11 Training Deep Neural NetworksCHAPTER 12\n",
            "Custom Models and Training with\n",
            "TensorFlow\n",
            "With Early Release ebooks you get books in their earliest form\n",
            "the authors raw and unedited content as he or she writesso you\n",
            "can take advantage of these technologies long before the official\n",
            "release of these titles The following will be Chapter 12 in the final\n",
            "release of the book\n",
            "So far we have used only TensorFlows high level API tfkeras but it already got us\n",
            "pretty far we built various neural network architectures including regression and\n",
            "classification nets wide  deep nets and selfnormalizing nets using all sorts of tech\n",
            "niques such as Batch Normalization dropout learning rate schedules and more In\n",
            "fact 95 of the use cases you will encounter will not require anything else than\n",
            "tfkeras and tfdata see Chapter 13  But now its time to dive deeper into TensorFlow\n",
            "and take a look at its lowerlevel Python API  This will be useful when you need extra\n",
            "control to write custom loss functions custom metrics layers models initializers\n",
            "regularizers weight constraints and more Y ou may even need to fully control the\n",
            "training loop itself for example to apply special transformations or constraints to the\n",
            "gradients beyond just clipping them or to use multiple optimizers for different\n",
            "parts of the network We will cover all these cases in this chapter then we will also\n",
            "look at how you can boost your custom models and training algorithms using Ten\n",
            "sorFlows automatic graph generation feature But first lets take a quick tour of Ten\n",
            "sorFlow\n",
            "3671TensorFlow also includes another Deep Learning API called the Estimators API  but it is now recommended\n",
            "to use tfkeras instead\n",
            "TensorFlow 20 was released in March 2019 making TensorFlow\n",
            "much easier to use The first edition of this book used TF 1 while\n",
            "this edition uses TF 2\n",
            "A Quick Tour of TensorFlow\n",
            "As you know TensorFlow  is a powerful library for numerical computation particu\n",
            "larly well suited and finetuned for largescale Machine Learning but you could use\n",
            "it for anything else that requires heavy computations It was developed by the Google\n",
            "Brain team and it powers many of Googles largescale services such as Google Cloud\n",
            "Speech Google Photos and Google Search It was open sourced in November 2015\n",
            "and it is now the most popular deep learning library in terms of citations in papers\n",
            "adoption in companies stars on github etc countless projects use TensorFlow for\n",
            "all sorts of Machine Learning tasks such as image classification natural language\n",
            "processing NLP recommender systems time series forecasting and much more\n",
            "So what does TensorFlow actually offer Heres a summary\n",
            "Its core is very similar to NumPy but with GPU support\n",
            "It also supports distributed computing across multiple devices and servers\n",
            "It includes a kind of justintime JIT compiler that allows it to optimize compu\n",
            "tations for speed and memory usage it works by extracting the computation\n",
            "graph  from a Python function then optimizing it eg by pruning unused nodes\n",
            "and finally running it efficiently eg by automatically running independent\n",
            "operations in parallel\n",
            "Computation graphs can be exported to a portable format so you can train a\n",
            "TensorFlow model in one environment eg using Python on Linux and run it\n",
            "in another eg using Java on an Android device\n",
            "It implements autodiff see Chapter 10  and  and provides some excellent\n",
            "optimizers such as RMSProp Nadam and FTRL see Chapter 11  so you can\n",
            "easily minimize all sorts of loss functions\n",
            "TensorFlow offers many more features built on top of these core features the\n",
            "most important is of course tfkeras1 but it also has data loading  preprocessing\n",
            "ops tfdata tfio etc image processing ops tfimage signal processing ops\n",
            "tfsignal and more see Figure 121  for an overview of TensorFlows Python\n",
            "API\n",
            "368  Chapter 12 Custom Models and Training with TensorFlow2If you ever need to but you probably wont you can write your own operations using the C API\n",
            "3If you are a researcher you may be eligible to use these TPUs for free see httpstensorfloworgtfrc  for more\n",
            "details\n",
            "Figure 121 TensorFlows Python API\n",
            "We will cover many of the packages and functions of the Tensor\n",
            "Flow API but its impossible to cover them all so you should really\n",
            "take some time to browse through the API you will find that it is\n",
            "quite rich and well documented\n",
            "At the lowest level each TensorFlow operation is implemented using highly efficient\n",
            "C code2 Many operations or ops for short have multiple implementations called\n",
            "kernels  each kernel is dedicated to a specific device type such as CPUs GPUs or\n",
            "even TPUs  Tensor Processing Units  As you may know GPUs can dramatically speed\n",
            "up computations by splitting computations into many smaller chunks and running\n",
            "them in parallel across many GPU threads TPUs are even faster Y ou can purchase\n",
            "your own GPU devices for now TensorFlow only supports Nvidia cards with CUDA\n",
            "Compute Capability 35 but TPUs are only available on Google Cloud Machine\n",
            "Learning Engine  see 3\n",
            "TensorFlows architecture is shown in Figure 122  most of the time your code will\n",
            "use the high level APIs especially tfkeras and tfdata but when you need more flexi\n",
            "bility you will use the lower level Python API handling tensors directly Note that\n",
            "APIs for other languages are also available In any case TensorFlows execution\n",
            "A Quick Tour of TensorFlow  369engine will take care of running the operations efficiently even across multiple devi\n",
            "ces and machines if you tell it to\n",
            "Figure 122 TensorFlows architecture\n",
            "TensorFlow runs not only on Windows Linux and MacOS but also on mobile devi\n",
            "ces using TensorFlow Lite  including both iOS and Android see  If you do not\n",
            "want to use the Python API there are also C Java Go and Swift APIs There is\n",
            "even a Javascript implementation called TensorFlowjs  that makes it possible to run\n",
            "your models directly in your browser\n",
            "Theres more to TensorFlow than just the library TensorFlow is at the center of an\n",
            "extensive ecosystem of libraries First theres TensorBoard for visualization see\n",
            "Chapter 10  Next theres TensorFlow Extended TFX  which is a set of libraries built\n",
            "by Google to productionize TensorFlow projects it includes tools for data validation\n",
            "preprocessing model analysis and serving with TF Serving see  Google also\n",
            "launched TensorFlow Hub  a way to easily download and reuse pretrained neural net\n",
            "works Y ou can also get many neural network architectures some of them pretrained\n",
            "in TensorFlows model garden  Check out the TensorFlow Resources  or https\n",
            "githubcomjtoyawesometensorflow  for more TensorFlowbased projects Y ou will\n",
            "find hundreds of TensorFlow projects on GitHub so it is often easy to find existing\n",
            "code for whatever you are trying to do\n",
            "More and more ML papers are released along with their implemen\n",
            "tation and sometimes even with pretrained models Check out\n",
            "httpspaperswithcodecom  to easily find them\n",
            "370  Chapter 12 Custom Models and Training with TensorFlowLast but not least TensorFlow has a dedicated team of passionate and helpful devel\n",
            "opers and a large community contributing to improving it To ask technical ques\n",
            "tions you should use httpstackoverflowcom  and tag your question with tensorflow\n",
            "and python  Y ou can file bugs and feature requests through GitHub For general dis\n",
            "cussions join the Google group \n",
            "Okay its time to start coding\n",
            "Using TensorFlow like NumPy\n",
            "TensorFlows API revolves around tensors  hence the name TensorFlow A tensor is\n",
            "usually a multidimensional array exactly like a NumPy ndarray  but it can also hold\n",
            "a scalar a simple value such as 42 These tensors will be important when we create\n",
            "custom cost functions custom metrics custom layers and more so lets see how to\n",
            "create and manipulate them\n",
            "Tensors and Operations\n",
            "Y ou can easily create a tensor using tfconstant  For example here is a tensor\n",
            "representing a matrix with two rows and three columns of floats\n",
            " tfconstant 1 2 3 4 5 6  matrix\n",
            "tfTensor id0 shape2 3 dtypefloat32 numpy\n",
            "array1 2 3\n",
            "       4 5 6 dtypefloat32\n",
            " tfconstant 42  scalar\n",
            "tfTensor id1 shape dtypeint32 numpy42\n",
            "Just like an ndarray  a tfTensor  has a shape and a data type  dtype \n",
            " t  tfconstant 1 2 3 4 5 6\n",
            " tshape\n",
            "TensorShape2 3\n",
            " tdtype\n",
            "tffloat32\n",
            "Indexing works much like in NumPy\n",
            " t 1\n",
            "tfTensor id5 shape2 2 dtypefloat32 numpy\n",
            "array2 3\n",
            "       5 6 dtypefloat32\n",
            " t 1 tfnewaxis\n",
            "tfTensor id15 shape2 1 dtypefloat32 numpy\n",
            "array2\n",
            "       5 dtypefloat32\n",
            "Most importantly all sorts of tensor operations are available\n",
            " t  10\n",
            "tfTensor id18 shape2 3 dtypefloat32 numpy\n",
            "Using TensorFlow like NumPy  371array11 12 13\n",
            "       14 15 16 dtypefloat32\n",
            " tfsquaret\n",
            "tfTensor id20 shape2 3 dtypefloat32 numpy\n",
            "array 1  4  9\n",
            "       16 25 36 dtypefloat32\n",
            " t  tftranspose t\n",
            "tfTensor id24 shape2 2 dtypefloat32 numpy\n",
            "array14 32\n",
            "       32 77 dtypefloat32\n",
            "Note that writing t  10  is equivalent to calling tfaddt 10  indeed Python calls\n",
            "the magic method tadd10  which just calls tfaddt 10  Other operators\n",
            "like   etc are also supported The  operator was added in Python 35 for matrix\n",
            "multiplication it is equivalent to calling the tfmatmul  function\n",
            "Y ou will find all the basic math operations you need eg tfadd  tfmultiply \n",
            "tfsquare  tfexp  tfsqrt  and more generally most operations that you\n",
            "can find in NumPy eg tfreshape  tfsqueeze  tftile  but sometimes\n",
            "with a different name eg tfreducemean  tfreducesum  tfreducemax \n",
            "tfmathlog  are the equivalent of npmean  npsum  npmax  and nplog \n",
            "When the name differs there is often a good reason for it for example in Tensor\n",
            "Flow you must write tftransposet  you cannot just write tT like in NumPy The\n",
            "reason is that it does not do exactly the same thing in TensorFlow a new tensor is\n",
            "created with its own copy of the transposed data while in NumPy tT is just a trans\n",
            "posed view on the same data Similarly the tfreducesum  operation is named this\n",
            "way because its GPU kernel ie GPU implementation uses a reduce algorithm that\n",
            "does not guarantee the order in which the elements are added because 32bit floats\n",
            "have limited precision this means that the result may change ever so slightly every\n",
            "time you call this operation The same is true of tfreducemean  but of course\n",
            "tfreducemax  is deterministic\n",
            "372  Chapter 12 Custom Models and Training with TensorFlow4A notable exception is tfmathlog  which is commonly used but there is no tflog  alias as it might be\n",
            "confused with logging\n",
            "Many functions and classes have aliases For example tfadd\n",
            "and tfmathadd  are the same function This allows TensorFlow\n",
            "to have concise names for the most common operations4 while\n",
            "preserving well organized packages\n",
            "Keras LowLevel API\n",
            "The Keras API actually has its own lowlevel API located in kerasbackend  It\n",
            "includes functions like square  exp  sqrt  and so on In tfkeras these func\n",
            "tions generally just call the corresponding TensorFlow operations If you want to\n",
            "write code that will be portable to other Keras implementations you should use these\n",
            "Keras functions However they only cover a subset of all functions available in Ten\n",
            "sorFlow so in this book we will use the TensorFlow operations directly Here is as\n",
            "simple example using kerasbackend  which is commonly named K for short\n",
            " from tensorflow  import keras\n",
            " K  kerasbackend\n",
            " KsquareKtranspose t  10\n",
            "tfTensor id39 shape3 2 dtypefloat32 numpy\n",
            "array11 26\n",
            "       14 35\n",
            "       19 46 dtypefloat32\n",
            "Tensors and NumPy\n",
            "Tensors play nice with NumPy you can create a tensor from a NumPy array and vice\n",
            "versa and you can even apply TensorFlow operations to NumPy arrays and NumPy\n",
            "operations to tensors\n",
            " a  nparray2 4 5\n",
            " tfconstant a\n",
            "tfTensor id111 shape3 dtypefloat64 numpyarray2 4 5\n",
            " tnumpy  or nparrayt\n",
            "array1 2 3\n",
            "       4 5 6 dtypefloat32\n",
            " tfsquarea\n",
            "tfTensor id116 shape3 dtypefloat64 numpyarray4 16 25\n",
            " npsquaret\n",
            "array 1  4  9\n",
            "       16 25 36 dtypefloat32\n",
            "Using TensorFlow like NumPy  373Notice that NumPy uses 64bit precision by default while Tensor\n",
            "Flow uses 32bit This is because 32bit precision is generally more\n",
            "than enough for neural networks plus it runs faster and uses less\n",
            "RAM So when you create a tensor from a NumPy array make sure\n",
            "to set dtypetffloat32 \n",
            "Type Conversions\n",
            "Type conversions can significantly hurt performance and they can easily go unno\n",
            "ticed when they are done automatically To avoid this TensorFlow does not perform\n",
            "any type conversions automatically it just raises an exception if you try to execute an\n",
            "operation on tensors with incompatible types For example you cannot add a float\n",
            "tensor and an integer tensor and you cannot even add a 32bit float and a 64bit float\n",
            " tfconstant 2  tfconstant 40\n",
            "TracebackInvalidArgumentErrorexpected to be a float\n",
            " tfconstant 2  tfconstant 40 dtypetffloat64\n",
            "TracebackInvalidArgumentErrorexpected to be a double\n",
            "This may be a bit annoying at first but remember that its for a good cause And of\n",
            "course you can use tfcast  when you really need to convert types\n",
            " t2  tfconstant 40 dtypetffloat64\n",
            " tfconstant 20  tfcastt2 tffloat32\n",
            "tfTensor id136 shape dtypefloat32 numpy420\n",
            "Variables\n",
            "So far we have used constant tensors as their name suggests you cannot modify\n",
            "them However the weights in a neural network need to be tweaked by backpropaga\n",
            "tion and other parameters may also need to change over time eg a momentum\n",
            "optimizer keeps track of past gradients What we need is a tfVariable \n",
            " v  tfVariable 1 2 3 4 5 6\n",
            " v\n",
            "tfVariable Variable0 shape2 3 dtypefloat32 numpy\n",
            "array1 2 3\n",
            "       4 5 6 dtypefloat32\n",
            "A tfVariable  acts much like a constant tensor you can perform the same opera\n",
            "tions with it it plays nicely with NumPy as well and it is just as picky with types But\n",
            "it can also be modified in place using the assign  method or assignadd  or\n",
            "assignsub  which increment or decrement the variable by the given value Y ou\n",
            "can also modify individual cells or slices using the cells or slices assign\n",
            "method direct item assignment will not work or using the scatterupdate  or\n",
            "scatterndupdate  methods\n",
            "vassign2  v             2 4 6 8 10 12\n",
            "v0 1assign42          2 42 6 8 10 12\n",
            "374  Chapter 12 Custom Models and Training with TensorFlowv 2assign0 1    2 42 0 8 10 1\n",
            "vscatterndupdate indices0 0 1 2 updates100 200\n",
            "                            100 42 0 8 10 200\n",
            "In practice you will rarely have to create variables manually since\n",
            "Keras provides an addweight  method that will take care of it for\n",
            "you as we will see Moreover model parameters will generally be\n",
            "updated directly by the optimizers so you will rarely need to\n",
            "update variables manually\n",
            "Other Data Structures\n",
            "TensorFlow supports several other data structures including the following please see\n",
            "the notebook or  for more details\n",
            "Sparse tensors  tfSparseTensor  efficiently represent tensors containing mostly\n",
            "0s The tfsparse  package contains operations for sparse tensors\n",
            "Tensor arrays  tfTensorArray  are lists of tensors They have a fixed size by\n",
            "default but can optionally be made dynamic All tensors they contain must have\n",
            "the same shape and data type\n",
            "Ragged tensors  tfRaggedTensor  represent static lists of lists of tensors where\n",
            "every tensor has the same shape and data type The tfragged  package contains\n",
            "operations for ragged tensors\n",
            "String tensors  are regular tensors of type tfstring  These actually represent byte\n",
            "strings not Unicode strings so if you create a string tensor using a Unicode\n",
            "string eg a regular Python 3 string like caf  then it will get encoded to\n",
            "UTF8 automatically eg bcafxc3xa9  Alternatively you can represent\n",
            "Unicode strings using tensors of type tfint32  where each item represents a\n",
            "Unicode codepoint eg 99 97 102 233  The tfstrings  package with\n",
            "an s contains ops for byte strings and Unicode strings and to convert one into\n",
            "the other\n",
            "Sets are just represented as regular tensors or sparse tensors containing one or\n",
            "more sets and you can manipulate them using operations from the tfsets\n",
            "package\n",
            "Queues  including First In First Out FIFO queues  FIFOQueue  queues that can\n",
            "prioritize some items  PriorityQueue  queues that shuffle their items  Random\n",
            "ShuffleQueue  and queues that can batch items of different shapes by padding\n",
            "PaddingFIFOQueue  These classes are all in the tfqueue  package\n",
            "With tensors operations variables and various data structures at your disposal you\n",
            "are now ready to customize your models and training algorithms\n",
            "Using TensorFlow like NumPy  375Customizing Models and Training Algorithms\n",
            "Lets start by creating a custom loss function which is a simple and common use case\n",
            "Custom Loss Functions\n",
            "Suppose you want to train a regression model but your training set is a bit noisy Of\n",
            "course you start by trying to clean up your dataset by removing or fixing the outliers\n",
            "but it turns out to be insufficient the dataset is still noisy Which loss function should\n",
            "you use The mean squared error might penalize large errors too much so your\n",
            "model will end up being imprecise The mean absolute error would not penalize out\n",
            "liers as much but training might take a while to converge and the trained model\n",
            "might not be very precise This is probably a good time to use the Huber loss intro\n",
            "duced in Chapter 10  instead of the good old MSE The Huber loss is not currently\n",
            "part of the official Keras API but it is available in tfkeras just use an instance of the\n",
            "keraslossesHuber  class But lets pretend its not there implementing it is easy as\n",
            "pie Just create a function that takes the labels and predictions as arguments and use\n",
            "TensorFlow operations to compute every instances loss\n",
            "def huberfn ytrue ypred\n",
            "    error  ytrue  ypred\n",
            "    issmallerror   tfabserror  1\n",
            "    squaredloss   tfsquareerror  2\n",
            "    linearloss    tfabserror  05\n",
            "    return tfwhereissmallerror  squaredloss  linearloss \n",
            "For better performance you should use a vectorized implementa\n",
            "tion as in this example Moreover if you want to benefit from Ten\n",
            "sorFlows graph features you should use only TensorFlow\n",
            "operations\n",
            "It is also preferable to return a tensor containing one loss per instance rather than\n",
            "returning the mean loss This way Keras can apply class weights or sample weights\n",
            "when requested see Chapter 10 \n",
            "Next you can just use this loss when you compile the Keras model then train your\n",
            "model\n",
            "modelcompilelosshuberfn  optimizer nadam\n",
            "modelfitXtrain ytrain \n",
            "And thats it For each batch during training Keras will call the huberfn  function\n",
            "to compute the loss and use it to perform a Gradient Descent step Moreover it will\n",
            "keep track of the total loss since the beginning of the epoch and it will display the\n",
            "mean loss\n",
            "376  Chapter 12 Custom Models and Training with TensorFlowBut what happens to this custom loss when we save the model\n",
            "Saving and Loading Models That Contain Custom Components\n",
            "Saving a model containing a custom loss function actually works fine as Keras just\n",
            "saves the name of the function However whenever you load it you need to provide a\n",
            "dictionary that maps the function name to the actual function More generally when\n",
            "you load a model containing custom objects you need to map the names to the\n",
            "objects\n",
            "model  kerasmodelsloadmodel mymodelwithacustomlossh5 \n",
            "                                customobjects huberfn  huberfn \n",
            "With the current implementation any error between 1 and 1 is considered small \n",
            "But what if we want a different threshold One solution is to create a function that\n",
            "creates a configured loss function\n",
            "def createhuber threshold 10\n",
            "    def huberfn ytrue ypred\n",
            "        error  ytrue  ypred\n",
            "        issmallerror   tfabserror  threshold\n",
            "        squaredloss   tfsquareerror  2\n",
            "        linearloss    threshold   tfabserror  threshold 2  2\n",
            "        return tfwhereissmallerror  squaredloss  linearloss \n",
            "    return huberfn\n",
            "modelcompilelosscreatehuber 20 optimizer nadam\n",
            "Unfortunately when you save the model the threshold  will not be saved This means\n",
            "that you will have to specify the threshold  value when loading the model note that\n",
            "the name to use is huberfn  which is the name of the function we gave Keras not\n",
            "the name of the function that created it\n",
            "model  kerasmodelsloadmodel mymodelwithacustomlossthreshold2h5 \n",
            "                                customobjects huberfn  createhuber 20\n",
            "Y ou can solve this by creating a subclass of the keraslossesLoss  class and imple\n",
            "ment its getconfig  method\n",
            "class HuberLoss keraslossesLoss\n",
            "    def init self threshold 10 kwargs\n",
            "        selfthreshold   threshold\n",
            "        superinit kwargs\n",
            "    def callself ytrue ypred\n",
            "        error  ytrue  ypred\n",
            "        issmallerror   tfabserror  selfthreshold\n",
            "        squaredloss   tfsquareerror  2\n",
            "        linearloss    selfthreshold   tfabserror  selfthreshold 2  2\n",
            "        return tfwhereissmallerror  squaredloss  linearloss \n",
            "    def getconfig self\n",
            "        baseconfig   supergetconfig \n",
            "        return baseconfig  threshold  selfthreshold \n",
            "Customizing Models and Training Algorithms  3775It would not be a good idea to use a weighted mean if we did then two instances with the same weight but in\n",
            "different batches would have a different impact on training depending on the total weight of each batch\n",
            "The Keras API only specifies how to use subclassing to define lay\n",
            "ers models callbacks and regularizers If you build other compo\n",
            "nents such as losses metrics initializers or constraints using\n",
            "subclassing they may not be portable to other Keras implementa\n",
            "tions\n",
            "Lets walk through this code\n",
            "The constructor accepts kwargs  and passes them to the parent constructor\n",
            "which handles standard hyperparameters the name  of the loss and the reduction\n",
            "algorithm to use to aggregate the individual instance losses By default it is\n",
            "sumoverbatchsize  which means that the loss will be the sum of the\n",
            "instance losses possibly weighted by the sample weights if any and then divide\n",
            "the result by the batch size not by the sum of weights so this is not the weighted\n",
            "mean5 Other possible values are sum  and None \n",
            "The call  method takes the labels and predictions computes all the instance\n",
            "losses and returns them\n",
            "The getconfig  method returns a dictionary mapping each hyperparameter\n",
            "name to its value It first calls the parent classs getconfig  method then adds\n",
            "the new hyperparameters to this dictionary note that the convenient x  syn\n",
            "tax was added in Python 35\n",
            "Y ou can then use any instance of this class when you compile the model\n",
            "modelcompilelossHuberLoss 2 optimizer nadam\n",
            "When you save the model the threshold will be saved along with it and when you\n",
            "load the model you just need to map the class name to the class itself\n",
            "model  kerasmodelsloadmodel mymodelwithacustomlossclassh5 \n",
            "                                customobjects HuberLoss  HuberLoss \n",
            "When you save a model Keras calls the loss instances getconfig  method and\n",
            "saves the config as JSON in the HDF5 file When you load the model it calls the\n",
            "fromconfig  class method on the HuberLoss  class this method is implemented by\n",
            "the base class  Loss  and just creates an instance of the class passing config  to the\n",
            "constructor\n",
            "Thats it for losses It was not too hard was it Well its just as simple for custom acti\n",
            "vation functions initializers regularizers and constraints Lets look at these now\n",
            "378  Chapter 12 Custom Models and Training with TensorFlowCustom Activation Functions Initializers Regularizers and\n",
            "Constraints\n",
            "Most Keras functionalities such as losses regularizers constraints initializers met\n",
            "rics activation functions layers and even full models can be customized in very much\n",
            "the same way Most of the time you will just need to write a simple function with the\n",
            "appropriate inputs and outputs For example here are examples of a custom activa\n",
            "tion function equivalent to kerasactivationssoftplus  or tfnnsoftplus  a\n",
            "custom Glorot initializer equivalent to kerasinitializersglorotnormal  a cus\n",
            "tom 1 regularizer equivalent to kerasregularizersl1001  and a custom con\n",
            "straint that ensures weights are all positive equivalent to\n",
            "kerasconstraintsnonneg  or tfnnrelu \n",
            "def mysoftplus z  return value is just tfnnsoftplusz\n",
            "    return tfmathlogtfexpz  10\n",
            "def myglorotinitializer shape dtypetffloat32\n",
            "    stddev  tfsqrt2  shape0  shape1\n",
            "    return tfrandomnormalshape stddevstddev dtypedtype\n",
            "def myl1regularizer weights\n",
            "    return tfreducesum tfabs001  weights\n",
            "def mypositiveweights weights  return value is just tfnnreluweights\n",
            "    return tfwhereweights  0 tfzeroslike weights weights\n",
            "As you can see the arguments depend on the type of custom function These custom\n",
            "functions can then be used normally for example\n",
            "layer  keraslayersDense30 activation mysoftplus \n",
            "                           kernelinitializer myglorotinitializer \n",
            "                           kernelregularizer myl1regularizer \n",
            "                           kernelconstraint mypositiveweights \n",
            "The activation function will be applied to the output of this Dense  layer and its result\n",
            "will be passed on to the next layer The layers weights will be initialized using the\n",
            "value returned by the initializer At each training step the weights will be passed to the\n",
            "regularization function to compute the regularization loss which will be added to the\n",
            "main loss to get the final loss used for training Finally the constraint function will be\n",
            "called after each training step and the layers weights will be replaced by the con\n",
            "strained weights\n",
            "If a function has some hyperparameters that need to be saved along with the model\n",
            "then you will want to subclass the appropriate class such as kerasregulariz\n",
            "ersRegularizer  kerasconstraintsConstraint  kerasinitializersInitial\n",
            "izer  or keraslayersLayer  for any layer including activation functions For\n",
            "example much like we did for the custom loss here is a simple class for 1 regulariza\n",
            "Customizing Models and Training Algorithms  3796However the Huber loss is seldom used as a metric the MAE or MSE are preferredtion that saves its factor  hyperparameter this time we do not need to call the parent\n",
            "constructor or the getconfig  method as they are not defined by the parent class\n",
            "class MyL1Regularizer kerasregularizers Regularizer \n",
            "    def init self factor\n",
            "        selffactor  factor\n",
            "    def call self weights\n",
            "        return tfreducesum tfabsselffactor  weights\n",
            "    def getconfig self\n",
            "        return factor  selffactor\n",
            "Note that you must implement the call  method for losses layers including activa\n",
            "tion functions and models or the call  method for regularizers initializers\n",
            "and constraints For metrics things are a bit different as we will see now\n",
            "Custom Metrics\n",
            "Losses and metrics are conceptually not the same thing losses are used by Gradient\n",
            "Descent to train  a model so they must be differentiable at least where they are evalu\n",
            "ated and their gradients should not be 0 everywhere Plus its okay if they are not\n",
            "easily interpretable by humans eg crossentropy In contrast metrics are used to\n",
            "evaluate  a model they must be more easily interpretable and they can be non\n",
            "differentiable or have 0 gradients everywhere eg accuracy\n",
            "That said in most cases defining a custom metric function is exactly the same as\n",
            "defining a custom loss function In fact we could even use the Huber loss function we\n",
            "created earlier as a metric6 it would work just fine and persistence would also work\n",
            "the same way in this case only saving the name of the function huberfn \n",
            "modelcompilelossmse optimizer nadam metricscreatehuber 20\n",
            "For each batch during training Keras will compute this metric and keep track of its\n",
            "mean since the beginning of the epoch Most of the time this is exactly what you\n",
            "want But not always Consider a binary classifiers precision for example As we saw\n",
            "in Chapter 3  precision is the number of true positives divided by the number of posi\n",
            "tive predictions including both true positives and false positives Suppose the model\n",
            "made 5 positive predictions in the first batch 4 of which were correct thats 80 pre\n",
            "cision Then suppose the model made 3 positive predictions in the second batch but\n",
            "they were all incorrect thats 0 precision for the second batch If you just compute\n",
            "the mean of these two precisions you get 40 But wait a second this is not the mod\n",
            "els precision over these two batches Indeed there were a total of 4 true positives 4 \n",
            "0 out of 8 positive predictions 5  3 so the overall precision is 50 not 40 What\n",
            "we need is an object that can keep track of the number of true positives and the num\n",
            "380  Chapter 12 Custom Models and Training with TensorFlowber of false positives and compute their ratio when requested This is precisely what\n",
            "the kerasmetricsPrecision  class does\n",
            " precision   kerasmetricsPrecision \n",
            " precision 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            "tfTensor id581729 shape dtypefloat32 numpy08\n",
            " precision 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0\n",
            "tfTensor id581780 shape dtypefloat32 numpy05\n",
            "In this example we created a Precision  object then we used it like a function pass\n",
            "ing it the labels and predictions for the first batch then for the second batch note\n",
            "that we could also have passed sample weights We used the same number of true\n",
            "and false positives as in the example we just discussed After the first batch it returns\n",
            "the precision of 80 then after the second batch it returns 50 which is the overall\n",
            "precision so far not the second batchs precision This is called a streaming metric  or\n",
            "stateful metric  as it is gradually updated batch after batch\n",
            "At any point we can call the result  method to get the current value of the metric\n",
            "We can also look at its variables tracking the number of true and false positives\n",
            "using the variables  attribute and reset these variables using the resetstates\n",
            "method\n",
            " presult\n",
            "tfTensor id581794 shape dtypefloat32 numpy05\n",
            " pvariables\n",
            "tfVariable truepositives0  numpyarray4 dtypefloat32\n",
            " tfVariable falsepositives0  numpyarray4 dtypefloat32\n",
            " presetstates   both variables get reset to 00\n",
            "If you need to create such a streaming metric you can just create a subclass of the\n",
            "kerasmetricsMetric  class Here is a simple example that keeps track of the total\n",
            "Huber loss and the number of instances seen so far When asked for the result it\n",
            "returns the ratio which is simply the mean Huber loss\n",
            "class HuberMetric kerasmetricsMetric\n",
            "    def init self threshold 10 kwargs\n",
            "        superinit kwargs  handles base args eg dtype\n",
            "        selfthreshold   threshold\n",
            "        selfhuberfn   createhuber threshold \n",
            "        selftotal  selfaddweight total initializer zeros\n",
            "        selfcount  selfaddweight count initializer zeros\n",
            "    def updatestate self ytrue ypred sampleweight None\n",
            "        metric  selfhuberfn ytrue ypred\n",
            "        selftotalassignadd tfreducesum metric\n",
            "        selfcountassignadd tfcasttfsizeytrue tffloat32\n",
            "    def resultself\n",
            "        return selftotal  selfcount\n",
            "    def getconfig self\n",
            "        baseconfig   supergetconfig \n",
            "        return baseconfig  threshold  selfthreshold \n",
            "Customizing Models and Training Algorithms  3817This class is for illustration purposes only A simpler and better implementation would just subclass the\n",
            "kerasmetricsMean  class see the notebook for an example\n",
            "Lets walk through this code7\n",
            "The constructor uses the addweight  method to create the variables needed to\n",
            "keep track of the metrics state over multiple batches in this case the sum of all\n",
            "Huber losses  total  and the number of instances seen so far  count  Y ou could\n",
            "just create variables manually if you preferred Keras tracks any tfVariable  that\n",
            "is set as an attribute and more generally any trackable object such as layers or\n",
            "models\n",
            "The updatestate  method is called when you use an instance of this class as a\n",
            "function as we did with the Precision  object It updates the variables given the\n",
            "labels and predictions for one batch and sample weights but in this case we just\n",
            "ignore them\n",
            "The result  method computes and returns the final result in this case just the\n",
            "mean Huber metric over all instances When you use the metric as a function the\n",
            "updatestate  method gets called first then the result  method is called\n",
            "and its output is returned\n",
            "We also implement the getconfig  method to ensure the threshold  gets\n",
            "saved along with the model\n",
            "The default implementation of the resetstates  method just resets all vari\n",
            "ables to 00 but you can override it if needed\n",
            "Keras will take care of variable persistence seamlessly no action is\n",
            "required\n",
            "When you define a metric using a simple function Keras automatically calls it for\n",
            "each batch and it keeps track of the mean during each epoch just like we did man\n",
            "ually So the only benefit of our HuberMetric  class is that the threshold  will be saved\n",
            "But of course some metrics like precision cannot simply be averaged over batches\n",
            "in thoses cases theres no other option than to implement a streaming metric\n",
            "Now that we have built a streaming metric building a custom layer will seem like a\n",
            "walk in the park\n",
            "382  Chapter 12 Custom Models and Training with TensorFlowCustom Layers\n",
            "Y ou may occasionally want to build an architecture that contains an exotic layer for\n",
            "which TensorFlow does not provide a default implementation In this case you will\n",
            "need to create a custom layer Or sometimes you may simply want to build a very\n",
            "repetitive architecture containing identical blocks of layers repeated many times and\n",
            "it would be convenient to treat each block of layers as a single layer For example if\n",
            "the model is a sequence of layers A B C A B C A B C then you might want to\n",
            "define a custom layer D containing layers A B C and your model would then simply\n",
            "be D D D Lets see how to build custom layers\n",
            "First some layers have no weights such as keraslayersFlatten  or keraslay\n",
            "ersReLU  If you want to create a custom layer without any weights the simplest\n",
            "option is to write a function and wrap it in a keraslayersLambda  layer For exam\n",
            "ple the following layer will apply the exponential function to its inputs\n",
            "exponentiallayer   keraslayersLambdalambda x tfexpx\n",
            "This custom layer can then be used like any other layer using the sequential API the\n",
            "functional API or the subclassing API Y ou can also use it as an activation function\n",
            "or you could just use activationtfexp  or activationkerasactivationsexpo\n",
            "nential  or simply activationexponential  The exponential layer is sometimes\n",
            "used in the output layer of a regression model when the values to predict have very\n",
            "different scales eg 0001 10 1000\n",
            "As you probably guessed by now to build a custom stateful layer ie a layer with\n",
            "weights you need to create a subclass of the keraslayersLayer  class For exam\n",
            "ple the following class implements a simplified version of the Dense  layer\n",
            "class MyDensekeraslayersLayer\n",
            "    def init self units activation None kwargs\n",
            "        superinit kwargs\n",
            "        selfunits  units\n",
            "        selfactivation   kerasactivations getactivation \n",
            "    def buildself batchinputshape \n",
            "        selfkernel  selfaddweight \n",
            "            namekernel  shapebatchinputshape 1 selfunits\n",
            "            initializer glorotnormal \n",
            "        selfbias  selfaddweight \n",
            "            namebias shapeselfunits initializer zeros\n",
            "        superbuildbatchinputshape   must be at the end\n",
            "    def callself X\n",
            "        return selfactivation X  selfkernel  selfbias\n",
            "    def computeoutputshape self batchinputshape \n",
            "        return tfTensorShape batchinputshape aslist1  selfunits\n",
            "Customizing Models and Training Algorithms  3838This function is specific to tfkeras Y ou could use kerasactivationsActivation  instead\n",
            "9The Keras API calls this argument inputshape  but since it also includes the batch dimension I prefer to call\n",
            "it batchinputshape  Same for computeoutputshape     def getconfig self\n",
            "        baseconfig   supergetconfig \n",
            "        return baseconfig  units selfunits\n",
            "                activation  kerasactivations serialize selfactivation \n",
            "Lets walk through this code\n",
            "The constructor takes all the hyperparameters as arguments in this example just\n",
            "units  and activation  and importantly it also takes a kwargs  argument It\n",
            "calls the parent constructor passing it the kwargs  this takes care of standard\n",
            "arguments such as inputshape  trainable  name  and so on Then it saves the\n",
            "hyperparameters as attributes converting the activation  argument to the\n",
            "appropriate activation function using the kerasactivationsget  function it\n",
            "accepts functions standard strings like relu  or selu  or simply None 8\n",
            "The build  methods role is to create the layers variables by calling the\n",
            "addweight  method for each weight The build  method is called the first\n",
            "time the layer is used At that point Keras will know the shape of this layers\n",
            "inputs and it will pass it to the build  method9 which is often necessary to cre\n",
            "ate some of the weights For example we need to know the number of neurons in\n",
            "the previous layer in order to create the connection weights matrix ie the ker\n",
            "nel  this corresponds to the size of the last dimension of the inputs At the end\n",
            "of the build  method and only at the end you must call the parents build\n",
            "method this tells Keras that the layer is built it just sets selfbuilt  True \n",
            "The call  method actually performs the desired operations In this case we\n",
            "compute the matrix multiplication of the inputs X and the layers kernel we add\n",
            "the bias vector we apply the activation function to the result and this gives us the\n",
            "output of the layer\n",
            "The computeoutputshape  method simply returns the shape of this layers\n",
            "outputs In this case it is the same shape as the inputs except the last dimension\n",
            "is replaced with the number of neurons in the layer Note that in tfkeras shapes\n",
            "are instances of the tfTensorShape  class which you can convert to Python lists\n",
            "using aslist \n",
            "The getconfig  method is just like earlier Note that we save the activation\n",
            "functions full configuration by calling kerasactivationsserialize \n",
            "Y ou can now use a MyDense  layer just like any other layer\n",
            "384  Chapter 12 Custom Models and Training with TensorFlowY ou can generally omit the computeoutputshape  method as\n",
            "tfkeras automatically infers the output shape except when the\n",
            "layer is dynamic as we will see shortly In other Keras implemen\n",
            "tations this method is either required or by default it assumes the\n",
            "output shape is the same as the input shape\n",
            "To create a layer with multiple inputs eg Concatenate  the argument to the call\n",
            "method should be a tuple containing all the inputs and similarly the argument to the\n",
            "computeoutputshape  method should be a tuple containing each inputs batch\n",
            "shape To create a layer with multiple outputs the call  method should return the\n",
            "list of outputs and the computeoutputshape  should return the list of batch out\n",
            "put shapes one per output For example the following toy layer takes two inputs\n",
            "and returns three outputs\n",
            "class MyMultiLayer keraslayersLayer\n",
            "    def callself X\n",
            "        X1 X2  X\n",
            "        return X1  X2 X1  X2 X1  X2\n",
            "    def computeoutputshape self batchinputshape \n",
            "        b1 b2  batchinputshape\n",
            "        return b1 b1 b1  should probably handle broadcasting rules\n",
            "This layer may now be used like any other layer but of course only using the func\n",
            "tional and subclassing APIs not the sequential API which only accepts layers with\n",
            "one input and one output\n",
            "If your layer needs to have a different behavior during training and during testing\n",
            "eg if it uses Dropout  or BatchNormalization  layers then you must add a train\n",
            "ing argument to the call  method and use this argument to decide what to do For\n",
            "example lets create a layer that adds Gaussian noise during training for regulariza\n",
            "tion but does nothing during testing Keras actually has a layer that does the same\n",
            "thing keraslayersGaussianNoise \n",
            "class MyGaussianNoise keraslayersLayer\n",
            "    def init self stddev kwargs\n",
            "        superinit kwargs\n",
            "        selfstddev  stddev\n",
            "    def callself X training None\n",
            "        if training \n",
            "            noise  tfrandomnormaltfshapeX stddevselfstddev\n",
            "            return X  noise\n",
            "        else\n",
            "            return X\n",
            "    def computeoutputshape self batchinputshape \n",
            "        return batchinputshape\n",
            "Customizing Models and Training Algorithms  38510The name subclassing API usually refers only to the creation of custom models by subclassing although\n",
            "many other things can be created by subclassing as we saw in this chapterWith that you can now build any custom layer you need Now lets create custom\n",
            "models\n",
            "Custom Models\n",
            "We already looked at custom model classes in Chapter 10  when we discussed the sub\n",
            "classing API10 It is actually quite straightforward just subclass the kerasmod\n",
            "elsModel  class create layers and variables in the constructor and implement the\n",
            "call  method to do whatever you want the model to do For example suppose you\n",
            "want to build the model represented in Figure 123 \n",
            "Figure 123 Custom Model Example\n",
            "The inputs go through a first dense layer then through a residual block  composed of\n",
            "two dense layers and an addition operation as we will see in Chapter 14  a residual\n",
            "block adds its inputs to its outputs then through this same residual block 3 more\n",
            "times then through a second residual block and the final result goes through a dense\n",
            "output layer Note that this model does not make much sense its just an example to\n",
            "illustrate the fact that you can easily build any kind of model you want even contain\n",
            "386  Chapter 12 Custom Models and Training with TensorFlowing loops and skip connections To implement this model it is best to first create a\n",
            "ResidualBlock  layer since we are going to create a couple identical blocks and we\n",
            "might want to reuse it in another model\n",
            "class ResidualBlock keraslayersLayer\n",
            "    def init self nlayers  nneurons  kwargs\n",
            "        superinit kwargs\n",
            "        selfhidden  keraslayersDensenneurons  activation elu\n",
            "                                          kernelinitializer henormal \n",
            "                       for  in rangenlayers \n",
            "    def callself inputs\n",
            "        Z  inputs\n",
            "        for layer in selfhidden\n",
            "            Z  layerZ\n",
            "        return inputs  Z\n",
            "This layer is a bit special since it contains other layers This is handled transparently\n",
            "by Keras it automatically detects that the hidden  attribute contains trackable objects\n",
            "layers in this case so their variables are automatically added to this layers list of\n",
            "variables The rest of this class is selfexplanatory Next lets use the subclassing API\n",
            "to define the model itself\n",
            "class ResidualRegressor kerasmodelsModel\n",
            "    def init self outputdim  kwargs\n",
            "        superinit kwargs\n",
            "        selfhidden1  keraslayersDense30 activation elu\n",
            "                                          kernelinitializer henormal \n",
            "        selfblock1  ResidualBlock 2 30\n",
            "        selfblock2  ResidualBlock 2 30\n",
            "        selfout  keraslayersDenseoutputdim \n",
            "    def callself inputs\n",
            "        Z  selfhidden1inputs\n",
            "        for  in range1  3\n",
            "            Z  selfblock1Z\n",
            "        Z  selfblock2Z\n",
            "        return selfoutZ\n",
            "We create the layers in the constructor and use them in the call  method This\n",
            "model can then be used like any other model compile it fit it evaluate it and use it to\n",
            "make predictions If you also want to be able to save the model using the save\n",
            "method and load it using the kerasmodelsloadmodel  function you must\n",
            "implement the getconfig  method as we did earlier in both the ResidualBlock\n",
            "class and the ResidualRegressor  class Alternatively you can just save and load the\n",
            "weights using the saveweights  and loadweights  methods\n",
            "The Model  class is actually a subclass of the Layer  class so models can be defined and\n",
            "used exactly like layers But a model also has some extra functionalities including of\n",
            "course its compile  fit  evaluate  and predict  methods and a few var\n",
            "Customizing Models and Training Algorithms  387iants such as trainonbatch  or fitgenerator  plus the getlayers\n",
            "method which can return any of the models layers by name or by index and the\n",
            "save  method and support for kerasmodelsloadmodel  and kerasmod\n",
            "elsclonemodel  So if models provide more functionalities than layers why not\n",
            "just define every layer as a model Well technically you could but it is probably\n",
            "cleaner to distinguish the internal components of your model layers or reusable\n",
            "blocks of layers from the model itself The former should subclass the Layer  class\n",
            "while the latter should subclass the Model  class\n",
            "With that you can quite naturally and concisely build almost any model that you find\n",
            "in a paper either using the sequential API the functional API the subclassing API or\n",
            "even a mix of these  Almost any model Y es there are still a couple things that we\n",
            "need to look at first how to define losses or metrics based on model internals and\n",
            "second how to build a custom training loop\n",
            "Losses and Metrics Based on Model Internals\n",
            "The custom losses and metrics we defined earlier were all based on the labels and the\n",
            "predictions and optionally sample weights However you will occasionally want to\n",
            "define losses based on other parts of your model such as the weights or activations of\n",
            "its hidden layers This may be useful for regularization purposes or to monitor some\n",
            "internal aspect of your model\n",
            "To define a custom loss based on model internals just compute it based on any part\n",
            "of the model you want then pass the result to the addloss  method For example\n",
            "the following custom model represents a standard MLP regressor with 5 hidden lay\n",
            "ers except it also implements a reconstruction loss  see  we add an extra Dense\n",
            "layer on top of the last hidden layer and its role is to try to reconstruct the inputs of\n",
            "the model Since the reconstruction must have the same shape as the models inputs\n",
            "we need to create this Dense  layer in the build  method to have access to the shape\n",
            "of the inputs In the call  method we compute both the regular output of the MLP \n",
            "plus the output of the reconstruction layer We then compute the mean squared dif\n",
            "ference between the reconstructions and the inputs and we add this value times\n",
            "005 to the models list of losses by calling addloss  During training Keras will\n",
            "add this loss to the main loss which is why we scaled down the reconstruction loss\n",
            "to ensure the main loss dominates As a result the model will be forced to preserve\n",
            "as much information as possible through the hidden layers even information that is\n",
            "not directly useful for the regression task itself In practice this loss sometimes\n",
            "improves generalization it is a regularization loss\n",
            "class ReconstructingRegressor kerasmodelsModel\n",
            "    def init self outputdim  kwargs\n",
            "        superinit kwargs\n",
            "        selfhidden  keraslayersDense30 activation selu\n",
            "                                          kernelinitializer lecunnormal \n",
            "388  Chapter 12 Custom Models and Training with TensorFlow                       for  in range5\n",
            "        selfout  keraslayersDenseoutputdim \n",
            "    def buildself batchinputshape \n",
            "        ninputs   batchinputshape 1\n",
            "        selfreconstruct   keraslayersDenseninputs \n",
            "        superbuildbatchinputshape \n",
            "    def callself inputs\n",
            "        Z  inputs\n",
            "        for layer in selfhidden\n",
            "            Z  layerZ\n",
            "        reconstruction   selfreconstruct Z\n",
            "        reconloss   tfreducemean tfsquarereconstruction   inputs\n",
            "        selfaddloss 005  reconloss \n",
            "        return selfoutZ\n",
            "Similarly you can add a custom metric based on model internals by computing it in\n",
            "any way you want as long at the result is the output of a metric object For example\n",
            "you can create a kerasmetricsMean  object in the constructor then call it in the\n",
            "call  method passing it the reconloss  and finally add it to the model by calling\n",
            "the models addmetric  method This way when you train the model Keras will\n",
            "display both the mean loss over each epoch the loss is the sum of the main loss plus\n",
            "005 times the reconstruction loss and the mean reconstruction error over each\n",
            "epoch Both will go down during training\n",
            "Epoch 15\n",
            "1161011610   loss 43092  reconstructionerror 17360\n",
            "Epoch 25\n",
            "1161011610   loss 11232  reconstructionerror 08964\n",
            "\n",
            "In over 99 of the cases everything we have discussed so far will be sufficient to\n",
            "implement whatever model you want to build even with complex architectures los\n",
            "ses metrics and so on However in some rare cases you may need to customize the\n",
            "training loop itself However before we get there we need to look at how to compute\n",
            "gradients automatically in TensorFlow\n",
            "Computing Gradients Using Autodiff\n",
            "To understand how to use autodiff see Chapter 10  and  to compute gradients\n",
            "automatically lets consider a simple toy function\n",
            "def fw1 w2\n",
            "    return 3  w1  2  2  w1  w2\n",
            "If you know calculus you can analytically find that the partial derivative of this func\n",
            "tion with regards to w1 is 6  w1   2  w2  Y ou can also find that its partial derivative\n",
            "with regards to w2 is 2  w1  For example at the point w1 w2   5 3  these par\n",
            "Customizing Models and Training Algorithms  389tial derivatives are equal to 36 and 10 respectively so the gradient vector at this point\n",
            "is 36 10 But if this were a neural network the function would be much more com\n",
            "plex typically with tens of thousands of parameters and finding the partial deriva\n",
            "tives analytically by hand would be an almost impossible task One solution could be\n",
            "to compute an approximation of each partial derivative by measuring how much the\n",
            "functions output changes when you tweak the corresponding parameter\n",
            " w1 w2  5 3\n",
            " eps  1e6\n",
            " fw1  eps w2  fw1 w2  eps\n",
            "36000003007075065\n",
            " fw1 w2  eps  fw1 w2  eps\n",
            "10000000003174137\n",
            "Looks about right This works rather well and it is trivial to implement but it is just\n",
            "an approximation and importantly you need to call f at least once per parameter\n",
            "not twice since we could compute fw1 w2  just once This makes this approach\n",
            "intractable for large neural networks So instead we should use autodiff see Chap\n",
            "ter 10  and  TensorFlow makes this pretty simple\n",
            "w1 w2  tfVariable 5 tfVariable 3\n",
            "with tfGradientTape  as tape\n",
            "    z  fw1 w2\n",
            "gradients   tapegradient z w1 w2\n",
            "We first define two variables w1 and w2 then we create a tfGradientTape  context\n",
            "that will automatically record every operation that involves a variable and finally we\n",
            "ask this tape to compute the gradients of the result z with regards to both variables\n",
            "w1 w2  Lets take a look at the gradients that TensorFlow computed\n",
            " gradients\n",
            "tfTensor id828234 shape dtypefloat32 numpy360\n",
            " tfTensor id828229 shape dtypefloat32 numpy100\n",
            "Perfect Not only is the result accurate the precision is only limited by the floating\n",
            "point errors but the gradient  method only goes through the recorded computa\n",
            "tions once in reverse order no matter how many variables there are so it is incredi\n",
            "bly efficient Its like magic\n",
            "Only put the strict minimum inside the tfGradientTape  block\n",
            "to save memory Alternatively you can pause recording by creating\n",
            "a with tapestoprecording  block inside the tfGradient\n",
            "Tape  block\n",
            "The tape is automatically erased immediately after you call its gradient  method so\n",
            "you will get an exception if you try to call gradient  twice\n",
            "390  Chapter 12 Custom Models and Training with TensorFlowwith tfGradientTape  as tape\n",
            "    z  fw1 w2\n",
            "dzdw1  tapegradient z w1   tensor 360\n",
            "dzdw2  tapegradient z w2  RuntimeError\n",
            "If you need to call gradient  more than once you must make the tape persistent\n",
            "and delete it when you are done with it to free resources\n",
            "with tfGradientTape persistent True as tape\n",
            "    z  fw1 w2\n",
            "dzdw1  tapegradient z w1   tensor 360\n",
            "dzdw2  tapegradient z w2   tensor 100 works fine now\n",
            "del tape\n",
            "By default the tape will only track operations involving variables so if you try to\n",
            "compute the gradient of z with regards to anything else than a variable the result will\n",
            "be None \n",
            "c1 c2  tfconstant 5 tfconstant 3\n",
            "with tfGradientTape  as tape\n",
            "    z  fc1 c2\n",
            "gradients   tapegradient z c1 c2  returns None None\n",
            "However you can force the tape to watch any tensors you like to record every opera\n",
            "tion that involves them Y ou can then compute gradients with regards to these ten\n",
            "sors as if they were variables\n",
            "with tfGradientTape  as tape\n",
            "    tapewatchc1\n",
            "    tapewatchc2\n",
            "    z  fc1 c2\n",
            "gradients   tapegradient z c1 c2  returns tensor 36 tensor 10\n",
            "This can be useful in some cases for example if you want to implement a regulariza\n",
            "tion loss that penalizes activations that vary a lot when the inputs vary little the loss\n",
            "will be based on the gradient of the activations with regards to the inputs Since the\n",
            "inputs are not variables you would need to tell the tape to watch them\n",
            "If you compute the gradient of a list of tensors eg z1 z2 z3  with regards to\n",
            "some variables eg w1 w2  TensorFlow actually efficiently computes the sum of\n",
            "the gradients of these tensors ie gradientz1 w1 w2  plus gradientz2\n",
            "w1 w2  plus gradientz3 w1 w2  Due to the way reversemode autodiff\n",
            "works it is not possible to compute the individual gradients  z1 z2 and z3 without\n",
            "actually calling gradient  multiple times once for z1 once for z2 and once for z3\n",
            "which requires making the tape persistent and deleting it afterwards\n",
            "Customizing Models and Training Algorithms  391Moreover it is actually possible to compute second order partial derivatives the Hes\n",
            "sians ie the partial derivatives of the partial derivatives To do this we need to\n",
            "record the operations that are performed when computing the firstorder partial\n",
            "derivatives the Jacobians this requires a second tape Here is how it works\n",
            "with tfGradientTape persistent True as hessiantape \n",
            "    with tfGradientTape  as jacobiantape \n",
            "        z  fw1 w2\n",
            "    jacobians   jacobiantape gradient z w1 w2\n",
            "hessians   hessiantape gradient jacobian  w1 w2\n",
            "            for jacobian  in jacobians \n",
            "del hessiantape\n",
            "The inner tape is used to compute the Jacobians as we did earlier The outer tape is\n",
            "used to compute the partial derivatives of each Jacobian Since we need to call gradi\n",
            "ent  once for each Jacobian or else we would get the sum of the partial derivatives\n",
            "over all the Jabobians as explained earlier we need the outer tape to be persistent so\n",
            "we delete it at the end The Jacobians are obviously the same as earlier 36 and 5 but\n",
            "now we also have the Hessians\n",
            " hessians   dzdw1dw1 dzdw1dw2 dzdw2dw1 dzdw2dw2\n",
            "tfTensor id830578 shape dtypefloat32 numpy60\n",
            "  tfTensor id830595 shape dtypefloat32 numpy20\n",
            " tfTensor id830600 shape dtypefloat32 numpy20 None\n",
            "Lets verify these Hessians The first two are the partial derivatives of 6  w1   2  w2\n",
            "which is as we saw earlier the partial derivative of f with regards to w1 with\n",
            "regards to w1 and w2 The result is correct 6 for w1 and 2 for w2 The next two are the\n",
            "partial derivatives of 2  w1  the partial derivative of f with regards to w2 with\n",
            "regards to w1 and w2 which are 2 for w1 and 0 for w2 Note that TensorFlow returns\n",
            "None  instead of 0 since w2 does not appear at all in 2  w1  TensorFlow also returns\n",
            "None  when you use an operation whose gradients are not defined eg tfargmax \n",
            "In some rare cases you may want to stop gradients from backpropagating through\n",
            "some part of your neural network To do this you must use the tfstopgradient\n",
            "function it just returns its inputs during the forward pass like tfidentity  but\n",
            "it does not let gradients through during backpropagation it acts like a constant For\n",
            "example\n",
            "def fw1 w2\n",
            "    return 3  w1  2  tfstopgradient 2  w1  w2\n",
            "with tfGradientTape  as tape\n",
            "    z  fw1 w2  same result as without stopgradient\n",
            "gradients   tapegradient z w1 w2   returns tensor 30 None\n",
            "392  Chapter 12 Custom Models and Training with TensorFlowFinally you may occasionally run into some numerical issues when computing gradi\n",
            "ents For example if you compute the gradients of the mysoftplus  function for\n",
            "large inputs the result will be NaN\n",
            " x  tfVariable 100\n",
            " with tfGradientTape  as tape\n",
            "     z  mysoftplus x\n",
            "\n",
            " tapegradient z x\n",
            "tfTensor  numpyarraynan dtypefloat32\n",
            "This is because computing the gradients of this function using autodiff leads to some\n",
            "numerical difficulties due to floating point precision errors autodiff ends up com\n",
            "puting infinity divided by infinity which returns NaN Fortunately we can analyti\n",
            "cally find that the derivative of the softplus function is just 1  1  1  expx which\n",
            "is numerically stable Next we can tell TensorFlow to use this stable function when\n",
            "computing the gradients of the mysoftplus  function by decorating it with\n",
            "tfcustomgradient  and making it return both its normal output and the function\n",
            "that computes the derivatives note that it will receive as input the gradients that were\n",
            "backpropagated so far down to the softplus function and according to the chain rule\n",
            "we should multiply them with this functions gradients\n",
            "tfcustomgradient\n",
            "def mybettersoftplus z\n",
            "    exp  tfexpz\n",
            "    def mysoftplusgradients grad\n",
            "        return grad  1  1  exp\n",
            "    return tfmathlogexp  1 mysoftplusgradients\n",
            "Now when we compute the gradients of the mybettersoftplus  function we get\n",
            "the proper result even for large input values however the main output still explodes\n",
            "because of the exponential one workaround is to use tfwhere  to just return the\n",
            "inputs when they are large\n",
            "Congratulations Y ou can now compute the gradients of any function provided it is\n",
            "differentiable at the point where you compute it you can even compute Hessians\n",
            "block backpropagation when needed and even write your own gradient functions\n",
            "This is probably more flexibility than you will ever need even if you build your own\n",
            "custom training loops as we will see now\n",
            "Custom Training Loops\n",
            "In some rare cases the fit  method may not be flexible enough for what you need\n",
            "to do For example the Wide and Deep paper we discussed in Chapter 10  actually\n",
            "uses two different optimizers one for the wide path and the other for the deep path\n",
            "Since the fit  method only uses one optimizer the one that we specify when\n",
            "Customizing Models and Training Algorithms  393compiling the model implementing this paper requires writing your own custom\n",
            "loop\n",
            "Y ou may also like to write your own custom training loops simply to feel more confi\n",
            "dent that it does precisely what you intent it to do perhaps you are unsure about\n",
            "some details of the fit  method It can sometimes feel safer to make everything\n",
            "explicit However remember that writing a custom training loop will make your code\n",
            "longer more error prone and harder to maintain\n",
            "Unless you really need the extra flexibility you should prefer using\n",
            "the fit  method rather than implementing your own training\n",
            "loop especially if you work in a team\n",
            "First lets build a simple model No need to compile it since we will handle the train\n",
            "ing loop manually\n",
            "l2reg  kerasregularizers l2005\n",
            "model  kerasmodelsSequential \n",
            "    keraslayersDense30 activation elu kernelinitializer henormal \n",
            "                       kernelregularizer l2reg\n",
            "    keraslayersDense1 kernelregularizer l2reg\n",
            "\n",
            "Next lets create a tiny function that will randomly sample a batch of instances from\n",
            "the training set in Chapter 13  we will discuss the Data API which offers a much bet\n",
            "ter alternative\n",
            "def randombatch X y batchsize 32\n",
            "    idx  nprandomrandintlenX sizebatchsize \n",
            "    return Xidx yidx\n",
            "Lets also define a function that will display the training status including the number\n",
            "of steps the total number of steps the mean loss since the start of the epoch ie we\n",
            "will use the Mean  metric to compute it and other metrics\n",
            "def printstatusbar iteration  total loss metricsNone\n",
            "    metrics    join 4f formatmname mresult\n",
            "                         for m in loss  metrics or \n",
            "    end   if iteration   total else n\n",
            "    printr   formatiteration  total  metrics\n",
            "          endend\n",
            "This code is selfexplanatory unless you are unfamiliar with Python string format\n",
            "ting 4f  will format a float with 4 digits after the decimal point Moreover using\n",
            "r carriage return along with end  ensures that the status bar always gets printed\n",
            "on the same line In the notebook the printstatusbar  function also includes a\n",
            "progress bar but you could use the handy tqdm library instead\n",
            "394  Chapter 12 Custom Models and Training with TensorFlowWith that lets get down to business First we need to define some hyperparameters\n",
            "choose the optimizer the loss function and the metrics just the MAE in this exam\n",
            "ple\n",
            "nepochs   5\n",
            "batchsize   32\n",
            "nsteps  lenXtrain  batchsize\n",
            "optimizer   kerasoptimizers Nadamlr001\n",
            "lossfn  keraslossesmeansquarederror\n",
            "meanloss   kerasmetricsMean\n",
            "metrics  kerasmetricsMeanAbsoluteError \n",
            "And now we are ready to build the custom loop\n",
            "for epoch in range1 nepochs   1\n",
            "    printEpoch  formatepoch nepochs \n",
            "    for step in range1 nsteps  1\n",
            "        Xbatch ybatch  randombatch Xtrainscaled  ytrain\n",
            "        with tfGradientTape  as tape\n",
            "            ypred  modelXbatch training True\n",
            "            mainloss   tfreducemean lossfnybatch ypred\n",
            "            loss  tfaddnmainloss   modellosses\n",
            "        gradients   tapegradient loss modeltrainablevariables \n",
            "        optimizer applygradients zipgradients  modeltrainablevariables \n",
            "        meanloss loss\n",
            "        for metric in metrics\n",
            "            metricybatch ypred\n",
            "        printstatusbar step  batchsize  lenytrain meanloss  metrics\n",
            "    printstatusbar lenytrain lenytrain meanloss  metrics\n",
            "    for metric in meanloss   metrics\n",
            "        metricresetstates \n",
            "Theres a lot going on in this code so lets walk through it\n",
            "We create two nested loops one for the epochs the other for the batches within\n",
            "an epoch\n",
            "Then we sample a random batch from the training set\n",
            "Inside the tfGradientTape  block we make a prediction for one batch using\n",
            "the model as a function and we compute the loss it is equal to the main loss\n",
            "plus the other losses in this model there is one regularization loss per layer\n",
            "Since the meansquarederror  function returns one loss per instance we\n",
            "compute the mean over the batch using tfreducemean  if you wanted to\n",
            "apply different weights to each instance this is where you would do it The regu\n",
            "larization losses are already reduced to a single scalar each so we just need to\n",
            "sum them using tfaddn  which sums multiple tensors of the same shape\n",
            "and data type\n",
            "Customizing Models and Training Algorithms  39511The truth is we did not process every single instance in the training set because we sampled instances ran\n",
            "domly so some were processed more than once while others were not processed at all In practice thats fine\n",
            "Moreover if the training set size is not a multiple of the batch size we will miss a few instances\n",
            "12Alternatively check out Klearningphase  Ksetlearningphase  and Klearningphasescope \n",
            "13With the exception of optimizers as very few people ever customize these see the notebook for an exampleNext we ask the tape  to compute the gradient of the loss with regards to each\n",
            "trainable variable  not all variables and we apply them to the optimizer to per\n",
            "form a Gradient Descent step\n",
            "Next we update the mean loss and the metrics over the current epoch and we\n",
            "display the status bar\n",
            "At the end of each epoch we display the status bar again to make it look com\n",
            "plete11 and to print a line feed and we reset the states of the mean loss and the\n",
            "metrics\n",
            "If you set the optimizers clipnorm  or clipvalue  hyperparameters it will take care of\n",
            "this for you If you want to apply any other transformation to the gradients simply do\n",
            "so before calling the applygradients  method\n",
            "If you add weight constraints to your model eg by setting kernelconstraint  or\n",
            "biasconstraint  when creating a layer you should update the training loop to\n",
            "apply these constraints just after applygradients \n",
            "for variable  in modelvariables \n",
            "    if variable constraint  is not None\n",
            "        variable assignvariable constraint variable \n",
            "Most importantly this training loop does not handle layers that behave differently\n",
            "during training and testing eg BatchNormalization  or Dropout  To handle these\n",
            "you need to call the model with trainingTrue  and make sure it propagates this to\n",
            "every layer that needs it12\n",
            "As you can see there are quite a lot of things you need to get right it is easy to make a\n",
            "mistake But on the bright side you get full control so its your call\n",
            "Now that you know how to customize any part of your models13 and training algo\n",
            "rithms lets see how you can use TensorFlows automatic graph generation feature it\n",
            "can speed up your custom code considerably and it will also make it portable to any\n",
            "platform supported by TensorFlow see \n",
            "TensorFlow Functions and Graphs\n",
            "In TensorFlow 1 graphs were unavoidable as were the complexities that came with\n",
            "them they were a central part of TensorFlows API In TensorFlow 2 they are still\n",
            "396  Chapter 12 Custom Models and Training with TensorFlowthere but not as central and much much simpler to use To demonstrate this lets\n",
            "start with a trivial function that just computes the cube of its input\n",
            "def cubex\n",
            "    return x  3\n",
            "We can obviously call this function with a Python value such as an int or a float or\n",
            "we can call it with a tensor\n",
            " cube2\n",
            "8\n",
            " cubetfconstant 20\n",
            "tfTensor id18634148 shape dtypefloat32 numpy80\n",
            "Now lets use tffunction  to convert this Python function to a TensorFlow Func\n",
            "tion\n",
            " tfcube  tffunction cube\n",
            " tfcube\n",
            "tensorflowpythoneagerdeffunctionFunction at 0x1546fc080\n",
            "This TF Function can then be used exactly like the original Python function and it\n",
            "will return the same result but as tensors\n",
            " tfcube2\n",
            "tfTensor id18634201 shape dtypeint32 numpy8\n",
            " tfcubetfconstant 20\n",
            "tfTensor id18634211 shape dtypefloat32 numpy80\n",
            "Under the hood tffunction  analyzed the computations performed by the cube\n",
            "function and generated an equivalent computation graph As you can see it was\n",
            "rather painless we will see how this works shortly Alternatively we could have used\n",
            "tffunction  as a decorator this is actually more common\n",
            "tffunction\n",
            "def tfcubex\n",
            "    return x  3\n",
            "The original Python function is still available via the TF Functions pythonfunction\n",
            "attribute in case you ever need it\n",
            " tfcubepythonfunction 2\n",
            "8\n",
            "TensorFlow optimizes the computation graph pruning unused nodes simplifying\n",
            "expressions eg 1  2 would get replaced with 3 and more Once the optimized\n",
            "graph is ready the TF Function efficiently executes the operations in the graph in the\n",
            "appropriate order and in parallel when it can As a result a TF Function will usually\n",
            "run much faster than the original Python function especially if it performs complex\n",
            "TensorFlow Functions and Graphs  39714However in this trivial example the computation graph is so small that there is nothing at all to optimize so\n",
            "tfcube  actually runs much slower than cube \n",
            "computations14 Most of the time you will not really need to know more than that\n",
            "when you want to boost a Python function just transform it into a TF Function\n",
            "Thats all\n",
            "Moreover when you write a custom loss function a custom metric a custom layer or\n",
            "any other custom function and you use it in a Keras model as we did throughout\n",
            "this chapter Keras automatically converts your function into a TF Function no need\n",
            "to use tffunction  So most of the time all this magic is 100 transparent\n",
            "Y ou can tell Keras not to convert your Python functions to TF\n",
            "Functions by setting dynamicTrue  when creating a custom layer\n",
            "or a custom model Alternatively you can set runeagerlyTrue\n",
            "when calling the models compile  method\n",
            "TF Function generates a new graph for every unique set of input shapes and data\n",
            "types and it caches it for subsequent calls For example if you call tfcubetfcon\n",
            "stant10  a graph will be generated for int32 tensors of shape  Then if you call\n",
            "tfcubetfconstant20  the same graph will be reused But if you then call\n",
            "tfcubetfconstant10 20  a new graph will be generated for int32 tensors\n",
            "of shape 2 This is how TF Functions handle polymorphism ie varying argument\n",
            "types and shapes However this is only true for tensor arguments if you pass numer\n",
            "ical Python values to a TF Function a new graph will be generated for every distinct\n",
            "value for example calling tfcube10  and tfcube20  will generate two graphs\n",
            "If you call a TF Function many times with different numerical\n",
            "Python values then many graphs will be generated slowing down\n",
            "your program and using up a lot of RAM Python values should be\n",
            "reserved for arguments that will have few unique values such as\n",
            "hyperparameters like the number of neurons per layer This allows\n",
            "TensorFlow to better optimize each variant of your model\n",
            "Autograph and Tracing\n",
            "So how does TensorFlow generate graphs Well first it starts by analyzing the Python\n",
            "functions source code to capture all the control flow statements such as for loops\n",
            "and while  loops if statements as well as break  continue  and return  statements\n",
            "This first step is called autograph  The reason TensorFlow has to analyze the source\n",
            "code is that Python does not provide any other way to capture control flow state\n",
            "ments it offers magic methods like add  or mul  to capture operators like\n",
            "398  Chapter 12 Custom Models and Training with TensorFlow and  but there are no while  or if  magic methods After analyzing\n",
            "the functions code autograph outputs an upgraded version of that function in which\n",
            "all the control flow statements are replaced by the appropriate TensorFlow opera\n",
            "tions such as tfwhileloop  for loops and tfcond  for if statements For\n",
            "example in Figure 124  autograph analyzes the source code of the sumsquares\n",
            "Python function and it generates the tfsumsquares  function In this function\n",
            "the for loop is replaced by the definition of the loopbody  function containing\n",
            "the body of the original for loop followed by a call to the forstmt  function This\n",
            "call will build the appropriate tfwhileloop  operation in the computation graph\n",
            "Figure 124 How TensorFlow generates graphs using autograph and tracing\n",
            "Next TensorFlow calls this upgraded function but instead of passing the actual\n",
            "argument it passes a symbolic tensor  meaning a tensor without any actual value only\n",
            "a name a data type and a shape For example if you call sumsquarestfcon\n",
            "stant10  then the tfsumsquares  function will actually be called with a sym\n",
            "bolic tensor of type int32 and shape  The function will run in graph mode  meaning\n",
            "that each TensorFlow operation will just add a node in the graph to represent itself\n",
            "and its output tensors as opposed to the regular mode called eager execution  or\n",
            "eager mode  In graph mode TF operations do not perform any actual computations\n",
            "This should feel familiar if you know TensorFlow 1 as graph mode was the default\n",
            "mode In Figure 124  you can see the tfsumsquares  function being called with\n",
            "a symbolic tensor as argument in this case an int32 tensor of shape  and the final\n",
            "graph generated during tracing The ellipses represent operations and the arrows\n",
            "represent tensors both the generated function and the graph are simplified\n",
            "TensorFlow Functions and Graphs  399To view the generated functions source code you can call tfauto\n",
            "graphtocodesumsquarespythonfunction  The code is not\n",
            "meant to be pretty but it can sometimes help for debugging\n",
            "TF Function Rules\n",
            "Most of the time converting a Python function that performs TensorFlow operations\n",
            "into a TF Function is trivial just decorate it with tffunction  or let Keras take care\n",
            "of it for you However there are a few rules to respect\n",
            "If you call any external library including NumPy or even the standard library\n",
            "this call will run only during tracing it will not be part of the graph Indeed a\n",
            "TensorFlow graph can only include TensorFlow constructs tensors operations\n",
            "variables datasets and so on So make sure you use tfreducesum  instead of\n",
            "npsum  and tfsort  instead of the builtin sorted  function and so on\n",
            "unless you really want the code to run only during tracing\n",
            "For example if you define a TF function fx  that just returns npran\n",
            "domrand  a random number will only be generated when the function is\n",
            "traced so ftfconstant2  and ftfconstant3  will return the\n",
            "same random number but ftfconstant2 3  will return a different\n",
            "one If you replace nprandomrand  with tfrandomuniform  then a\n",
            "new random number will be generated upon every call since the operation\n",
            "will be part of the graph\n",
            "If your nonTensorFlow code has sideeffects such as logging something or\n",
            "updating a Python counter then you should not expect that sideeffect to\n",
            "occur every time you call the TF Function as it will only occur when the func\n",
            "tion is traced\n",
            "Y ou can wrap arbitrary Python code in a tfpyfunction  operation but\n",
            "this will hinder performance as TensorFlow will not be able to do any graph\n",
            "optimization on this code and it will also reduce portability as the graph will\n",
            "only run on platforms where Python is available and the right libraries\n",
            "installed\n",
            "Y ou can call other Python functions or TF Functions but they should follow the\n",
            "same rules as TensorFlow will also capture their operations in the computation\n",
            "graph Note that these other functions do not need to be decorated with\n",
            "tffunction \n",
            "If the function creates a TensorFlow variable or any other stateful TensorFlow\n",
            "object such as a dataset or a queue it must do so upon the very first call and\n",
            "only then or else you will get an exception It is usually preferable to create vari\n",
            "ables outside of the TF Function eg in the build  method of a custom layer\n",
            "400  Chapter 12 Custom Models and Training with TensorFlowThe source code of your Python function should be available to TensorFlow If\n",
            "the source code is unavailable for example if you define your function in the\n",
            "Python shell which does not give access to the source code or if you deploy only\n",
            "the compiled Python files pyc  to production then the graph generation pro\n",
            "cess will fail or have limited functionality\n",
            "TensorFlow will only capture for loops that iterate over a tensor or a Dataset  So\n",
            "make sure you use for i in tfrange10  rather than for i in range10  or\n",
            "else the loop will not be captured in the graph Instead it will run during tracing\n",
            "This may be what you want if the for loop is meant to build the graph for exam\n",
            "ple to create each layer in a neural network\n",
            "And as always for performance reasons you should prefer a vectorized imple\n",
            "mentation whenever you can rather than using loops\n",
            "Its time to sum up In this chapter we started with a brief overview of TensorFlow\n",
            "then we looked at TensorFlows lowlevel API including tensors operations variables\n",
            "and special data structures We then used these tools to customize almost every com\n",
            "ponent in tfkeras Finally we looked at how TF Functions can boost performance\n",
            "how graphs are generated using autograph and tracing and what rules to follow when\n",
            "you write TF Functions if you would like to open the black box a bit further for\n",
            "example to explore the generated graphs you will find further technical details\n",
            "in \n",
            "In the next chapter we will look at how to efficiently load and preprocess data with\n",
            "TensorFlow\n",
            "TensorFlow Functions and Graphs  401CHAPTER 13\n",
            "Loading and Preprocessing Data with\n",
            "TensorFlow\n",
            "With Early Release ebooks you get books in their earliest form\n",
            "the authors raw and unedited content as he or she writesso you\n",
            "can take advantage of these technologies long before the official\n",
            "release of these titles The following will be Chapter 13 in the final\n",
            "release of the book\n",
            "So far we have used only datasets that fit in memory but Deep Learning systems are\n",
            "often trained on very large datasets that will not fit in RAM Ingesting a large dataset\n",
            "and preprocessing it efficiently can be tricky to implement with other Deep Learning\n",
            "libraries but TensorFlow makes it easy thanks to the Data API  you just create a data\n",
            "set object tell it where to get the data then transform it in any way you want and\n",
            "TensorFlow takes care of all the implementation details such as multithreading\n",
            "queuing batching prefetching and so on\n",
            "Off the shelf the Data API can read from text files such as CSV files binary files\n",
            "with fixedsize records and binary files that use TensorFlows TFRecord format\n",
            "which supports records of varying sizes TFRecord is a flexible and efficient binary\n",
            "format based on Protocol Buffers an open source binary format The Data API also\n",
            "has support for reading from SQL databases Moreover many Open Source exten\n",
            "sions are available to read from all sorts of data sources such as Googles BigQuery\n",
            "service\n",
            "However reading huge datasets efficiently is not the only difficulty the data also\n",
            "needs to be preprocessed Indeed it is not always composed strictly of convenient\n",
            "numerical fields sometimes there will be text features categorical features and so on\n",
            "To handle this TensorFlow provides the Features API  it lets you easily convert these\n",
            "features to numerical features that can be consumed by your neural network For\n",
            "403example categorical features with a large number of categories such as cities or\n",
            "words can be encoded using embeddings  as we will see an embedding is a trainable\n",
            "dense vector that represents a category\n",
            "Both the Data API and the Features API work seamlessly with\n",
            "tfkeras\n",
            "In this chapter we will cover the Data API the TFRecord format and the Features\n",
            "API in detail We will also take a quick look at a few related projects from Tensor\n",
            "Flows ecosystem\n",
            "TF Transform  tfTransform  makes it possible to write a single preprocessing\n",
            "function that can be run both in batch mode on your full training set before\n",
            "training to speed it up and then exported to a TF Function and incorporated\n",
            "into your trained model so that once it is deployed in production it can take\n",
            "care of preprocessing new instances on the fly\n",
            "TF Datasets TFDS provides a convenient function to download many common\n",
            "datasets of all kinds including large ones like ImageNet and it provides conve\n",
            "nient dataset objects to manipulate them using the Data API\n",
            "So lets get started\n",
            "The Data API\n",
            "The whole Data API revolves around the concept of a dataset  as you might suspect\n",
            "this represents a sequence of data items Usually you will use datasets that gradually\n",
            "read data from disk but for simplicity lets just create a dataset entirely in RAM using\n",
            "tfdataDatasetfromtensorslices \n",
            " X  tfrange10   any data tensor\n",
            " dataset  tfdataDatasetfromtensorslices X\n",
            " dataset\n",
            "TensorSliceDataset shapes  types tfint32\n",
            "The fromtensorslices  function takes a tensor and creates a tfdataDataset\n",
            "whose elements are all the slices of X along the first dimension so this dataset con\n",
            "tains 10 items tensors 0 1 2  9 In this case we would have obtained the same\n",
            "dataset if we had used tfdataDatasetrange10 \n",
            "Y ou can simply iterate over a datasets items like this\n",
            " for item in dataset\n",
            "     printitem\n",
            "404  Chapter 13 Loading and Preprocessing Data with TensorFlow\n",
            "tfTensor0 shape dtypeint32\n",
            "tfTensor1 shape dtypeint32\n",
            "tfTensor2 shape dtypeint32\n",
            "\n",
            "tfTensor9 shape dtypeint32\n",
            "Chaining Transformations\n",
            "Once you have a dataset you can apply all sorts of transformations to it by calling its\n",
            "transformation methods Each method returns a new dataset so you can chain trans\n",
            "formations like this this chain is illustrated in Figure 131 \n",
            " dataset  datasetrepeat3batch7\n",
            " for item in dataset\n",
            "     printitem\n",
            "\n",
            "tfTensor0 1 2 3 4 5 6 shape7 dtypeint32\n",
            "tfTensor7 8 9 0 1 2 3 shape7 dtypeint32\n",
            "tfTensor4 5 6 7 8 9 0 shape7 dtypeint32\n",
            "tfTensor1 2 3 4 5 6 7 shape7 dtypeint32\n",
            "tfTensor8 9 shape2 dtypeint32\n",
            "Figure 131 Chaining Dataset Transformations\n",
            "In this example we first call the repeat  method on the original dataset and it\n",
            "returns a new dataset that will repeat the items of the original dataset 3 times Of\n",
            "course this will not copy the whole data in memory 3 times In fact if you call this\n",
            "method with no arguments the new dataset will repeat the source dataset forever\n",
            "Then we call the batch  method on this new dataset and again this creates a new\n",
            "dataset This one will group the items of the previous dataset in batches of 7 items\n",
            "Finally we iterate over the items of this final dataset As you can see the batch\n",
            "method had to output a final batch of size 2 instead of 7 but you can call it with\n",
            "dropremainderTrue  if you want it to drop this final batch so that all batches have\n",
            "the exact same size\n",
            "The Data API  405The dataset methods do not modify datasets they create new ones\n",
            "so make sure to keep a reference to these new datasets eg data\n",
            "set    or else nothing will happen\n",
            "Y ou can also apply any transformation you want to the items by calling the map\n",
            "method For example this creates a new dataset with all items doubled\n",
            " dataset  datasetmaplambda x x  2  Items 024681012\n",
            "This function is the one you will call to apply any preprocessing you want to your\n",
            "data Sometimes this will include computations that can be quite intensive such as\n",
            "reshaping or rotating an image so you will usually want to spawn multiple threads to\n",
            "speed things up its as simple as setting the numparallelcalls  argument\n",
            "While the map  applies a transformation to each item the apply  method applies a\n",
            "transformation to the dataset as a whole For example the following code unbatches\n",
            "the dataset by applying the unbatch  function to the dataset this function is cur\n",
            "rently experimental but it will most likely move to the core API in a future release\n",
            "Each item in the new dataset will be a single integer tensor instead of a batch of 7\n",
            "integers\n",
            " dataset  datasetapplytfdataexperimental unbatch  Items 024\n",
            "It is also possible to simply filter the dataset using the filter  method\n",
            " dataset  datasetfilterlambda x x  10  Items 0 2 4 6 8 0 2 4 6\n",
            "Y ou will often want to look at just a few items from a dataset Y ou can use the take\n",
            "method for that\n",
            " for item in datasettake3\n",
            "     printitem\n",
            "\n",
            "tfTensor0 shape dtypeint64\n",
            "tfTensor2 shape dtypeint64\n",
            "tfTensor4 shape dtypeint64\n",
            "Shuffling  the Data\n",
            "As you know Gradient Descent works best when the instances in the training set are\n",
            "independent and identically distributed see Chapter 4  A simple way to ensure this\n",
            "is to shuffle the instances For this you can just use the shuffle  method It will\n",
            "create a new dataset that will start by filling up a buffer with the first items of the\n",
            "source dataset then whenever it is asked for an item it will pull one out randomly\n",
            "from the buffer and replace it with a fresh one from the source dataset until it has\n",
            "iterated entirely through the source dataset At this point it continues to pull out\n",
            "items randomly from the buffer until it is empty Y ou must specify the buffer size and\n",
            "406  Chapter 13 Loading and Preprocessing Data with TensorFlow1Imagine a sorted deck of cards on your left suppose you just take the top 3 cards and shuffle them then pick\n",
            "one randomly and put it to your right keeping the other 2 in your hands Take another card on your left\n",
            "shuffle the 3 cards in your hands and pick one of them randomly and put it on your right When you are\n",
            "done going through all the cards like this you will have a deck of cards on your right do you think it will be\n",
            "perfectly shuffled\n",
            "it is important to make it large enough or else shuffling will not be very efficient1\n",
            "However obviously do not exceed the amount of RAM you have and even if you\n",
            "have plenty of it theres no need to go well beyond the datasets size Y ou can provide\n",
            "a random seed if you want the same random order every time you run your program\n",
            " dataset  tfdataDatasetrange10repeat3  0 to 9 three times\n",
            " dataset  datasetshufflebuffersize 5 seed42batch7\n",
            " for item in dataset\n",
            "     printitem\n",
            "\n",
            "tfTensor0 2 3 6 7 9 4 shape7 dtypeint64\n",
            "tfTensor5 0 1 1 8 6 5 shape7 dtypeint64\n",
            "tfTensor4 8 7 1 2 3 0 shape7 dtypeint64\n",
            "tfTensor5 4 2 7 8 9 9 shape7 dtypeint64\n",
            "tfTensor3 6 shape2 dtypeint64\n",
            "If you call repeat  on a shuffled dataset by default it will generate\n",
            "a new order at every iteration This is generally a good idea but if\n",
            "you prefer to reuse the same order at each iteration eg for tests\n",
            "or debugging you can set reshuffleeachiterationFalse \n",
            "For a large dataset that does not fit in memory this simple shufflingbuffer approach\n",
            "may not be sufficient since the buffer will be small compared to the dataset One sol\n",
            "ution is to shuffle the source data itself for example on Linux you can shuffle text\n",
            "files using the shuf  command This will definitely improve shuffling a lot However\n",
            "even if the source data is shuffled you will usually want to shuffle it some more or\n",
            "else the same order will be repeated at each epoch and the model may end up being\n",
            "biased eg due to some spurious patterns present by chance in the source datas\n",
            "order To shuffle the instances some more a common approach is to split the source\n",
            "data into multiple files then read them in a random order during training However\n",
            "instances located in the same file will still end up close to each other To avoid this\n",
            "you can pick multiple files randomly and read them simultaneously interleaving\n",
            "their lines Then on top of that you can add a shuffling buffer using the shuffle\n",
            "method If all this sounds like a lot of work dont worry the Data API actually makes\n",
            "all this possible in just a few lines of code Lets see how to do this\n",
            "The Data API  407Interleaving Lines From Multiple Files\n",
            "First lets suppose that you loaded the California housing dataset you shuffled it\n",
            "unless it was already shuffled you split it into a training set a validation set and a\n",
            "test set then you split each set into many CSV files that each look like this each row\n",
            "contains 8 input features plus the target median house value\n",
            "MedIncHouseAgeAveRoomsAveBedrmsPopulAveOccupLatLongMedianHouseValue\n",
            "35214150304991106514470160593763122431442\n",
            "5327550649000991034640344333369117391687\n",
            "31290754231591513280225083844122981621\n",
            "\n",
            "Lets also suppose trainfilepaths  contains the list of file paths and you also have\n",
            "validfilepaths  and testfilepaths \n",
            " trainfilepaths\n",
            "datasetshousingmytrain00csv datasetshousingmytrain01csv\n",
            "Now lets create a dataset containing only these file paths\n",
            "filepathdataset   tfdataDatasetlistfiles trainfilepaths  seed42\n",
            "By default the listfiles  function returns a dataset that shuffles the file paths In\n",
            "general this is a good thing but you can set shuffleFalse  if you do not want that\n",
            "for some reason\n",
            "Next we can call the interleave  method to read from 5 files at a time and inter\n",
            "leave their lines skipping the first line of each file which is the header row using the\n",
            "skip  method\n",
            "nreaders   5\n",
            "dataset  filepathdataset interleave \n",
            "    lambda filepath  tfdataTextLineDataset filepath skip1\n",
            "    cyclelength nreaders \n",
            "The interleave  method will create a dataset that will pull 5 file paths from the\n",
            "filepathdataset  and for each one it will call the function we gave it a lambda in\n",
            "this example to create a new dataset in this case a TextLineDataset  It will then\n",
            "cycle through these 5 datasets reading one line at a time from each until all datasets\n",
            "are out of items Then it will get the next 5 file paths from the filepathdataset  and\n",
            "interleave them the same way and so on until it runs out of file paths\n",
            "For interleaving to work best it is preferable to have files of identi\n",
            "cal length or else the end of the longest files will not be interleaved\n",
            "408  Chapter 13 Loading and Preprocessing Data with TensorFlowBy default interleave  does not use parallelism it just reads one line at a time\n",
            "from each file sequentially However if you want it to actually read files in parallel\n",
            "you can set the numparallelcalls  argument to the number of threads you want\n",
            "Y ou can even set it to tfdataexperimentalAUTOTUNE  to make TensorFlow choose\n",
            "the right number of threads dynamically based on the available CPU however this is\n",
            "an experimental feature for now Lets look at what the dataset contains now\n",
            " for line in datasettake5\n",
            "     printlinenumpy\n",
            "\n",
            "b420834405323209171846023370374712222782\n",
            "b4181252057013099656920240273373118313215\n",
            "b3687544045244099304570319583404118151625\n",
            "b334563704514009084458032253366712172526\n",
            "b35214150304991106514470160593763122431442\n",
            "These are the first rows ignoring the header row of 5 CSV files chosen randomly\n",
            "Looks good But as you can see these are just byte strings we need to parse them\n",
            "and also scale the data\n",
            "Preprocessing the Data\n",
            "Lets implement a small function that will perform this preprocessing\n",
            "Xmean Xstd    mean and scale of each feature in the training set\n",
            "ninputs   8\n",
            "def preprocess line\n",
            "  defs  0  ninputs   tfconstant  dtypetffloat32\n",
            "  fields  tfiodecodecsv line recorddefaults defs\n",
            "  x  tfstackfields1\n",
            "  y  tfstackfields1\n",
            "  return x  Xmean  Xstd y\n",
            "Lets walk through this code\n",
            "First we assume that you have precomputed the mean and standard deviation of\n",
            "each feature in the training set Xmean  and Xstd  are just 1D tensors or NumPy\n",
            "arrays containing 8 floats one per input feature\n",
            "The preprocess  function takes one CSV line and starts by parsing it For this\n",
            "it uses the tfiodecodecsv  function which takes two arguments the first is\n",
            "the line to parse and the second is an array containing the default value for each\n",
            "column in the CSV file This tells TensorFlow not only the default value for each\n",
            "column but also the number of columns and the type of each column In this\n",
            "example we tell it that all feature columns are floats and missing values should\n",
            "default to 0 but we provide an empty array of type tffloat32  as the default\n",
            "value for the last column the target this tells TensorFlow that this column con\n",
            "The Data API  409tains floats but that there is no default value so it will raise an exception if it\n",
            "encounters a missing value\n",
            "The decodecsv  function returns a list of scalar tensors one per column but\n",
            "we need to return 1D tensor arrays So we call tfstack  on all tensors except\n",
            "for the last one the target this will stack these tensors into a 1D array We then\n",
            "do the same for the target value this makes it a 1D tensor array with a single\n",
            "value rather than a scalar tensor\n",
            "Finally we scale the input features by subtracting the feature means and then\n",
            "dividing by the feature standard deviations and we return a tuple containing the\n",
            "scaled features and the target\n",
            "Lets test this preprocessing function\n",
            " preprocess b420834405323209171846023370374712222782 \n",
            "tfTensor id6227 shape8 dtypefloat32 numpy\n",
            " array 016579159  1216324   005204564 039215982 05277444 \n",
            "        02633488   08543046  13072058  dtypefloat32\n",
            " tfTensor  numpyarray2782 dtypefloat32\n",
            "We can now apply this preprocessing function to the dataset\n",
            "Putting Everything Together\n",
            "To make the code reusable lets put together everything we have discussed so far into\n",
            "a small helper function it will create and return a dataset that will efficiently load Cal\n",
            "ifornia housing data from multiple CSV files then shuffle it preprocess it and batch it\n",
            "see Figure 132 \n",
            "def csvreaderdataset filepaths  repeatNone nreaders 5\n",
            "                       nreadthreads None shufflebuffersize 10000\n",
            "                       nparsethreads 5 batchsize 32\n",
            "    dataset  tfdataDatasetlistfiles filepaths repeatrepeat\n",
            "    dataset  datasetinterleave \n",
            "        lambda filepath  tfdataTextLineDataset filepath skip1\n",
            "        cyclelength nreaders  numparallelcalls nreadthreads \n",
            "    dataset  datasetshuffleshufflebuffersize \n",
            "    dataset  datasetmappreprocess  numparallelcalls nparsethreads \n",
            "    dataset  datasetbatchbatchsize \n",
            "    return datasetprefetch 1\n",
            "410  Chapter 13 Loading and Preprocessing Data with TensorFlow2In general just prefetching one batch is fine but in some cases you may need to prefetch a few more Alterna\n",
            "tively you can let TensorFlow decide automatically by passing tfdataexperimentalAUTOTUNE  this is an\n",
            "experimental feature for now\n",
            "Figure 132 Loading and Preprocessing Data From Multiple CSV Files\n",
            "Everything should make sense in this code except the very last line  prefetch1 \n",
            "which is actually quite important for performance\n",
            "Prefetching\n",
            "By calling prefetch1  at the end we are creating a dataset that will do its best to\n",
            "always be one batch ahead2 In other words while our training algorithm is working\n",
            "on one batch the dataset will already be working in parallel on getting the next batch\n",
            "ready This can improve performance dramatically as is illustrated on Figure 133  If\n",
            "we also ensure that loading and preprocessing are multithreaded by setting numpar\n",
            "allelcalls  when calling interleave  and map  we can exploit multiple cores\n",
            "on the CPU and hopefully make preparing one batch of data shorter than running a\n",
            "training step on the GPU this way the GPU will be almost 100 utilized except for\n",
            "the data transfer time from the CPU to the GPU and training will run much faster\n",
            "The Data API  411Figure 133 Speedup Training Thanks  to Prefetching and Multithreading\n",
            "If you plan to purchase a GPU card its processing power and its\n",
            "memory size are of course very important in particular a large\n",
            "RAM is crucial for computer vision but its memory bandwidth  is\n",
            "just as important as the processing power to get good performance\n",
            "this is the number of gigabytes of data it can get in or out of its\n",
            "RAM per second\n",
            "With that you can now build efficient input pipelines to load and preprocess data\n",
            "from multiple text files We have discussed the most common dataset methods but\n",
            "there are a few more you may want to look at concatenate  zip  window \n",
            "reduce  cache  shard  flatmap  and paddedbatch  There are also a cou\n",
            "ple more class methods fromgenerator  and fromtensors  which create a new\n",
            "dataset from a Python generator or a list of tensors respectively Please check the API\n",
            "documentation for more details Also note that there are experimental features avail\n",
            "able in tfdataexperimental  many of which will most likely make it to the core\n",
            "API in future releases eg check out the CsvDataset  class and the SqlDataset\n",
            "classes\n",
            "412  Chapter 13 Loading and Preprocessing Data with TensorFlow3Support for datasets is specific to tfkeras it will not work on other implementations of the Keras API\n",
            "4The number of steps per epoch is optional if the dataset just goes through the data once but if you do not\n",
            "specify it the progress bar will not be displayed during the first epoch\n",
            "5Note that for now the dataset must be created within the TF Function This may be fixed by the time you read\n",
            "these lines see TensorFlow issue 25414Using the Dataset With tfkeras\n",
            "Now we can use the csvreaderdataset  function to create a dataset for the train\n",
            "ing set ensuring it repeats the data forever the validation set and the test set\n",
            "trainset   csvreaderdataset trainfilepaths  repeatNone\n",
            "validset   csvreaderdataset validfilepaths \n",
            "testset   csvreaderdataset testfilepaths \n",
            "And now we can simply build and train a Keras model using these datasets3 All we\n",
            "need to do is to call the fit  method with the datasets instead of Xtrain  and\n",
            "ytrain  and specify the number of steps per epoch for each set4\n",
            "model  kerasmodelsSequential \n",
            "modelcompile\n",
            "modelfittrainset  stepsperepoch lenXtrain  batchsize  epochs10\n",
            "          validationdata validset \n",
            "          validationsteps lenXvalid  batchsize \n",
            "Similarly we can pass a dataset to the evaluate  and predict  methods and again\n",
            "specify the number of steps per epoch\n",
            "modelevaluate testset  stepslenXtest  batchsize \n",
            "modelpredictnewset stepslenXnew  batchsize \n",
            "Unlike the other sets the newset  will usually not contain labels if it does Keras will\n",
            "just ignore them Note that in all these cases you can still use NumPy arrays instead\n",
            "of datasets if you want but of course they need to have been loaded and preprocessed\n",
            "first\n",
            "If you want to build your own custom training loop as in Chapter 12  you can just\n",
            "iterate over the training set very naturally\n",
            "for Xbatch ybatch in trainset \n",
            "      perform one gradient descent step\n",
            "In fact it is even possible to create a tffunction see Chapter 12  that performs the\n",
            "whole training loop5\n",
            "tffunction\n",
            "def trainmodel optimizer  lossfn nepochs  \n",
            "    trainset   csvreaderdataset trainfilepaths  repeatnepochs  \n",
            "    for Xbatch ybatch in trainset \n",
            "        with tfGradientTape  as tape\n",
            "The Data API  413            ypred  modelXbatch\n",
            "            mainloss   tfreducemean lossfnybatch ypred\n",
            "            loss  tfaddnmainloss   modellosses\n",
            "        grads  tapegradient loss modeltrainablevariables \n",
            "        optimizer applygradients zipgrads modeltrainablevariables \n",
            "Congratulations you now know how to build powerful input pipelines using the Data\n",
            "API However so far we have used CSV files which are common simple and conve\n",
            "nient but they are not really efficient and they do not support large or complex data\n",
            "structures very well such as images or audio So lets use TFRecords instead\n",
            "If you are happy with CSV files or whatever other format you are\n",
            "using you do not have  to use TFRecords As the saying goes if it\n",
            "aint broke dont fix it TFRecords are useful when the bottleneck\n",
            "during training is loading and parsing the data\n",
            "The TFRecord Format\n",
            "The TFRecord format is TensorFlows preferred format for storing large amounts of\n",
            "data and reading it efficiently It is a very simple binary format that just contains a\n",
            "sequence of binary records of varying sizes each record just has a length a CRC\n",
            "checksum to check that the length was not corrupted then the actual data and finally\n",
            "a CRC checksum for the data Y ou can easily create a TFRecord file using the\n",
            "tfioTFRecordWriter  class\n",
            "with tfioTFRecordWriter mydatatfrecord  as f\n",
            "    fwritebThis is the first record \n",
            "    fwritebAnd this is the second record \n",
            "And you can then use a tfdataTFRecordDataset  to read one or more TFRecord\n",
            "files\n",
            "filepaths   mydatatfrecord \n",
            "dataset  tfdataTFRecordDataset filepaths \n",
            "for item in dataset\n",
            "    printitem\n",
            "This will output\n",
            "tfTensorbThis is the first record shape dtypestring\n",
            "tfTensorbAnd this is the second record shape dtypestring\n",
            "By default a TFRecordDataset  will read files one by one but you\n",
            "can make it read multiple files in parallel and interleave their\n",
            "records by setting numparallelreads  Alternatively you could\n",
            "obtain the same result by using listfiles  and interleave\n",
            "as we did earlier to read multiple CSV files\n",
            "414  Chapter 13 Loading and Preprocessing Data with TensorFlow6Since protobuf objects are meant to be serialized and transmitted they are called messages Compressed TFRecord Files\n",
            "It can sometimes be useful to compress your TFRecord files especially if they need to\n",
            "be loaded via a network connection Y ou can create a compressed TFRecord file by\n",
            "setting the options  argument\n",
            "options  tfioTFRecordOptions compressiontype GZIP\n",
            "with tfioTFRecordWriter mycompressedtfrecord  options as f\n",
            "  \n",
            "When reading a compressed TFRecord file you need to specify the compression type\n",
            "dataset  tfdataTFRecordDataset mycompressedtfrecord \n",
            "                                  compressiontype GZIP\n",
            "A Brief Introduction to Protocol Buffers\n",
            "Even though each record can use any binary format you want TFRecord files usually\n",
            "contain serialized Protocol Buffers also called protobufs  This is a portable extensi\n",
            "ble and efficient binary format developed at Google back in 2001 and Open Sourced\n",
            "in 2008 and they are now widely used in particular in gRPC  Googles remote proce\n",
            "dure call system Protocol Buffers are defined using a simple language that looks like\n",
            "this\n",
            "syntax  proto3 \n",
            "message Person \n",
            "  string name  1\n",
            "  int32 id  2\n",
            "  repeated  string email  3\n",
            "\n",
            "This definition says we are using the protobuf format version 3 and it specifies that\n",
            "each Person  object6 may optionally have a name  of type string  an id of type int32 \n",
            "and zero or more email  fields each of type string  The numbers 1 2 and 3 are the\n",
            "field identifiers they will be used in each records binary representation Once you\n",
            "have a definition in a proto  file you can compile it This requires protoc  the proto\n",
            "buf compiler to generate access classes in Python or some other language Note that\n",
            "the protobuf definitions we will use have already been compiled for you and their\n",
            "Python classes are part of TensorFlow so you will not need to use protoc  All you\n",
            "need to know is how to use protobuf access classes in Python To illustrate the basics\n",
            "lets look at a simple example that uses the access classes generated for the Person\n",
            "protobuf the code is explained in the comments\n",
            " from personpb2  import Person   import the generated access class\n",
            " person  PersonnameAl id123 emailabcom    create a Person\n",
            " printperson   display the Person\n",
            "The TFRecord Format  4157This chapter contains the bare minimum you need to know about protobufs to use TFRecords To learn more\n",
            "about protobufs please visit httpshomlinfoprotobuf name Al\n",
            "id 123\n",
            "email abcom\n",
            " personname   read a field\n",
            "Al\n",
            " personname  Alice   modify a field\n",
            " personemail0   repeated fields can be accessed like arrays\n",
            "abcom\n",
            " personemailappendcdcom    add an email address\n",
            " s  personSerializeToString    serialize the object to a byte string\n",
            " s\n",
            "bnx05Alicex10x1ax07abcomx1ax07cdcom\n",
            " person2  Person   create a new Person\n",
            " person2ParseFromString s   parse the byte string 27 bytes long\n",
            "27\n",
            " person  person2   now they are equal\n",
            "True\n",
            "In short we import the Person  class generated by protoc  we create an instance and\n",
            "we play with it visualizing it reading and writing some fields then we serialize it\n",
            "using the SerializeToString  method This is the binary data that is ready to be\n",
            "saved or transmitted over the network When reading or receiving this binary data\n",
            "we can parse it using the ParseFromString  method and we get a copy of the object\n",
            "that was serialized7\n",
            "We could save the serialized Person  object to a TFRecord file then we could load and\n",
            "parse it everything would work fine However SerializeToString  and ParseFrom\n",
            "String  are not TensorFlow operations and neither are the other operations in this\n",
            "code so they cannot be included in a TensorFlow Function except by wrapping\n",
            "them in a tfpyfunction  operation which would make the code slower and less\n",
            "portable as we saw in Chapter 12  Fortunately TensorFlow does include special pro\n",
            "tobuf definitions for which it provides parsing operations\n",
            "TensorFlow Protobufs\n",
            "The main protobuf typically used in a TFRecord file is the Example  protobuf which\n",
            "represents one instance in a dataset It contains a list of named features where each\n",
            "feature can either be a list of byte strings a list of floats or a list of integers Here is the\n",
            "protobuf definition\n",
            "syntax  proto3 \n",
            "message BytesList   repeated  bytes value  1 \n",
            "message FloatList   repeated  float value  1 packed  true \n",
            "message Int64List   repeated  int64 value  1 packed  true \n",
            "416  Chapter 13 Loading and Preprocessing Data with TensorFlow8Why was Example  even defined since it contains no more than a Features  object Well TensorFlow may one\n",
            "day decide to add more fields to it As long as the new Example  definition still contains the features  field\n",
            "with the same id it will be backward compatible This extensibility is one of the great features of protobufsmessage Feature \n",
            "    oneof kind \n",
            "        BytesList  byteslist   1\n",
            "        FloatList  floatlist   2\n",
            "        Int64List  int64list   3\n",
            "    \n",
            "\n",
            "message Features   mapstring Feature feature  1 \n",
            "message Example  Features  features   1 \n",
            "The definitions of BytesList  FloatList  and Int64List  are straightforward enough\n",
            "packed  true  is used for repeated numerical fields for a more efficient encod\n",
            "ing A Feature  either contains a BytesList  a FloatList  or an Int64List  A Fea\n",
            "tures  with an s contains a dictionary that maps a feature name to the\n",
            "corresponding feature value And finally an Example  just contains a Features  object8\n",
            "Here is how you could create a tftrainExample  representing the same person as\n",
            "earlier and write it to TFRecord file\n",
            "from tensorflowtrain  import BytesList  FloatList  Int64List\n",
            "from tensorflowtrain  import Feature Features  Example\n",
            "personexample   Example\n",
            "    features Features \n",
            "        feature\n",
            "            name Featurebyteslist BytesList valuebAlice\n",
            "            id Featureint64list Int64List value123\n",
            "            emails  Featurebyteslist BytesList valuebabcom \n",
            "                                                          b cdcom \n",
            "        \n",
            "The code is a bit verbose and repetitive but its rather straightforward and you could\n",
            "easily wrap it inside a small helper function Now that we have an Example  protobuf\n",
            "we can serialize it by calling its SerializeToString  method then write the result\n",
            "ing data to a TFRecord file\n",
            "with tfioTFRecordWriter mycontactstfrecord  as f\n",
            "    fwritepersonexample SerializeToString \n",
            "Normally you would write much more than just one example Typically you would\n",
            "create a conversion script that reads from your current format say CSV files creates\n",
            "an Example  protobuf for each instance serializes them and saves them to several\n",
            "TFRecord files ideally shuffling them in the process This requires a bit of work so\n",
            "once again make sure it is really necessary perhaps your pipeline works fine with\n",
            "CSV files\n",
            "The TFRecord Format  417Now that we have a nice TFRecord file containing a serialized Example  lets try to\n",
            "load it\n",
            "Loading and Parsing Examples\n",
            "To load the serialized Example  protobufs we will use a tfdataTFRecordDataset\n",
            "once again and we will parse each Example  using tfioparsesingleexample \n",
            "This is a TensorFlow operation so it can be included in a TF Function It requires at\n",
            "least two arguments a string scalar tensor containing the serialized data and a\n",
            "description of each feature The description is a dictionary that maps each feature\n",
            "name to either a tfioFixedLenFeature  descriptor indicating the features shape\n",
            "type and default value or a tfioVarLenFeature  descriptor indicating only the type\n",
            "if the length may vary such as for the emails  feature For example\n",
            "featuredescription   \n",
            "    name tfioFixedLenFeature  tfstring defaultvalue \n",
            "    id tfioFixedLenFeature  tfint64 defaultvalue 0\n",
            "    emails  tfioVarLenFeature tfstring\n",
            "\n",
            "for serializedexample  in tfdataTFRecordDataset mycontactstfrecord \n",
            "    parsedexample   tfioparsesingleexample serializedexample \n",
            "                                                featuredescription \n",
            "The fixed length features are parsed as regular tensors but the variable length fea\n",
            "tures are parsed as sparse tensors Y ou can convert a sparse tensor to a dense tensor\n",
            "using tfsparsetodense  but in this case it is simpler to just access its values\n",
            " tfsparsetodense parsedexample emails  defaultvalue b\n",
            "tfTensor  dtypestring numpyarraybabcom bcdcom \n",
            " parsedexample emails values\n",
            "tfTensor  dtypestring numpyarraybabcom bcdcom \n",
            "A BytesList  can contain any binary data you want including any serialized object\n",
            "For example you can use tfioencodejpeg  to encode an image using the JPEG\n",
            "format and put this binary data in a BytesList  Later when your code reads the\n",
            "TFRecord it will start by parsing the Example  then you will need to call\n",
            "tfiodecodejpeg  to parse the data and get the original image or you can use\n",
            "tfiodecodeimage  which can decode any BMP  GIF JPEG or PNG image Y ou\n",
            "can also store any tensor you want in a BytesList  by serializing the tensor using\n",
            "tfioserializetensor  then putting the resulting byte string in a BytesList\n",
            "feature Later when you parse the TFRecord you can parse this data using\n",
            "tfioparsetensor \n",
            "Instead of parsing examples one by one using tfioparsesingleexample  you\n",
            "may want to parse them batch by batch using tfioparseexample \n",
            "418  Chapter 13 Loading and Preprocessing Data with TensorFlowdataset  tfdataTFRecordDataset mycontactstfrecord batch10\n",
            "for serializedexamples  in dataset\n",
            "    parsedexamples   tfioparseexample serializedexamples \n",
            "                                          featuredescription \n",
            "As you can see the Example  proto will probably be sufficient for most use cases\n",
            "However it may be a bit cumbersome to use when you are dealing with lists of lists\n",
            "For example suppose you want to classify text documents Each document may be\n",
            "represented as a list of sentences where each sentence is represented as a list of\n",
            "words And perhaps each document also has a list of comments where each com\n",
            "ment is also represented as a list of words Moreover there may be some contextual\n",
            "data as well such as the documents author title and publication date TensorFlows\n",
            "SequenceExample  protobuf is designed for such use cases\n",
            "Handling Lists of Lists Using the SequenceExample  Protobuf\n",
            "Here is the definition of the SequenceExample  protobuf\n",
            "message FeatureList   repeated  Feature feature  1 \n",
            "message FeatureLists   mapstring FeatureList  featurelist   1 \n",
            "message SequenceExample  \n",
            "    Features  context  1\n",
            "    FeatureLists  featurelists   2\n",
            "\n",
            "A SequenceExample  contains a Features  object for the contextual data and a Fea\n",
            "tureLists  object which contains one or more named FeatureList  objects eg a\n",
            "FeatureList  named content  and another named comments  Each FeatureList\n",
            "just contains a list of Feature  objects each of which may be a list of byte strings a list\n",
            "of 64bit integers or a list of floats in this example each Feature  would represent a\n",
            "sentence or a comment perhaps in the form of a list of word identifiers Building a\n",
            "SequenceExample  serializing it and parsing it is very similar to building serializing\n",
            "and parsing an Example  but you must use tfioparsesinglesequenceexam\n",
            "ple  to parse a single SequenceExample  or tfioparsesequenceexample  to\n",
            "parse a batch and both functions return a tuple containing the context features as a\n",
            "dictionary and the feature lists also as a dictionary If the feature lists contain\n",
            "sequences of varying sizes as in the example above you may want to convert them\n",
            "to ragged tensors using tfRaggedTensorfromsparse  see the notebook for the\n",
            "full code\n",
            "parsedcontext  parsedfeaturelists   tfioparsesinglesequenceexample \n",
            "    serializedsequenceexample  contextfeaturedescriptions \n",
            "    sequencefeaturedescriptions \n",
            "parsedcontent   tfRaggedTensor fromsparse parsedfeaturelists content \n",
            "Now that you know how to efficiently store load and parse data the next step is to\n",
            "prepare it so that it can be fed to a neural network This means converting all features\n",
            "The TFRecord Format  419into numerical features ideally not too sparse scaling them and more In particular\n",
            "if your data contains categorical features or text features they need to be converted to\n",
            "numbers For this the Features API  can help\n",
            "The Features API\n",
            "Preprocessing your data can be performed in many ways it can be done ahead of\n",
            "time when preparing your data files using any tool you like Or you can preprocess\n",
            "your data on the fly when loading it with the Data API eg using the datasets map\n",
            "method as we saw earlier Or you can include a preprocessing layer directly in your\n",
            "model Whichever solution you prefer the Features API can help you it is a set of\n",
            "functions available in the tffeaturecolumn  package which let you define how\n",
            "each feature or group of features in your data should be preprocessed therefore you\n",
            "can think of this API as the analog of ScikitLearns ColumnTransformer  class We\n",
            "will start by looking at the different types of columns available and then we will look\n",
            "at how to use them\n",
            "Lets go back to the variant of the California housing dataset that we used in Chap\n",
            "ter 2  since it includes a categorical feature and missing data Here is a simple numeri\n",
            "cal column named housingmedianage \n",
            "housingmedianage   tffeaturecolumn numericcolumn housingmedianage \n",
            "Numeric columns let you specify a normalization function using the normalizerfn\n",
            "argument For example lets tweak the housingmedianage  column to define how\n",
            "it should be scaled Note that this requires computing ahead of time the mean and\n",
            "standard deviation of this feature in the training set\n",
            "agemean  agestd  Xmean1 Xstd1   The median age is column in 1\n",
            "housingmedianage   tffeaturecolumn numericcolumn \n",
            "    housingmedianage  normalizerfn lambda x x  agemean   agestd\n",
            "In some cases it might improve performance to bucketize some numerical features\n",
            "effectively transforming a numerical feature into a categorical feature For example\n",
            "lets create a bucketized column based on the medianincome  column with 5 buckets\n",
            "less than 15 15000 then 15 to 3 3 to 45 45 to 6 and above 6 notice that when\n",
            "you specify 4 boundaries there are actually 5 buckets\n",
            "medianincome   tffeaturecolumn numericcolumn medianincome \n",
            "bucketizedincome   tffeaturecolumn bucketizedcolumn \n",
            "    medianincome  boundaries 15 3 45 6\n",
            "If the medianincome  feature is equal to say 32 then the bucketizedincome  feature\n",
            "will automatically be equal to 2 ie the index of the corresponding income bucket\n",
            "Choosing the right boundaries can be somewhat of an art but one approach is to just\n",
            "use percentiles of the data eg the 10th percentile the 20th percentile and so on If\n",
            "a feature is multimodal  meaning it has separate peaks in its distribution you may\n",
            "420  Chapter 13 Loading and Preprocessing Data with TensorFlowwant to define a bucket for each mode placing the boundaries in between the peaks\n",
            "Whether you use the percentiles or the modes you need to analyze the distribution of\n",
            "your data ahead of time just like we had to measure the mean and standard deviation\n",
            "ahead of time to normalize the housingmedianage  column\n",
            "Categorical Features\n",
            "For categorical features such as oceanproximity  there are several options If it is\n",
            "already represented as a category ID ie an integer from 0 to the max ID then you\n",
            "can use the categoricalcolumnwithidentity  function specifying the max\n",
            "ID If not and you know the list of all possible categories then you can use categori\n",
            "calcolumnwithvocabularylist \n",
            "oceanproxvocab   1H OCEAN  INLAND  ISLAND  NEAR BAY  NEAR OCEAN \n",
            "oceanproximity   tffeaturecolumn categoricalcolumnwithvocabularylist \n",
            "    oceanproximity  oceanproxvocab \n",
            "If you prefer to have TensorFlow load the vocabulary from a file you can call catego\n",
            "ricalcolumnwithvocabularyfile  instead As you might expect these two\n",
            "functions will simply map each category to its index in the vocabulary eg NEAR\n",
            "BAY  will be mapped to 3 and unknown categories will be mapped to 1\n",
            "For categorical columns with a large vocabulary eg for zipcodes cities words\n",
            "products users etc it may not be convenient to get the full list of possible cate\n",
            "gories or perhaps categories may be added or removed so frequently that using cate\n",
            "gory indices would be too unreliable In this case you may prefer to use a\n",
            "categoricalcolumnwithhashbucket  If we had a city  feature in the dataset\n",
            "we could encode it like this\n",
            "cityhash   tffeaturecolumn categoricalcolumnwithhashbucket \n",
            "    city hashbucketsize 1000\n",
            "This feature will compute a hash for each category ie for each city modulo the\n",
            "number of hash buckets  hashbucketsize  Y ou must set the number of buckets\n",
            "high enough to avoid getting too many collisions ie different categories ending up\n",
            "in the same bucket but the higher you set it the more RAM will be used by the\n",
            "embedding table as we will see shortly\n",
            "Crossed Categorical Features\n",
            "If you suspect that two or more categorical features are more meaningful when used\n",
            "jointly then you can create a crossed column  For example suppose people are partic\n",
            "ularly fond of old houses inland and new houses near the ocean then it might help to\n",
            "The Features API  4219Since the housingmedianage  feature was normalized the boundaries are for normalized agescreate a bucketized column for the housingmedianage  feature9 and cross it with\n",
            "the oceanproximity  column The crossed column will compute a hash of every age\n",
            " ocean proximity combination it comes across modulo the hashbucketsize  and\n",
            "this will give it the cross category ID Y ou may then choose to use only this crossed\n",
            "column in your model or also include the individual columns\n",
            "bucketizedage   tffeaturecolumn bucketizedcolumn \n",
            "    housingmedianage  boundaries 1 05 0 05 1  age was scaled\n",
            "ageandoceanproximity   tffeaturecolumn crossedcolumn \n",
            "    bucketizedage  oceanproximity  hashbucketsize 100\n",
            "Another common use case for crossed columns is to cross latitude and longitude into\n",
            "a single categorical feature you start by bucketizing the latitude and longitude for\n",
            "example into 20 buckets each then you cross these bucketized features into a loca\n",
            "tion  column This will create a 2020 grid over California and each cell in the grid\n",
            "will correspond to one category\n",
            "latitude   tffeaturecolumn numericcolumn latitude \n",
            "longitude   tffeaturecolumn numericcolumn longitude \n",
            "bucketizedlatitude   tffeaturecolumn bucketizedcolumn \n",
            "    latitude  boundaries listnplinspace 32 42 20  1\n",
            "bucketizedlongitude   tffeaturecolumn bucketizedcolumn \n",
            "    longitude  boundaries listnplinspace 125 114 20  1\n",
            "location   tffeaturecolumn crossedcolumn \n",
            "    bucketizedlatitude  bucketizedlongitude  hashbucketsize 1000\n",
            "Encoding Categorical Features Using OneHot Vectors\n",
            "No matter which option you choose to build a categorical feature categorical col\n",
            "umns bucketized columns or crossed columns it must be encoded before you can\n",
            "feed it to a neural network There are two options to encode a categorical feature\n",
            "onehot vectors or embeddings  For the first option simply use the indicatorcol\n",
            "umn  function\n",
            "oceanproximityonehot   tffeaturecolumn indicatorcolumn oceanproximity \n",
            "A onehot vector encoding has the size of the vocabulary length which is fine if there\n",
            "are just a few possible categories but if the vocabulary is large you will end up with\n",
            "too many inputs fed to your neural network it will have too many weights to learn\n",
            "and it will probably not perform very well In particular this will typically be the case\n",
            "when you use hash buckets In this case you should probably encode them using\n",
            "embeddings  instead\n",
            "422  Chapter 13 Loading and Preprocessing Data with TensorFlowAs a rule of thumb but your mileage may vary if the number of\n",
            "categories is lower than 10 then onehot encoding is generally the\n",
            "way to go If the number of categories is greater than 50 which is\n",
            "often the case when you use hash buckets then embeddings are\n",
            "usually preferable In between 10 and 50 categories you may want\n",
            "to experiment with both options and see which one works best for\n",
            "your use case Also embeddings typically require more training\n",
            "data unless you can reuse pretrained embeddings\n",
            "Encoding Categorical Features Using Embeddings\n",
            "An embedding is a trainable dense vector that represents a category By default\n",
            "embeddings are initialized randomly so for example the NEAR BAY  category could\n",
            "be represented initially by a random vector such as 0131 0890  while the NEAR\n",
            "OCEAN  category may be represented by another random vector such as 0631\n",
            "0791  in this example we are using 2D embeddings but the number of dimensions\n",
            "is a hyperparameter you can tweak Since these embeddings are trainable they will\n",
            "gradually improve during training and as they represent fairly similar categories\n",
            "Gradient Descent will certainly end up pushing them closer together while it will\n",
            "tend to move them away from the INLAND  categorys embedding see Figure 134 \n",
            "Indeed the better the representation the easier it will be for the neural network to\n",
            "make accurate predictions so training tends to make embeddings useful representa\n",
            "tions of the categories This is called representation learning  we will see other types of\n",
            "representation learning in \n",
            "The Features API  42310Distributed Representations of Words and Phrases and their Compositionality  T Mikolov et al 2013\n",
            "Figure 134 Embeddings Will Gradually Improve During Training\n",
            "Word Embeddings\n",
            "Not only will embeddings generally be useful representations for the task at hand but\n",
            "quite often these same embeddings can be reused successfully for other tasks as well\n",
            "The most common example of this is word embeddings  ie embeddings of individual\n",
            "words when you are working on a natural language processing task you are often\n",
            "better off reusing pretrained word embeddings than training your own The idea of\n",
            "using vectors to represent words dates back to the 1960s and many sophisticated\n",
            "techniques have been used to generate useful vectors including using neural net\n",
            "works but things really took off in 2013 when Tom Mikolov and other Google\n",
            "researchers published a paper10 describing how to learn word embeddings using deep\n",
            "neural networks much faster than previous attempts This allowed them to learn\n",
            "embeddings on a very large corpus of text they trained a deep neural network to pre\n",
            "dict the words near any given word This allowed them to obtain astounding word\n",
            "embeddings For example synonyms had very close embeddings and semantically\n",
            "related words such as France Spain Italy and so on ended up clustered together But\n",
            "its not just about proximity word embeddings were also organized along meaningful\n",
            "axes in the embedding space Here is a famous example if you compute King  Man\n",
            " Woman adding and subtracting the embedding vectors of these words then the\n",
            "result will be very close to the embedding of the word Queen see Figure 135  In\n",
            "other words the word embeddings encode the concept of gender Similarly you can\n",
            "compute Madrid  Spain  France and of course the result is close to Paris which\n",
            "seems to show that the notion of capital city was also encoded in the embeddings\n",
            "424  Chapter 13 Loading and Preprocessing Data with TensorFlowFigure 135 Word Embeddings\n",
            "Lets go back to the Features API Here is how you could encode the oceanproxim\n",
            "ity categories as 2D embeddings\n",
            "oceanproximityembed   tffeaturecolumn embeddingcolumn oceanproximity \n",
            "                                                           dimension 2\n",
            "Each of the five oceanproximity  categories will now be represented as a 2D vector\n",
            "These vectors are stored in an embedding matrix  with one row per category and one\n",
            "column per embedding dimension so in this example it is a 52 matrix When an\n",
            "embedding column is given a category index as input say 3 which corresponds to\n",
            "the category NEAR BAY  it just performs a lookup in the embedding matrix and\n",
            "returns the corresponding row say 0331 0190  Unfortunately the embedding\n",
            "matrix can be quite large especially when you have a large vocabulary if this is the\n",
            "case the model can only learn good representations for the categories for which it has\n",
            "sufficient training data To reduce the size of the embedding matrix you can of\n",
            "course try lowering the dimension  hyperparameter but if you reduce this parameter\n",
            "too much the representations may not be as good Another option is to reduce the\n",
            "vocabulary size eg if you are dealing with text you can try dropping the rare words\n",
            "from the vocabulary and replace them all with a token like unknown  or UNK \n",
            "If you are using hash buckets you can also try reducing the hashbucketsize  but\n",
            "not too much or else you will get collisions\n",
            "The Features API  425If there are no pretrained embeddings that you can reuse for the\n",
            "task you are trying to tackle and if you do not have enough train\n",
            "ing data to learn them then you can try to learn them on some\n",
            "auxiliary task for which it is easier to obtain plenty of training data\n",
            "After that you can reuse the trained embeddings for your main\n",
            "task\n",
            "Using Feature Columns for Parsing\n",
            "Lets suppose you have created feature columns for each of your input features as well\n",
            "as for the target What can you do with them Well for one you can pass them to the\n",
            "makeparseexamplespec  function to generate feature descriptions so you dont\n",
            "have to do it manually as we did earlier\n",
            "columns  bucketizedage   medianhousevalue   all features  target\n",
            "featuredescriptions   tffeaturecolumn makeparseexamplespec columns\n",
            "Y ou dont always have to create a separate feature column for each\n",
            "and every feature For example instead of having 2 numerical fea\n",
            "ture columns you could choose to have a single 2D column just\n",
            "set shape2  when calling numericalcolumn \n",
            "Y ou can then create a function that parses serialized examples using these feature\n",
            "descriptions and separates the target column from the input features\n",
            "def parseexamples serializedexamples \n",
            "    examples   tfioparseexample serializedexamples  featuredescriptions \n",
            "    targets  examples popmedianhousevalue   separate the targets\n",
            "    return examples  targets\n",
            "Next you can create a TFRecordDataset  that will read batches of serialized examples\n",
            "assuming the TFRecord file contains serialized Example  protobufs with the appropri\n",
            "ate features\n",
            "batchsize   32\n",
            "dataset  tfdataTFRecordDataset mydatawithfeaturestfrecords \n",
            "dataset  datasetrepeatshuffle10000batchbatchsize mapparseexamples \n",
            "Using Feature Columns in Your Models\n",
            "Feature columns can also be used directly in your model to convert all your input\n",
            "features into a single dense vector which the neural network can then process For\n",
            "this all you need to do is add a keraslayersDenseFeatures  layer as the first layer\n",
            "in your model passing it the list of feature columns excluding the target column\n",
            "columnswithouttarget   columns1\n",
            "model  kerasmodelsSequential \n",
            "    keraslayersDenseFeatures featurecolumns columnswithouttarget \n",
            "426  Chapter 13 Loading and Preprocessing Data with TensorFlow    keraslayersDense1\n",
            "\n",
            "modelcompilelossmse optimizer sgd metricsaccuracy \n",
            "stepsperepoch   lenXtrain  batchsize\n",
            "history  modelfitdataset stepsperepoch stepsperepoch  epochs5\n",
            "The DenseFeatures  layer will take care of converting every input feature to a dense\n",
            "representation and it will also apply any extra transformation we specified such as\n",
            "scaling the housingmedianage  using the normalizerfn  function we provided Y ou\n",
            "can take a closer look at what the DenseFeatures  layer does by calling it directly\n",
            " somecolumns   oceanproximityembed  bucketizedincome \n",
            " densefeatures   keraslayersDenseFeatures somecolumns \n",
            " densefeatures \n",
            "     oceanproximity  NEAR OCEAN  INLAND  INLAND \n",
            "     medianincome  3 72 1\n",
            " \n",
            "\n",
            "tfTensor id559790 shape3 7 dtypefloat32 numpy\n",
            "array 0  0  1  0  0 036277947  030109018\n",
            "        0  0  0  0  1  022548223  033142096\n",
            "        1  0  0  0  0  022548223  033142096 dtypefloat32\n",
            "In this example we create a DenseFeatures  layer with just two columns and we call\n",
            "it with some data in the form of a dictionary of features In this case since the bucke\n",
            "tizedincome  column relies on the medianincome  column the dictionary must\n",
            "include the medianincome  key and similarly since the oceanproximityembed\n",
            "column is based on the oceanproximity  column the dictionary must include the\n",
            "oceanproximity  key Columns are handled in alphabetical order so first we look\n",
            "at the bucketized income column its name is the same as the medianincome  column\n",
            "name plus bucketized  The incomes 3 72 and 1 get mapped respectively to cat\n",
            "egory 2 for incomes between 15 and 3 category 0 for incomes below 15 and cat\n",
            "egory 4 for incomes greater than 6 Then these category IDs get onehot encoded\n",
            "category 2 gets encoded as 0 0 1 0 0  and so on note that bucketized\n",
            "columns get onehot encoded by default no need to call indicatorcolumn  Now\n",
            "on to the oceanproximityembed  column The NEAR OCEAN  and INLAND  cate\n",
            "gories just get mapped to their respective embeddings which were initialized ran\n",
            "domly The resulting tensor is the concatenation of the onehot vectors and the\n",
            "embeddings\n",
            "Now you can feed all kinds of features to a neural network including numerical fea\n",
            "tures categorical features and even text by splitting the text into words then using\n",
            "word embedding However performing all the preprocessing on the fly can slow\n",
            "down training Lets see how this can be improved\n",
            "The Features API  427TF Transform\n",
            "If preprocessing is computationally expensive then handling it before training rather\n",
            "than on the fly may give you a significant speedup the data will be preprocessed just\n",
            "once per instance before  training rather than once per instance and per epoch during\n",
            "training Tools like Apache Beam let you run efficient data processing pipelines over\n",
            "large amounts of data even distributed across multiple servers so why not use it to\n",
            "preprocess all the training data This works great and indeed can speed up training\n",
            "but there is one problem once your model is trained suppose you want to deploy it\n",
            "to a mobile app you will need to write some code in your app to take care of prepro\n",
            "cessing the data before it is fed to the model And suppose you also want to deploy\n",
            "the model to TensorFlowjs so it runs in a web browser Once again you will need to\n",
            "write some preprocessing code This can become a maintenance nightmare when\n",
            "ever you want to change the preprocessing logic you will need to update your Apache\n",
            "Beam code your mobile app code and your Javascript code It is not only time con\n",
            "suming but also error prone you may end up with subtle differences between the\n",
            "preprocessing operations performed before training and the ones performed in your\n",
            "app or in the browser This trainingserving skew  will lead to bugs or degraded perfor\n",
            "mance\n",
            "One improvement would be to take the trained model trained on data that was pre\n",
            "processed by your Apache Beam code and before deploying it to your app or the\n",
            "browser add an extra input layer to take care of preprocessing on the fly either by\n",
            "writing a custom layer or by using a DenseFeatures  layer Thats definitely better\n",
            "since now you just have two versions of your preprocessing code the Apache Beam\n",
            "code and the preprocessing layers code\n",
            "But what if you could define your preprocessing operations just once This is what\n",
            "TF Transform was designed for It is part of TensorFlow Extended  TFX an endto\n",
            "end platform for productionizing TensorFlow models First to use a TFX component\n",
            "such as TF Transform you must install it it does not come bundled with TensorFlow\n",
            "Y ou define your preprocessing function just once in Python by using TF Transform\n",
            "functions for scaling bucketizing crossing features and more Y ou can also use any\n",
            "TensorFlow operation you need Here is what this preprocessing function might look\n",
            "like if we just had two features\n",
            "import tensorflowtransform  as tft\n",
            "def preprocess inputs   inputs is a batch of input features\n",
            "    medianage   inputshousingmedianage \n",
            "    oceanproximity   inputsoceanproximity \n",
            "    standardizedage   tftscaletozscore medianage   tftmeanmedianage \n",
            "    oceanproximityid   tftcomputeandapplyvocabulary oceanproximity \n",
            "    return \n",
            "        standardizedmedianage  standardizedage \n",
            "428  Chapter 13 Loading and Preprocessing Data with TensorFlow11At the time of writing TFDS requires you to download a few files manually for ImageNet for legal reasons\n",
            "but this will hopefully get resolved soon\n",
            "        oceanproximityid  oceanproximityid\n",
            "    \n",
            "Next TF Transform lets you apply this preprocess  function to the whole training\n",
            "set using Apache Beam it provides an AnalyzeAndTransformDataset  class that you\n",
            "can use for this purpose in your Apache Beam pipeline In the process it will also\n",
            "compute all the necessary statistics over the whole training set in this example the\n",
            "mean and standard deviation of the housingmedianage  feature and the vocabulary\n",
            "for the oceanproximity  feature The components that compute these statistics are\n",
            "called analyzers \n",
            "Importantly TF Transform will also generate an equivalent TensorFlow Function that\n",
            "you can plug into the model you deploy This TF Function contains all the necessary\n",
            "statistics computed by Apache Beam the mean standard deviation and vocabulary\n",
            "simply included as constants\n",
            "At the time of this writing TF Transform only supports Tensor\n",
            "Flow 1 Moreover Apache Beam only has partial support for\n",
            "Python 3 That said both these limitations will likely be fixed by\n",
            "the time your read this\n",
            "With the Data API TFRecords the Features API and TF Transform you can build\n",
            "highly scalable input pipelines for training and also benefit from fast and portable\n",
            "data preprocessing in production\n",
            "But what if you just wanted to use a standard dataset Well in that case things are\n",
            "much simpler just use TFDS\n",
            "The TensorFlow Datasets TFDS Project\n",
            "The TensorFlow Datasets  project makes it trivial to download common datasets from\n",
            "small ones like MNIST or Fashion MNIST to huge datasets like ImageNet11 you will\n",
            "need quite a bit of disk space The list includes image datasets text datasets includ\n",
            "ing translation datasets audio and video datasets and more Y ou can visit https\n",
            "homlinfotfds  to view the full list along with a description of each dataset\n",
            "TFDS is not bundled with TensorFlow so you need to install the tensorflow\n",
            "datasets  library eg using pip Then all you need to do is call the tfdsload\n",
            "function and it will download the data you want unless it was already downloaded\n",
            "earlier and return the data as a dictionary of Datasets  typically one for training\n",
            "The TensorFlow Datasets TFDS Project  429and one for testing but this depends on the dataset you choose For example lets\n",
            "download MNIST\n",
            "import tensorflowdatasets  as tfds\n",
            "dataset  tfdsloadnamemnist\n",
            "mnisttrain  mnisttest   datasettrain datasettest\n",
            "Y ou can then apply any transformation you want typically repeating batching and\n",
            "prefetching and youre ready to train your model Here is a simple example\n",
            "mnisttrain   mnisttrain repeat5batch32prefetch 1\n",
            "for item in mnisttrain \n",
            "    images  itemimage\n",
            "    labels  itemlabel\n",
            "    \n",
            "In general load  returns a shuffled training set so theres no need\n",
            "to shuffle it some more\n",
            "Note that each item in the dataset is a dictionary containing both the features and the\n",
            "labels But Keras expects each item to be a tuple containing 2 elements again the fea\n",
            "tures and the labels Y ou could transform the dataset using the map  method like\n",
            "this\n",
            "mnisttrain   mnisttrain repeat5batch32\n",
            "mnisttrain   mnisttrain maplambda items itemsimage itemslabel\n",
            "mnisttrain   mnisttrain prefetch 1\n",
            "Or you can just ask the load  function to do this for you by setting assuper\n",
            "visedTrue  obviously this works only for labeled datasets Y ou can also specify the\n",
            "batch size if you want Then the dataset can be passed directly to your tfkeras model\n",
            "dataset  tfdsloadnamemnist batchsize 32 assupervised True\n",
            "mnisttrain   datasettrainrepeatprefetch 1\n",
            "model  kerasmodelsSequential \n",
            "modelcompilelosssparsecategoricalcrossentropy  optimizer sgd\n",
            "modelfitmnisttrain  stepsperepoch 60000  32 epochs5\n",
            "This was quite a technical chapter and you may feel that it is a bit far from the\n",
            "abstract beauty of neural networks but the fact is deep learning often involves large\n",
            "amounts of data and knowing how to load parse and preprocess it efficiently is a\n",
            "crucial skill to have In the next chapter we will look at Convolutional Neural Net\n",
            "works which are among the most successful neural net architectures for image pro\n",
            "cessing and many other applications\n",
            "430  Chapter 13 Loading and Preprocessing Data with TensorFlowCHAPTER 14\n",
            "Deep Computer Vision Using Convolutional\n",
            "Neural Networks\n",
            "With Early Release ebooks you get books in their earliest form\n",
            "the authors raw and unedited content as he or she writesso you\n",
            "can take advantage of these technologies long before the official\n",
            "release of these titles The following will be Chapter 14 in the final\n",
            "release of the book\n",
            "Although IBMs Deep Blue supercomputer beat the chess world champion Garry Kas\n",
            "parov back in 1996 it wasnt until fairly recently that computers were able to reliably\n",
            "perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\n",
            "spoken words Why are these tasks so effortless to us humans The answer lies in the\n",
            "fact that perception largely takes place outside the realm of our consciousness within\n",
            "specialized visual auditory and other sensory modules in our brains By the time\n",
            "sensory information reaches our consciousness it is already adorned with highlevel\n",
            "features for example when you look at a picture of a cute puppy you cannot choose\n",
            "not to see the puppy or not to notice its cuteness Nor can you explain how you rec\n",
            "ognize a cute puppy its just obvious to you Thus we cannot trust our subjective\n",
            "experience perception is not trivial at all and to understand it we must look at how\n",
            "the sensory modules work\n",
            "Convolutional neural networks CNNs emerged from the study of the brains visual\n",
            "cortex and they have been used in image recognition since the 1980s In the last few\n",
            "years thanks to the increase in computational power the amount of available training\n",
            "data and the tricks presented in Chapter 11  for training deep nets CNNs have man\n",
            "aged to achieve superhuman performance on some complex visual tasks They power\n",
            "image search services selfdriving cars automatic video classification systems and\n",
            "more Moreover CNNs are not restricted to visual perception they are also successful\n",
            "4311Single Unit Activity in Striate Cortex of Unrestrained Cats  D Hubel and T Wiesel 1958\n",
            "2Receptive Fields of Single Neurones in the Cats Striate Cortex  D Hubel and T Wiesel 1959\n",
            "3Receptive Fields and Functional Architecture of Monkey Striate Cortex  D Hubel and T Wiesel 1968at many other tasks such as voice recognition  or natural language processing  NLP\n",
            "however we will focus on visual applications for now\n",
            "In this chapter we will present where CNNs came from what their building blocks\n",
            "look like and how to implement them using TensorFlow and Keras Then we will dis\n",
            "cuss some of the best CNN architectures and discuss other visual tasks including\n",
            "object detection  classifying multiple objects in an image and placing bounding boxes\n",
            "around them and semantic segmentation  classifying each pixel according to the class\n",
            "of the object it belongs to\n",
            "The Architecture of the Visual Cortex\n",
            "David H Hubel and Torsten Wiesel performed a series of experiments on cats in\n",
            "19581 and 19592 and a few years later on monkeys3 giving crucial insights on the\n",
            "structure of the visual cortex the authors received the Nobel Prize in Physiology or\n",
            "Medicine in 1981 for their work In particular they showed that many neurons in\n",
            "the visual cortex have a small local receptive field meaning they react only to visual\n",
            "stimuli located in a limited region of the visual field see Figure 141  in which the\n",
            "local receptive fields of five neurons are represented by dashed circles The receptive\n",
            "fields of different neurons may overlap and together they tile the whole visual field\n",
            "Moreover the authors showed that some neurons react only to images of horizontal\n",
            "lines while others react only to lines with different orientations two neurons may\n",
            "have the same receptive field but react to different line orientations They also\n",
            "noticed that some neurons have larger receptive fields and they react to more com\n",
            "plex patterns that are combinations of the lowerlevel patterns These observations\n",
            "led to the idea that the higherlevel neurons are based on the outputs of neighboring\n",
            "lowerlevel neurons in Figure 141  notice that each neuron is connected only to a\n",
            "few neurons from the previous layer This powerful architecture is able to detect all\n",
            "sorts of complex patterns in any area of the visual field\n",
            "432  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks4Neocognitron A Selforganizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\n",
            "by Shift in Position  K Fukushima 1980\n",
            "5GradientBased Learning Applied to Document Recognition  Y  LeCun et al 1998\n",
            "Figure 141 Local receptive fields  in the visual cortex\n",
            "These studies of the visual cortex inspired the neocognitron introduced in 1980 4\n",
            "which gradually evolved into what we now call convolutional neural networks  An\n",
            "important milestone was a 1998 paper5 by Y ann LeCun Lon Bottou Y oshua Bengio\n",
            "and Patrick Haffner which introduced the famous LeNet5  architecture widely used\n",
            "to recognize handwritten check numbers This architecture has some building blocks\n",
            "that you already know such as fully connected layers and sigmoid activation func\n",
            "tions but it also introduces two new building blocks convolutional layers  and pooling\n",
            "layers  Lets look at them now\n",
            "Why not simply use a regular deep neural network with fully con\n",
            "nected layers for image recognition tasks Unfortunately although\n",
            "this works fine for small images eg MNIST it breaks down for\n",
            "larger images because of the huge number of parameters it\n",
            "requires For example a 100  100 image has 10000 pixels and if\n",
            "the first layer has just 1000 neurons which already severely\n",
            "restricts the amount of information transmitted to the next layer\n",
            "this means a total of 10 million connections And thats just the first\n",
            "layer CNNs solve this problem using partially connected layers and\n",
            "weight sharing\n",
            "The Architecture of the Visual Cortex  4336A convolution is a mathematical operation that slides one function over another and measures the integral of\n",
            "their pointwise multiplication It has deep connections with the Fourier transform and the Laplace transform\n",
            "and is heavily used in signal processing Convolutional layers actually use crosscorrelations which are very\n",
            "similar to convolutions see httpshomlinfo76  for more details\n",
            "Convolutional Layer\n",
            "The most important building block of a CNN is the convolutional layer 6 neurons in\n",
            "the first convolutional layer are not connected to every single pixel in the input image\n",
            "like they were in previous chapters but only to pixels in their receptive fields see\n",
            "Figure 142  In turn each neuron in the second convolutional layer is connected\n",
            "only to neurons located within a small rectangle in the first layer This architecture\n",
            "allows the network to concentrate on small lowlevel features in the first hidden layer\n",
            "then assemble them into larger higherlevel features in the next hidden layer and so\n",
            "on This hierarchical structure is common in realworld images which is one of the\n",
            "reasons why CNNs work so well for image recognition\n",
            "Figure 142 CNN layers with rectangular local receptive fields\n",
            "Until now all multilayer neural networks we looked at had layers\n",
            "composed of a long line of neurons and we had to flatten input\n",
            "images to 1D before feeding them to the neural network Now each\n",
            "layer is represented in 2D which makes it easier to match neurons\n",
            "with their corresponding inputs\n",
            "A neuron located in row i column j of a given layer is connected to the outputs of the\n",
            "neurons in the previous layer located in rows i to i  fh  1 columns j to j  fw  1\n",
            "where fh and fw are the height and width of the receptive field see Figure 143  In\n",
            "order for a layer to have the same height and width as the previous layer it is com\n",
            "434  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksmon to add zeros around the inputs as shown in the diagram This is called zero pad\n",
            "ding\n",
            "Figure 143 Connections between layers and zero padding\n",
            "It is also possible to connect a large input layer to a much smaller layer by spacing out\n",
            "the receptive fields as shown in Figure 144  The shift from one receptive field to the\n",
            "next is called the stride  In the diagram a 5  7 input layer plus zero padding is con\n",
            "nected to a 3  4 layer using 3  3 receptive fields and a stride of 2 in this example\n",
            "the stride is the same in both directions but it does not have to be so A neuron loca\n",
            "ted in row i column j in the upper layer is connected to the outputs of the neurons in\n",
            "the previous layer located in rows i  sh to i  sh  fh  1 columns j  sw to j  sw  fw \n",
            "1 where sh and sw are the vertical and horizontal strides\n",
            "Convolutional Layer  435Figure 144 Reducing dimensionality using a stride of 2\n",
            "Filters\n",
            "A neurons weights can be represented as a small image the size of the receptive field\n",
            "For example Figure 145  shows two possible sets of weights called filters  or convolu\n",
            "tion kernels  The first one is represented as a black square with a vertical white line in\n",
            "the middle it is a 7  7 matrix full of 0s except for the central column which is full of\n",
            "1s neurons using these weights will ignore everything in their receptive field except\n",
            "for the central vertical line since all inputs will get multiplied by 0 except for the\n",
            "ones located in the central vertical line The second filter is a black square with a\n",
            "horizontal white line in the middle Once again neurons using these weights will\n",
            "ignore everything in their receptive field except for the central horizontal line\n",
            "Now if all neurons in a layer use the same vertical line filter and the same bias term\n",
            "and you feed the network the input image shown in Figure 145  bottom image the\n",
            "layer will output the topleft image Notice that the vertical white lines get enhanced\n",
            "while the rest gets blurred Similarly the upperright image is what you get if all neu\n",
            "rons use the same horizontal line filter notice that the horizontal white lines get\n",
            "enhanced while the rest is blurred out Thus a layer full of neurons using the same\n",
            "filter outputs a feature map  which highlights the areas in an image that activate the\n",
            "filter the most Of course you do not have to define the filters manually instead dur\n",
            "ing training the convolutional layer will automatically learn the most useful filters for\n",
            "its task and the layers above will learn to combine them into more complex patterns\n",
            "436  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 145 Applying two different  filters  to get two feature maps\n",
            "Stacking Multiple Feature Maps\n",
            "Up to now for simplicity I have represented the output of each convolutional layer as\n",
            "a thin 2D layer but in reality a convolutional layer has multiple filters you decide\n",
            "how many and it outputs one feature map per filter so it is more accurately repre\n",
            "sented in 3D see Figure 146  To do so it has one neuron per pixel in each feature\n",
            "map and all neurons within a given feature map share the same parameters ie the\n",
            "same weights and bias term However neurons in different feature maps use differ\n",
            "ent parameters A neurons receptive field is the same as described earlier but it\n",
            "extends across all the previous layers feature maps In short a convolutional layer\n",
            "simultaneously applies multiple trainable filters to its inputs making it capable of\n",
            "detecting multiple features anywhere in its inputs\n",
            "The fact that all neurons in a feature map share the same parame\n",
            "ters dramatically reduces the number of parameters in the model\n",
            "Moreover once the CNN has learned to recognize a pattern in one\n",
            "location it can recognize it in any other location In contrast once\n",
            "a regular DNN has learned to recognize a pattern in one location it\n",
            "can recognize it only in that particular location\n",
            "Moreover input images are also composed of multiple sublayers one per color chan\n",
            "nel There are typically three red green and blue RGB Grayscale images have just\n",
            "Convolutional Layer  437one channel but some images may have much morefor example satellite images\n",
            "that capture extra light frequencies such as infrared\n",
            "Figure 146 Convolution layers with multiple feature maps and images with three color\n",
            "channels\n",
            "Specifically a neuron located in row i column j of the feature map k in a given convo\n",
            "lutional layer l is connected to the outputs of the neurons in the previous layer l  1\n",
            "located in rows i  sh to i  sh  fh  1 and columns j  sw to j  sw  fw  1 across all\n",
            "feature maps in layer l  1 Note that all neurons located in the same row i and col\n",
            "umn j but in different feature maps are connected to the outputs of the exact same\n",
            "neurons in the previous layer\n",
            "Equation 141  summarizes the preceding explanations in one big mathematical equa\n",
            "tion it shows how to compute the output of a given neuron in a convolutional layer\n",
            "438  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksIt is a bit ugly due to all the different indices but all it does is calculate the weighted\n",
            "sum of all the inputs plus the bias term\n",
            "Equation 141 Computing the output of a neuron in a convolutional layer\n",
            "zijkbk\n",
            "u 0fh 1\n",
            "\n",
            "v 0fw 1\n",
            "\n",
            "k 0fn 1\n",
            "xijkwuvkkwithiishu\n",
            "jjswv\n",
            "zi j k is the output of the neuron located in row i column j in feature map k of the\n",
            "convolutional layer layer l\n",
            "As explained earlier sh and sw are the vertical and horizontal strides fh and fw are\n",
            "the height and width of the receptive field and fn is the number of feature maps\n",
            "in the previous layer layer l  1\n",
            "xi j k is the output of the neuron located in layer l  1 row i column j feature\n",
            "map k or channel k if the previous layer is the input layer\n",
            "bk is the bias term for feature map k in layer l Y ou can think of it as a knob that\n",
            "tweaks the overall brightness of the feature map k\n",
            "wu v k k is the connection weight between any neuron in feature map k of the layer\n",
            "l and its input located at row u column v relative to the neurons receptive field\n",
            "and feature map k\n",
            "TensorFlow Implementation\n",
            "In TensorFlow each input image is typically represented as a 3D tensor of shape\n",
            "height width channels  A minibatch is represented as a 4D tensor of shape\n",
            "minibatch size height width channels  The weights of a convolutional\n",
            "layer are represented as a 4D tensor of shape  fh fw fn fn The bias terms of a convo\n",
            "lutional layer are simply represented as a 1D tensor of shape  fn\n",
            "Lets look at a simple example The following code loads two sample images using\n",
            "ScikitLearns loadsampleimages  which loads two color images one of a Chi\n",
            "nese temple and the other of a flower The pixel intensities for each color channel\n",
            "is represented as a byte from 0 to 255 so we scale these features simply by dividing by\n",
            "255 to get floats ranging from 0 to 1 Then we create two 7  7 filters one with a\n",
            "vertical white line in the middle and the other with a horizontal white line in the\n",
            "middle and we apply them to both images using the tfnnconv2d  function\n",
            "which is part of TensorFlows lowlevel Deep Learning API In this example we use\n",
            "zero padding  paddingSAME  and a stride of 2 Finally we plot one of the resulting\n",
            "feature maps similar to the topright image in Figure 145 \n",
            "Convolutional Layer  439from sklearndatasets  import loadsampleimage\n",
            " Load sample images\n",
            "china  loadsampleimage chinajpg   255\n",
            "flower  loadsampleimage flowerjpg   255\n",
            "images  nparraychina flower\n",
            "batchsize  height width channels   imagesshape\n",
            " Create 2 filters\n",
            "filters  npzerosshape7 7 channels  2 dtypenpfloat32\n",
            "filters 3  0  1   vertical line\n",
            "filters3   1  1   horizontal line\n",
            "outputs  tfnnconv2dimages filters strides1 paddingSAME\n",
            "pltimshowoutputs0   1 cmapgray  plot 1st images 2nd feature map\n",
            "pltshow\n",
            "Most of this code is selfexplanatory but the tfnnconv2d  line deserves a bit of\n",
            "explanation\n",
            "images  is the input minibatch a 4D tensor as explained earlier\n",
            "filters  is the set of filters to apply also a 4D tensor as explained earlier\n",
            "strides  is equal to 1 but it could also be a 1D array with 4 elements where the\n",
            "two central elements are the vertical and horizontal strides  sh and sw The first\n",
            "and last elements must currently be equal to 1 They may one day be used to\n",
            "specify a batch stride to skip some instances and a channel stride to skip some\n",
            "of the previous layers feature maps or channels\n",
            "padding  must be either VALID  or SAME \n",
            "If set to VALID  the convolutional layer does not use zero padding and may\n",
            "ignore some rows and columns at the bottom and right of the input image\n",
            "depending on the stride as shown in Figure 147  for simplicity only the hor\n",
            "izontal dimension is shown here but of course the same logic applies to the\n",
            "vertical dimension\n",
            "If set to SAME  the convolutional layer uses zero padding if necessary In this\n",
            "case the number of output neurons is equal to the number of input neurons\n",
            "divided by the stride rounded up in this example 13  5  26 rounded up to\n",
            "3 Then zeros are added as evenly as possible around the inputs\n",
            "440  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 147 Padding optionsinput width 13 filter  width 6 stride 5\n",
            "In this example we manually defined the filters but in a real CNN you would nor\n",
            "mally define filters as trainable variables so the neural net can learn which filters\n",
            "work best as explained earlier Instead of manually creating the variables however\n",
            "you can simply use the keraslayersConv2D  layer\n",
            "conv  keraslayersConv2Dfilters32 kernelsize 3 strides1\n",
            "                           paddingSAME activation relu\n",
            "This code creates a Conv2D  layer with 32 filters each 3  3 using a stride of 1 both\n",
            "horizontally and vertically SAME padding and applying the ReLU activation func\n",
            "tion to its outputs As you can see convolutional layers have quite a few hyperpara\n",
            "meters you must choose the number of filters their height and width the strides and\n",
            "the padding type As always you can use crossvalidation to find the right hyperpara\n",
            "meter values but this is very timeconsuming We will discuss common CNN archi\n",
            "tectures later to give you some idea of what hyperparameter values work best in \n",
            "practice\n",
            "Memory Requirements\n",
            "Another problem with CNNs is that the convolutional layers require a huge amount\n",
            "of RAM This is especially true during training because the reverse pass of backpro\n",
            "pagation requires all the intermediate values computed during the forward pass\n",
            "For example consider a convolutional layer with 5  5 filters outputting 200 feature\n",
            "maps of size 150  100 with stride 1 and SAME padding If the input is a 150  100\n",
            "Convolutional Layer  4417A fully connected layer with 150  100 neurons each connected to all 150  100  3 inputs would have 1502\n",
            " 1002  3  675 million parameters\n",
            "8In the international system of units SI 1 MB  1000 kB  1000  1000 bytes  1000  1000  8 bits\n",
            "RGB image three channels then the number of parameters is 5  5  3  1  200\n",
            " 15200 the 1 corresponds to the bias terms which is fairly small compared to a\n",
            "fully connected layer7 However each of the 200 feature maps contains 150  100 neu\n",
            "rons and each of these neurons needs to compute a weighted sum of its 5  5  3 \n",
            "75 inputs thats a total of 225 million float multiplications Not as bad as a fully con\n",
            "nected layer but still quite computationally intensive Moreover if the feature maps\n",
            "are represented using 32bit floats then the convolutional layers output will occupy\n",
            "200  150  100  32  96 million bits 12 MB of RAM8 And thats just for one\n",
            "instance If a training batch contains 100 instances then this layer will use up 12 GB\n",
            "of RAM\n",
            "During inference ie when making a prediction for a new instance the RAM occu\n",
            "pied by one layer can be released as soon as the next layer has been computed so you\n",
            "only need as much RAM as required by two consecutive layers But during training\n",
            "everything computed during the forward pass needs to be preserved for the reverse\n",
            "pass so the amount of RAM needed is at least the total amount of RAM required by\n",
            "all layers\n",
            "If training crashes because of an outofmemory error you can try\n",
            "reducing the minibatch size Alternatively you can try reducing\n",
            "dimensionality using a stride or removing a few layers Or you can\n",
            "try using 16bit floats instead of 32bit floats Or you could distrib\n",
            "ute the CNN across multiple devices\n",
            "Now lets look at the second common building block of CNNs the pooling layer \n",
            "Pooling Layer\n",
            "Once you understand how convolutional layers work the pooling layers are quite\n",
            "easy to grasp Their goal is to subsample  ie shrink the input image in order to\n",
            "reduce the computational load the memory usage and the number of parameters\n",
            "thereby limiting the risk of overfitting\n",
            "Just like in convolutional layers each neuron in a pooling layer is connected to the\n",
            "outputs of a limited number of neurons in the previous layer located within a small\n",
            "rectangular receptive field Y ou must define its size the stride and the padding type\n",
            "just like before However a pooling neuron has no weights all it does is aggregate the\n",
            "inputs using an aggregation function such as the max or mean Figure 148  shows a\n",
            "max pooling layer  which is the most common type of pooling layer In this example\n",
            "442  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks9Other kernels we discussed so far had weights but pooling kernels do not they are just stateless sliding win\n",
            "dows\n",
            "we use a 2  2 pooling kernel9 with a stride of 2 and no padding Only the max\n",
            "input value in each receptive field makes it to the next layer while the other inputs\n",
            "are dropped For example in the lower left receptive field in Figure 148  the input\n",
            "values are 1 5 3 2 so only the max value 5 is propagated to the next layer Because\n",
            "of the stride of 2 the output image has half the height and half the width of the input\n",
            "image rounded down since we use no padding\n",
            "Figure 148 Max pooling layer 2  2 pooling kernel stride 2 no padding\n",
            "A pooling layer typically works on every input channel independ\n",
            "ently so the output depth is the same as the input depth\n",
            "Other than reducing computations memory usage and the number of parameters a\n",
            "max pooling layer also introduces some level of invariance  to small translations as\n",
            "shown in Figure 149  Here we assume that the bright pixels have a lower value than\n",
            "dark pixels and we consider 3 images A B C going through a max pooling layer\n",
            "with a 2  2 kernel and stride 2 Images B and C are the same as image A but shifted\n",
            "by one and two pixels to the right As you can see the outputs of the max pooling\n",
            "layer for images A and B are identical This is what translation invariance means\n",
            "However for image C the output is different it is shifted by one pixel to the right\n",
            "but there is still 75 invariance By inserting a max pooling layer every few layers in\n",
            "a CNN it is possible to get some level of translation invariance at a larger scale\n",
            "Moreover max pooling also offers a small amount of rotational invariance and a\n",
            "slight scale invariance Such invariance even if it is limited can be useful in cases\n",
            "where the prediction should not depend on these details such as in classification\n",
            "tasks\n",
            "Pooling Layer  443Figure 149 Invariance to small translations\n",
            "But max pooling has some downsides firstly it is obviously very destructive even\n",
            "with a tiny 2  2 kernel and a stride of 2 the output will be two times smaller in both\n",
            "directions so its area will be four times smaller simply dropping 75 of the input\n",
            "values And in some applications invariance is not desirable for example for seman\n",
            "tic segmentation  this is the task of classifying each pixel in an image depending on the\n",
            "object that pixel belongs to obviously if the input image is translated by 1 pixel to the\n",
            "right the output should also be translated by 1 pixel to the right The goal in this case\n",
            "is equivariance  not invariance a small change to the inputs should lead to a corre\n",
            "sponding small change in the output\n",
            "TensorFlow Implementation\n",
            "Implementing a max pooling layer in TensorFlow is quite easy The following code\n",
            "creates a max pooling layer using a 2  2 kernel The strides default to the kernel size\n",
            "so this layer will use a stride of 2 both horizontally and vertically By default it uses\n",
            "V ALID padding ie no padding at all\n",
            "maxpool   keraslayersMaxPool2D poolsize 2\n",
            "To create an average pooling layer  just use AvgPool2D  instead of MaxPool2D  As you\n",
            "might expect it works exactly like a max pooling layer except it computes the mean\n",
            "rather than the max Average pooling layers used to be very popular but people\n",
            "444  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksmostly use max pooling layers now as they generally perform better This may seem\n",
            "surprising since computing the mean generally loses less information than comput\n",
            "ing the max But on the other hand max pooling preserves only the strongest feature\n",
            "getting rid of all the meaningless ones so the next layers get a cleaner signal to work\n",
            "with Moreover max pooling offers stronger translation invariance than average\n",
            "pooling\n",
            "Note that max pooling and average pooling can be performed along the depth dimen\n",
            "sion rather than the spatial dimensions although this is not as common This can\n",
            "allow the CNN to learn to be invariant to various features For example it could learn\n",
            "multiple filters each detecting a different rotation of the same pattern such as hand\n",
            "written digits see Figure 1410  and the depthwise max pooling layer would ensure\n",
            "that the output is the same regardless of the rotation The CNN could similarly learn\n",
            "to be invariant to anything else thickness brightness skew color and so on\n",
            "Figure 1410 Depthwise max pooling can help the CNN learn any invariance\n",
            "Pooling Layer  445Keras does not include a depthwise max pooling layer but TensorFlows lowlevel\n",
            "Deep Learning API does just use the tfnnmaxpool  function and specify the\n",
            "kernel size and strides as 4tuples The first three values of each should be 1 this indi\n",
            "cates that the kernel size and stride along the batch height and width dimensions\n",
            "shoud be 1 The last value should be whatever kernel size and stride you want along\n",
            "the depth dimension for example 3 this must be a divisor of the input depth for\n",
            "example it will not work if the previous layer outputs 20 feature maps since 20 is not\n",
            "a multiple of 3\n",
            "output  tfnnmaxpool images\n",
            "                        ksize1 1 1 3\n",
            "                        strides1 1 1 3\n",
            "                        paddingVALID\n",
            "If you want to include this as a layer in your Keras models you can simply wrap it in\n",
            "a Lambda  layer or create a custom Keras layer\n",
            "depthpool   keraslayersLambda\n",
            "    lambda X tfnnmaxpool X ksize1 1 1 3 strides1 1 1 3\n",
            "                             paddingVALID\n",
            "One last type of pooling layer that you will often see in modern architectures is the\n",
            "global average pooling  layer It works very differently all it does is compute the mean\n",
            "of each entire feature map its like an average pooling layer using a pooling kernel\n",
            "with the same spatial dimensions as the inputs This means that it just outputs a sin\n",
            "gle number per feature map and per instance Although this is of course extremely\n",
            "destructive most of the information in the feature map is lost it can be useful as the\n",
            "output layer as we will see later in this chapter To create such a layer simply use the\n",
            "keraslayersGlobalAvgPool2D  class\n",
            "globalavgpool   keraslayersGlobalAvgPool2D \n",
            "It is actually equivalent to this simple Lamba  layer which computes the mean over the\n",
            "spatial dimensions height and width\n",
            "globalavgpool   keraslayersLambdalambda X tfreducemean X axis1 2\n",
            "Now you know all the building blocks to create a convolutional neural network Lets\n",
            "see how to assemble them\n",
            "CNN Architectures\n",
            "Typical CNN architectures stack a few convolutional layers each one generally fol\n",
            "lowed by a ReLU layer then a pooling layer then another few convolutional layers\n",
            "ReLU then another pooling layer and so on The image gets smaller and smaller\n",
            "as it progresses through the network but it also typically gets deeper and deeper ie\n",
            "with more feature maps thanks to the convolutional layers see Figure 1411  At the\n",
            "top of the stack a regular feedforward neural network is added composed of a few\n",
            "446  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksfully connected layers ReLUs and the final layer outputs the prediction eg a\n",
            "softmax layer that outputs estimated class probabilities\n",
            "Figure 1411 Typical CNN architecture\n",
            "A common mistake is to use convolution kernels that are too large\n",
            "For example instead of using a convolutional layer with a 5  5\n",
            "kernel it is generally preferable to stack two layers with 3  3 ker\n",
            "nels it will use less parameters and require less computations and\n",
            "it will usually perform better One exception to this recommenda\n",
            "tion is for the first convolutional layer it can typically have a large\n",
            "kernel eg 5  5 usually with stride of 2 or more this will reduce\n",
            "the spatial dimension of the image without losing too much infor\n",
            "mation and since the input image only has 3 channels in general it\n",
            "will not be too costly\n",
            "Here is how you can implement a simple CNN to tackle the fashion MNIST dataset\n",
            "introduced in Chapter 10 \n",
            "from functools  import partial\n",
            "DefaultConv2D   partialkeraslayersConv2D\n",
            "                        kernelsize 3 activation relu paddingSAME\n",
            "model  kerasmodelsSequential \n",
            "    DefaultConv2D filters64 kernelsize 7 inputshape 28 28 1\n",
            "    keraslayersMaxPooling2D poolsize 2\n",
            "    DefaultConv2D filters128\n",
            "    DefaultConv2D filters128\n",
            "    keraslayersMaxPooling2D poolsize 2\n",
            "    DefaultConv2D filters256\n",
            "    DefaultConv2D filters256\n",
            "    keraslayersMaxPooling2D poolsize 2\n",
            "    keraslayersFlatten\n",
            "    keraslayersDenseunits128 activation relu\n",
            "    keraslayersDropout05\n",
            "    keraslayersDenseunits64 activation relu\n",
            "    keraslayersDropout05\n",
            "    keraslayersDenseunits10 activation softmax \n",
            "\n",
            "CNN Architectures  447In this code we start by using the partial  function to define a thin wrapper\n",
            "around the Conv2D  class called DefaultConv2D  it simply avoids having to repeat\n",
            "the same hyperparameter values over and over again\n",
            "The first layer uses a large kernel size but no stride because the input images are\n",
            "not very large It also sets inputshape28 28 1  which means the images\n",
            "are 28  28 pixels with a single color channel ie grayscale\n",
            "Next we have a max pooling layer which divides each spatial dimension by a fac\n",
            "tor of two since poolsize2 \n",
            "Then we repeat the same structure twice two convolutional layers followed by a\n",
            "max pooling layer For larger images we could repeat this structure several times\n",
            "the number of repetitions is a hyperparameter you can tune\n",
            "Note that the number of filters grows as we climb up the CNN towards the out\n",
            "put layer it is initially 64 then 128 then 256 it makes sense for it to grow since\n",
            "the number of low level features is often fairly low eg small circles horizontal\n",
            "lines etc but there are many different ways to combine them into higher level\n",
            "features It is a common practice to double the number of filters after each pool\n",
            "ing layer since a pooling layer divides each spatial dimension by a factor of 2 we\n",
            "can afford doubling the number of feature maps in the next layer without fear of\n",
            "exploding the number of parameters memory usage or computational load\n",
            "Next is the fully connected network composed of 2 hidden dense layers and a\n",
            "dense output layer Note that we must flatten its inputs since a dense network\n",
            "expects a 1D array of features for each instance We also add two dropout layers\n",
            "with a dropout rate of 50 each to reduce overfitting\n",
            "This CNN reaches over 92 accuracy on the test set Its not the state of the art but it\n",
            "is pretty good and clearly much better than what we achieved with dense networks in\n",
            "Chapter 10 \n",
            "Over the years variants of this fundamental architecture have been developed lead\n",
            "ing to amazing advances in the field A good measure of this progress is the error rate\n",
            "in competitions such as the ILSVRC ImageNet challenge  In this competition the\n",
            "top5 error rate for image classification fell from over 26 to less than 23 in just six\n",
            "years The topfive error rate is the number of test images for which the systems top 5\n",
            "predictions did not include the correct answer The images are large 256 pixels high\n",
            "and there are 1000 classes some of which are really subtle try distinguishing 120\n",
            "dog breeds Looking at the evolution of the winning entries is a good way to under\n",
            "stand how CNNs work\n",
            "We will first look at the classical LeNet5 architecture 1998 then three of the win\n",
            "ners of the ILSVRC challenge AlexNet 2012 GoogLeNet 2014 and ResNet\n",
            "2015\n",
            "448  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks10GradientBased Learning Applied to Document Recognition  Y  LeCun L Bottou Y  Bengio and P  Haffner\n",
            "1998LeNet5\n",
            "The LeNet5 architecture10 is perhaps the most widely known CNN architecture As\n",
            "mentioned earlier it was created by Y ann LeCun in 1998 and widely used for hand\n",
            "written digit recognition MNIST It is composed of the layers shown in Table 141 \n",
            "Table 141 LeNet5 architecture\n",
            "Layer Type Maps Size Kernel size Stride Activation\n",
            "Out Fully Connected  10   RBF\n",
            "F6 Fully Connected  84   tanh\n",
            "C5 Convolution 120 1  1 5  5 1 tanh\n",
            "S4 Avg Pooling 16 5  5 2  2 2 tanh\n",
            "C3 Convolution 16 10  10 5  5 1 tanh\n",
            "S2 Avg Pooling 6 14  14 2  2 2 tanh\n",
            "C1 Convolution 6 28  28 5  5 1 tanh\n",
            "In Input 1 32  32   \n",
            "There are a few extra details to be noted\n",
            "MNIST images are 28  28 pixels but they are zeropadded to 32  32 pixels and\n",
            "normalized before being fed to the network The rest of the network does not use\n",
            "any padding which is why the size keeps shrinking as the image progresses\n",
            "through the network\n",
            "The average pooling layers are slightly more complex than usual each neuron\n",
            "computes the mean of its inputs then multiplies the result by a learnable coeffi\n",
            "cient one per map and adds a learnable bias term again one per map then\n",
            "finally applies the activation function\n",
            "Most neurons in C3 maps are connected to neurons in only three or four S2\n",
            "maps instead of all six S2 maps See table 1 page 8 in the original paper10 for\n",
            "details\n",
            "The output layer is a bit special instead of computing the matrix multiplication\n",
            "of the inputs and the weight vector each neuron outputs the square of the Eucli\n",
            "dian distance between its input vector and its weight vector Each output meas\n",
            "ures how much the image belongs to a particular digit class The cross entropy \n",
            "cost function is now preferred as it penalizes bad predictions much more pro\n",
            "ducing larger gradients and converging faster\n",
            "CNN Architectures  44911ImageNet Classification with Deep Convolutional Neural Networks  A Krizhevsky et al 2012Y ann LeCuns website  LENET section features great demos of LeNet5 classifying \n",
            "digits\n",
            "AlexNet\n",
            "The AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\n",
            "large margin it achieved 17 top5 error rate while the second best achieved only\n",
            "26 It was developed by Alex Krizhevsky hence the name Ilya Sutskever and\n",
            "Geoffrey Hinton It is quite similar to LeNet5 only much larger and deeper and it\n",
            "was the first to stack convolutional layers directly on top of each other instead of\n",
            "stacking a pooling layer on top of each convolutional layer Table 142  presents this\n",
            "architecture\n",
            "Table 142 AlexNet architecture\n",
            "Layer Type Maps Size Kernel size Stride Padding Activation\n",
            "Out Fully Connected  1000    Softmax\n",
            "F9 Fully Connected  4096    ReLU\n",
            "F8 Fully Connected  4096    ReLU\n",
            "C7 Convolution 256 13  13 3  3 1 SAME ReLU\n",
            "C6 Convolution 384 13  13 3  3 1 SAME ReLU\n",
            "C5 Convolution 384 13  13 3  3 1 SAME ReLU\n",
            "S4 Max Pooling 256 13  13 3  3 2 VALID \n",
            "C3 Convolution 256 27  27 5  5 1 SAME ReLU\n",
            "S2 Max Pooling 96 27  27 3  3 2 VALID \n",
            "C1 Convolution 96 55  55 11  11 4 VALID ReLU\n",
            "In Input 3 RGB 227  227    \n",
            "To reduce overfitting the authors used two regularization techniques first they\n",
            "applied dropout introduced in Chapter 11  with a 50 dropout rate during training\n",
            "to the outputs of layers F8 and F9 Second they performed data augmentation  by ran\n",
            "domly shifting the training images by various offsets flipping them horizontally and\n",
            "changing the lighting conditions\n",
            "Data Augmentation\n",
            "Data augmentation artificially increases the size of the training set by generating\n",
            "many realistic variants of each training instance This reduces overfitting making this\n",
            "a regularization technique The generated instances should be as realistic as possible\n",
            "450  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksideally given an image from the augmented training set a human should not be able\n",
            "to tell whether it was augmented or not Moreover simply adding white noise will not\n",
            "help the modifications should be learnable white noise is not\n",
            "For example you can slightly shift rotate and resize every picture in the training set\n",
            "by various amounts and add the resulting pictures to the training set see\n",
            "Figure 1412  This forces the model to be more tolerant to variations in the position\n",
            "orientation and size of the objects in the pictures If you want the model to be more\n",
            "tolerant to different lighting conditions you can similarly generate many images with\n",
            "various contrasts In general you can also flip the pictures horizontally except for\n",
            "text and other nonsymmetrical objects By combining these transformations you\n",
            "can greatly increase the size of your training set\n",
            "Figure 1412 Generating new training instances from existing ones\n",
            "AlexNet also uses a competitive normalization step immediately after the ReLU step\n",
            "of layers C1 and C3 called local response normalization  The most strongly activated\n",
            "neurons inhibit other neurons located at the same position in neighboring feature\n",
            "maps such competitive activation has been observed in biological neurons This\n",
            "encourages different feature maps to specialize pushing them apart and forcing them\n",
            "CNN Architectures  45112Going Deeper with Convolutions  C Szegedy et al 2015\n",
            "13In the 2010 movie Inception  the characters keep going deeper and deeper into multiple layers of dreams\n",
            "hence the name of these modulesto explore a wider range of features ultimately improving generalization Equation\n",
            "142  shows how to apply LRN\n",
            "Equation 142 Local response normalization\n",
            "biaik\n",
            "jjlowjhigh\n",
            "aj2\n",
            "withjhigh min ir\n",
            "2fn 1\n",
            "jlow max 0ir\n",
            "2\n",
            "bi is the normalized output of the neuron located in feature map i at some row u\n",
            "and column v note that in this equation we consider only neurons located at this\n",
            "row and column so u and v are not shown\n",
            "ai is the activation of that neuron after the ReLU step but before normalization\n",
            "k   and r are hyperparameters k is called the bias and r is called the depth\n",
            "radius \n",
            "fn is the number of feature maps\n",
            "For example if r  2 and a neuron has a strong activation it will inhibit the activation\n",
            "of the neurons located in the feature maps immediately above and below its own\n",
            "In AlexNet the hyperparameters are set as follows r  2   000002   075 and k\n",
            " 1 This step can be implemented using the tfnnlocalresponsenormaliza\n",
            "tion  function which you can wrap in a Lambda  layer if you want to use it in a\n",
            "Keras model\n",
            "A variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\n",
            "and won the 2013 ILSVRC challenge It is essentially AlexNet with a few tweaked \n",
            "hyperparameters number of feature maps kernel size stride etc\n",
            "GoogLeNet\n",
            "The GoogLeNet architecture  was developed by Christian Szegedy et al from Google\n",
            "Research12 and it won the ILSVRC 2014 challenge by pushing the top5 error rate\n",
            "below 7 This great performance came in large part from the fact that the network\n",
            "was much deeper than previous CNNs see Figure 1414  This was made possible by\n",
            "subnetworks called inception modules 13 which allow GoogLeNet to use parameters\n",
            "452  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksmuch more efficiently than previous architectures GoogLeNet actually has 10 times\n",
            "fewer parameters than AlexNet roughly 6 million instead of 60 million\n",
            "Figure 1413  shows the architecture of an inception module The notation 3  3 \n",
            "1S means that the layer uses a 3  3 kernel stride 1 and SAME padding The input\n",
            "signal is first copied and fed to four different layers All convolutional layers use the\n",
            "ReLU activation function Note that the second set of convolutional layers uses differ\n",
            "ent kernel sizes 1  1 3  3 and 5  5 allowing them to capture patterns at different\n",
            "scales Also note that every single layer uses a stride of 1 and SAME padding even\n",
            "the max pooling layer so their outputs all have the same height and width as their\n",
            "inputs This makes it possible to concatenate all the outputs along the depth dimen\n",
            "sion in the final depth concat layer  ie stack the feature maps from all four top con\n",
            "volutional layers This concatenation layer can be implemented in TensorFlow using\n",
            "the tfconcat  operation with axis3  axis 3 is the depth\n",
            "Figure 1413 Inception module\n",
            "Y ou may wonder why inception modules have convolutional layers with 1  1 ker\n",
            "nels Surely these layers cannot capture any features since they look at only one pixel\n",
            "at a time In fact these layers serve three purposes\n",
            "First although they cannot capture spatial patterns they can capture patterns\n",
            "along the depth dimension\n",
            "Second they are configured to output fewer feature maps than their inputs so\n",
            "they serve as bottleneck layers  meaning they reduce dimensionality This cuts the\n",
            "computational cost and the number of parameters speeding up training and\n",
            "improving generalization\n",
            "Lastly each pair of convolutional layers 1  1 3  3 and 1  1 5  5 acts like\n",
            "a single powerful convolutional layer capable of capturing more complex pat\n",
            "terns Indeed instead of sweeping a simple linear classifier across the image as a\n",
            "CNN Architectures  453single convolutional layer does this pair of convolutional layers sweeps a two\n",
            "layer neural network across the image\n",
            "In short you can think of the whole inception module as a convolutional layer on\n",
            "steroids able to output feature maps that capture complex patterns at various scales\n",
            "The number of convolutional kernels for each convolutional layer\n",
            "is a hyperparameter Unfortunately this means that you have six\n",
            "more hyperparameters to tweak for every inception layer you add\n",
            "Now lets look at the architecture of the GoogLeNet CNN see Figure 1414  The\n",
            "number of feature maps output by each convolutional layer and each pooling layer is\n",
            "shown before the kernel size The architecture is so deep that it has to be represented\n",
            "in three columns but GoogLeNet is actually one tall stack including nine inception\n",
            "modules the boxes with the spinning tops The six numbers in the inception mod\n",
            "ules represent the number of feature maps output by each convolutional layer in the\n",
            "module in the same order as in Figure 1413  Note that all the convolutional layers\n",
            "use the ReLU activation function\n",
            "454  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 1414 GoogLeNet architecture\n",
            "Lets go through this network\n",
            "The first two layers divide the images height and width by 4 so its area is divided\n",
            "by 16 to reduce the computational load The first layer uses a large kernel size\n",
            "so that much of the information is still preserved\n",
            "Then the local response normalization layer ensures that the previous layers learn\n",
            "a wide variety of features as discussed earlier\n",
            "Two convolutional layers follow where the first acts like a bottleneck layer  As\n",
            "explained earlier you can think of this pair as a single smarter convolutional\n",
            "layer\n",
            "Again a local response normalization layer ensures that the previous layers cap\n",
            "ture a wide variety of patterns\n",
            "CNN Architectures  45514Very Deep Convolutional Networks for LargeScale Image Recognition  K Simonyan and A Zisserman\n",
            "2015Next a max pooling layer reduces the image height and width by 2 again to speed\n",
            "up computations\n",
            "Then comes the tall stack of nine inception modules interleaved with a couple\n",
            "max pooling layers to reduce dimensionality and speed up the net\n",
            "Next the global average pooling layer simply outputs the mean of each feature\n",
            "map this drops any remaining spatial information which is fine since there was\n",
            "not much spatial information left at that point Indeed GoogLeNet input images\n",
            "are typically expected to be 224  224 pixels so after 5 max pooling layers each\n",
            "dividing the height and width by 2 the feature maps are down to 7  7 More\n",
            "over it is a classification task not localization so it does not matter where the\n",
            "object is Thanks to the dimensionality reduction brought by this layer there is\n",
            "no need to have several fully connected layers at the top of the CNN like in\n",
            "AlexNet and this considerably reduces the number of parameters in the net\n",
            "work and limits the risk of overfitting\n",
            "The last layers are selfexplanatory dropout for regularization then a fully con\n",
            "nected layer with 1000 units since there are a 1000 classes and a softmax acti\n",
            "vation function to output estimated class probabilities\n",
            "This diagram is slightly simplified the original GoogLeNet architecture also included\n",
            "two auxiliary classifiers plugged on top of the third and sixth inception modules\n",
            "They were both composed of one average pooling layer one convolutional layer two\n",
            "fully connected layers and a softmax activation layer During training their loss\n",
            "scaled down by 70 was added to the overall loss The goal was to fight the vanish\n",
            "ing gradients problem and regularize the network However it was later shown that\n",
            "their effect was relatively minor\n",
            "Several variants of the GoogLeNet architecture were later proposed by Google\n",
            "researchers including Inceptionv3 and Inceptionv4 using slightly different incep\n",
            "tion modules and reaching even better performance\n",
            "VGGNet\n",
            "The runner up in the ILSVRC 2014 challenge was  VGGNet14 developed by K Simon\n",
            "yan and A Zisserman It had a very simple and classical architecture with 2 or 3 con\n",
            "volutional layers a pooling layer then again 2 or 3 convolutional layers a pooling\n",
            "layer and so on with a total of just 16 convolutional layers plus a final dense net\n",
            "work with 2 hidden layers and the output layer It used only 3  3 filters but many\n",
            "filters\n",
            "456  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks15Deep Residual Learning for Image Recognition  K He 2015ResNet\n",
            "The ILSVRC 2015 challenge was won using a Residual Network  or ResNet  devel\n",
            "oped by Kaiming He et al15 which delivered an astounding top5 error rate under\n",
            "36 using an extremely deep CNN composed of 152 layers It confirmed the general\n",
            "trend models are getting deeper and deeper with fewer and fewer parameters The\n",
            "key to being able to train such a deep network is to use skip connections  also called\n",
            "shortcut connections  the signal feeding into a layer is also added to the output of a\n",
            "layer located a bit higher up the stack Lets see why this is useful\n",
            "When training a neural network the goal is to make it model a target function hx\n",
            "If you add the input x to the output of the network ie you add a skip connection\n",
            "then the network will be forced to model fx  hx  x rather than hx This is\n",
            "called residual learning  see Figure 1415 \n",
            "Figure 1415 Residual learning\n",
            "When you initialize a regular neural network its weights are close to zero so the net\n",
            "work just outputs values close to zero If you add a skip connection the resulting net\n",
            "work just outputs a copy of its inputs in other words it initially models the identity\n",
            "function If the target function is fairly close to the identity function which is often\n",
            "the case this will speed up training considerably\n",
            "Moreover if you add many skip connections the network can start making progress\n",
            "even if several layers have not started learning yet see Figure 1416  Thanks to skip\n",
            "connections the signal can easily make its way across the whole network The deep\n",
            "residual network can be seen as a stack of residual units  where each residual unit is a\n",
            "small neural network with a skip connection\n",
            "CNN Architectures  457Figure 1416 Regular deep neural network left  and deep residual network right\n",
            "Now lets look at ResNets architecture see Figure 1417  It is actually surprisingly\n",
            "simple It starts and ends exactly like GoogLeNet except without a dropout layer\n",
            "and in between is just a very deep stack of simple residual units Each residual unit is\n",
            "composed of two convolutional layers and no pooling layer with Batch Normaliza\n",
            "tion BN and ReLU activation using 3  3 kernels and preserving spatial dimensions\n",
            "stride 1 SAME padding\n",
            "Figure 1417 ResNet architecture\n",
            "Note that the number of feature maps is doubled every few residual units at the same\n",
            "time as their height and width are halved using a convolutional layer with stride 2\n",
            "When this happens the inputs cannot be added directly to the outputs of the residual\n",
            "unit since they dont have the same shape for example this problem affects the skip\n",
            "458  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks16Inceptionv4 InceptionResNet and the Impact of Residual Connections on Learning  C Szegedy et al\n",
            "2016\n",
            "17Xception Deep Learning with Depthwise Separable Convolutions  Franois Chollet 2016\n",
            "connection represented by the dashed arrow in Figure 1417  To solve this problem\n",
            "the inputs are passed through a 1  1 convolutional layer with stride 2 and the right\n",
            "number of output feature maps see Figure 1418 \n",
            "Figure 1418 Skip connection when changing feature map size and depth\n",
            "ResNet34 is the ResNet with 34 layers only counting the convolutional layers and\n",
            "the fully connected layer containing three residual units that output 64 feature maps\n",
            "4 RUs with 128 maps 6 RUs with 256 maps and 3 RUs with 512 maps We will imple\n",
            "ment this architecture later in this chapter\n",
            "ResNets deeper than that such as ResNet152 use slightly different residual units\n",
            "Instead of two 3  3 convolutional layers with say 256 feature maps they use three\n",
            "convolutional layers first a 1  1 convolutional layer with just 64 feature maps 4\n",
            "times less which acts as a bottleneck layer as discussed already then a 3  3 layer\n",
            "with 64 feature maps and finally another 1  1 convolutional layer with 256 feature\n",
            "maps 4 times 64 that restores the original depth ResNet152 contains three such\n",
            "RUs that output 256 maps then 8 RUs with 512 maps a whopping 36 RUs with 1024\n",
            "maps and finally 3 RUs with 2048 maps\n",
            "Googles Inceptionv416 architecture merged the ideas of GoogLe\n",
            "Net and ResNet and achieved close to 3 top5 error rate on\n",
            "ImageNet classification\n",
            "Xception\n",
            "Another variant of the GoogLeNet architecture is also worth noting Xception17\n",
            "which stands for Extreme Inception  was proposed in 2016 by Franois Chollet the\n",
            "CNN Architectures  45918This name can sometimes be ambiguous since spatially separable convolutions are often called separable\n",
            "convolutions as wellauthor of Keras and it significantly outperformed Inceptionv3 on a huge vision task\n",
            "350 million images and 17000 classes Just like Inceptionv4 it also merges the\n",
            "ideas of GoogLeNet and ResNet but it replaces the inception modules with a special\n",
            "type of layer called a depthwise separable convolution  or separable convolution  for\n",
            "short18 These layers had been used before in some CNN architectures but they were\n",
            "not as central as in the Xception architecture While a regular convolutional layer\n",
            "uses filters that try to simultaneously capture spatial patterns eg an oval and cross\n",
            "channel patterns eg mouth  nose  eyes  face a separable convolutional layer\n",
            "makes the strong assumption that spatial patterns and crosschannel patterns can be\n",
            "modeled separately see Figure 1419  Thus it is composed of two parts the first part\n",
            "applies a single spatial filter for each input feature map then the second part looks\n",
            "exclusively for crosschannel patternsit is just a regular convolutional layer with 1 \n",
            "1 filters\n",
            "Figure 1419 Depthwise Separable Convolutional Layer\n",
            "Since separable convolutional layers only have one spatial filter per input channel\n",
            "you should avoid using them after layers that have too few channels such as the input\n",
            "layer granted thats what Figure 1419  represents but it is just for illustration pur\n",
            "poses For this reason the Xception architecture starts with 2 regular convolutional\n",
            "layers but then the rest of the architecture uses only separable convolutions 34 in\n",
            "460  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks19Crafting GBDNet for Object Detection  X Zeng et al 2016\n",
            "20SqueezeandExcitation Networks  Jie Hu et al 2017\n",
            "all plus a few max pooling layers and the usual final layers a global average pooling\n",
            "layer and a dense output layer\n",
            "Y ou might wonder why Xception is considered a variant of GoogLeNet since it con\n",
            "tains no inception module at all Well as we discussed earlier an Inception module\n",
            "contains convolutional layers with 1  1 filters these look exclusively for cross\n",
            "channel patterns However the convolution layers that sit on top of them are regular\n",
            "convolutional layers that look both for spatial and crosschannel patterns So you can\n",
            "think of an Inception module as an intermediate between a regular convolutional\n",
            "layer which considers spatial patterns and crosschannel patterns jointly and a sepa\n",
            "rable convolutional layer which considers them separately In practice it seems that\n",
            "separable convolutions generally perform better\n",
            "Separable convolutions use less parameters less memory and less\n",
            "computations than regular convolutional layers and in general\n",
            "they even perform better so you should consider using them by\n",
            "default except after layers with few channels\n",
            "The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni\n",
            "versity of Hong Kong They used an ensemble of many different techniques includ\n",
            "ing a sophisticated objectdetection system called GBDNet19 to achieve a top5 error\n",
            "rate below 3 Although this result is unquestionably impressive the complexity of\n",
            "the solution contrasted with the simplicity of ResNets Moreover one year later\n",
            "another fairly simple architecture performed even better as we will see now\n",
            "SENet\n",
            "The winning architecture in the ILSVRC 2017 challenge was the Squeezeand\n",
            "Excitation Network  SENet20 This architecture extends existing architectures such as\n",
            "inception networks or ResNets and boosts their performance This allowed SENet to\n",
            "win the competition with an astonishing 225 top5 error rate The extended ver\n",
            "sions of inception networks and ResNet are called SEInception  and SEResNet  respec\n",
            "tively The boost comes from the fact that a SENet adds a small neural network called\n",
            "a SE Block  to every unit in the original architecture ie every inception module or\n",
            "every residual unit as shown in Figure 1420 \n",
            "CNN Architectures  461Figure 1420 SEInception Module left  and SEResNet Unit right\n",
            "A SE Block analyzes the output of the unit it is attached to focusing exclusively on\n",
            "the depth dimension it does not look for any spatial pattern and it learns which fea\n",
            "tures are usually most active together It then uses this information to recalibrate the\n",
            "feature maps as shown in Figure 1421  For example a SE Block may learn that\n",
            "mouths noses and eyes usually appear together in pictures if you see a mouth and a\n",
            "nose you should expect to see eyes as well So if a SE Block sees a strong activation in\n",
            "the mouth and nose feature maps but only mild activation in the eye feature map it\n",
            "will boost the eye feature map more accurately it will reduce irrelevant feature\n",
            "maps If the eyes were somewhat confused with something else this feature map\n",
            "recalibration will help resolve the ambiguity\n",
            "462  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 1421 An SE Block Performs Feature Map Recalibration\n",
            "A SE Block is composed of just 3 layers a global average pooling layer a hidden dense\n",
            "layer using the ReLU activation function and a dense output layer using the sigmoid\n",
            "activation function see Figure 1422 \n",
            "Figure 1422 SE Block Architecture\n",
            "CNN Architectures  463As earlier the global average pooling layer computes the mean activation for each fea\n",
            "ture map for example if its input contains 256 feature maps it will output 256 num\n",
            "bers representing the overall level of response for each filter The next layer is where\n",
            "the squeeze happens this layer has much less than 256 neurons typically 16 times\n",
            "less than the number of feature maps eg 16 neurons so the 256 numbers get com\n",
            "pressed into a small vector eg 16 dimensional This is a lowdimensional vector\n",
            "representation ie an embedding of the distribution of feature responses This bot\n",
            "tleneck step forces the SE Block to learn a general representation of the feature com\n",
            "binations we will see this principle in action again when we discuss autoencoders\n",
            "in  Finally the output layer takes the embedding and outputs a recalibration vec\n",
            "tor containing one number per feature map eg 256 each between 0 and 1 The\n",
            "feature maps are then multiplied by this recalibration vector so irrelevant features\n",
            "with a low recalibration score get scaled down while relevant features with a recali\n",
            "bration score close to 1 are left alone\n",
            "Implementing a ResNet34 CNN Using Keras\n",
            "Most CNN architectures described so far are fairly straightforward to implement\n",
            "although generally you would load a pretrained network instead as we will see To\n",
            "illustrate the process lets implement a ResNet34 from scratch using Keras First lets\n",
            "create a ResidualUnit  layer\n",
            "DefaultConv2D   partialkeraslayersConv2D kernelsize 3 strides1\n",
            "                        paddingSAME usebias False\n",
            "class ResidualUnit keraslayersLayer\n",
            "    def init self filters strides1 activation relu kwargs\n",
            "        superinit kwargs\n",
            "        selfactivation   kerasactivations getactivation \n",
            "        selfmainlayers   \n",
            "            DefaultConv2D filters stridesstrides\n",
            "            keraslayersBatchNormalization \n",
            "            selfactivation \n",
            "            DefaultConv2D filters\n",
            "            keraslayersBatchNormalization \n",
            "        selfskiplayers   \n",
            "        if strides  1\n",
            "            selfskiplayers   \n",
            "                DefaultConv2D filters kernelsize 1 stridesstrides\n",
            "                keraslayersBatchNormalization \n",
            "    def callself inputs\n",
            "        Z  inputs\n",
            "        for layer in selfmainlayers \n",
            "            Z  layerZ\n",
            "        skipZ  inputs\n",
            "        for layer in selfskiplayers \n",
            "464  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks            skipZ  layerskipZ\n",
            "        return selfactivation Z  skipZ\n",
            "As you can see this code matches Figure 1418  pretty closely In the constructor we\n",
            "create all the layers we will need the main layers are the ones on the right side of the\n",
            "diagram and the skip layers are the ones on the left only needed if the stride is\n",
            "greater than 1 Then in the call  method we simply make the inputs go through\n",
            "the main layers and the skip layers if any then we add both outputs and we apply\n",
            "the activation function\n",
            "Next we can build the ResNet34 simply using a Sequential  model since it is really\n",
            "just a long sequence of layers we can treat each residual unit as a single layer now\n",
            "that we have the ResidualUnit  class\n",
            "model  kerasmodelsSequential \n",
            "modeladdDefaultConv2D 64 kernelsize 7 strides2\n",
            "                        inputshape 224 224 3\n",
            "modeladdkeraslayersBatchNormalization \n",
            "modeladdkeraslayersActivation relu\n",
            "modeladdkeraslayersMaxPool2D poolsize 3 strides2 paddingSAME\n",
            "prevfilters   64\n",
            "for filters in 64  3  128  4  256  6  512  3\n",
            "    strides  1 if filters  prevfilters  else 2\n",
            "    modeladdResidualUnit filters stridesstrides\n",
            "    prevfilters   filters\n",
            "modeladdkeraslayersGlobalAvgPool2D \n",
            "modeladdkeraslayersFlatten\n",
            "modeladdkeraslayersDense10 activation softmax \n",
            "The only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\n",
            "to the model as explained earlier the first 3 RUs have 64 filters then the next 4 RUs\n",
            "have 128 filters and so on We then set the strides to 1 when the number of filters is\n",
            "the same as in the previous RU or else we set it to 2 Then we add the ResidualUnit \n",
            "and finally we update prevfilters \n",
            "It is quite amazing that in less than 40 lines of code we can build the model that won\n",
            "the ILSVRC 2015 challenge It demonstrates both the elegance of the ResNet model\n",
            "and the expressiveness of the Keras API Implementing the other CNN architectures\n",
            "is not much harder However Keras comes with several of these architectures built in\n",
            "so why not use them instead\n",
            "Using Pretrained Models From Keras\n",
            "In general you wont have to implement standard models like GoogLeNet or ResNet\n",
            "manually since pretrained networks are readily available with a single line of code in\n",
            "the kerasapplications  package For example\n",
            "model  kerasapplications resnet50 ResNet50 weightsimagenet \n",
            "Using Pretrained Models From Keras  46521In the ImageNet dataset each image is associated to a word in the WordNet dataset  the class ID is just a\n",
            "WordNet ID\n",
            "Thats all This will create a ResNet50 model and download weights pretrained on\n",
            "the ImageNet dataset To use it you first need to ensure that the images have the right\n",
            "size A ResNet50 model expects 224  224 images other models may expect other\n",
            "sizes such as 299  299 so lets use TensorFlows tfimageresize  function to\n",
            "resize the images we loaded earlier\n",
            "imagesresized   tfimageresizeimages 224 224\n",
            "The tfimageresize  will not preserve the aspect ratio If this is\n",
            "a problem you can try cropping the images to the appropriate\n",
            "aspect ratio before resizing Both operations can be done in one\n",
            "shot with tfimagecropandresize \n",
            "The pretrained models assume that the images are preprocessed in a specific way In\n",
            "some cases they may expect the inputs to be scaled from 0 to 1 or 1 to 1 and so on\n",
            "Each model provides a preprocessinput  function that you can use to preprocess\n",
            "your images These functions assume that the pixel values range from 0 to 255 so we\n",
            "must multiply them by 255 since earlier we scaled them to the 01 range\n",
            "inputs  kerasapplications resnet50 preprocessinput imagesresized   255\n",
            "Now we can use the pretrained model to make predictions\n",
            "Yproba  modelpredictinputs\n",
            "As usual the output Yproba  is a matrix with one row per image and one column per\n",
            "class in this case there are 1000 classes If you want to display the top K predic\n",
            "tions including the class name and the estimated probability of each predicted class\n",
            "you can use the decodepredictions  function For each image it returns an array\n",
            "containing the top K predictions where each prediction is represented as an array\n",
            "containing the class identifier21 its name and the corresponding confidence score\n",
            "topK  kerasapplications resnet50 decodepredictions Yproba top3\n",
            "for imageindex  in rangelenimages\n",
            "    printImage  formatimageindex \n",
            "    for classid  name yproba in topKimageindex \n",
            "        print    12s 2f formatclassid  name yproba  100\n",
            "    print\n",
            "The output looks like this\n",
            "Image 0\n",
            "  n03877845  palace       4287\n",
            "  n02825657  bellcote    4057\n",
            "  n03781244  monastery    1456\n",
            "466  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksImage 1\n",
            "  n04522168  vase         4683\n",
            "  n07930864  cup          778\n",
            "  n11939491  daisy        487\n",
            "The correct classes monastery and daisy appear in the top 3 results for both images\n",
            "Thats pretty good considering that the model had to choose among 1000 classes\n",
            "As you can see it is very easy to create a pretty good image classifier using a pre\n",
            "trained model Other vision models are available in kerasapplications  including\n",
            "several ResNet variants GoogLeNet variants like InceptionV3 and Xception\n",
            "VGGNet variants MobileNet and MobileNetV2 lightweight models for use in\n",
            "mobile applications and more\n",
            "But what if you want to use an image classifier for classes of images that are not part\n",
            "of ImageNet In that case you may still benefit from the pretrained models to per\n",
            "form transfer learning\n",
            "Pretrained Models for Transfer Learning\n",
            "If you want to build an image classifier but you do not have enough training data\n",
            "then it is often a good idea to reuse the lower layers of a pretrained model as we dis\n",
            "cussed in Chapter 11  For example lets train a model to classify pictures of flowers\n",
            "reusing a pretrained Xception model First lets load the dataset using TensorFlow\n",
            "Datasets see Chapter 13 \n",
            "import tensorflowdatasets  as tfds\n",
            "dataset info  tfdsloadtfflowers  assupervised True withinfo True\n",
            "datasetsize   infosplitstrainnumexamples   3670\n",
            "classnames   infofeatures labelnames  dandelion daisy \n",
            "nclasses   infofeatures labelnumclasses   5\n",
            "Note that you can get information about the dataset by setting withinfoTrue  Here\n",
            "we get the dataset size and the names of the classes Unfortunately there is only a\n",
            "train  dataset no test set or validation set so we need to split the training set The\n",
            "TF Datasets project provides an API for this For example lets take the first 10 of\n",
            "the dataset for testing the next 15 for validation and the remaining 75 for train\n",
            "ing\n",
            "testsplit  validsplit  trainsplit   tfdsSplitTRAINsubsplit 10 15 75\n",
            "testset   tfdsloadtfflowers  splittestsplit  assupervised True\n",
            "validset   tfdsloadtfflowers  splitvalidsplit  assupervised True\n",
            "trainset   tfdsloadtfflowers  splittrainsplit  assupervised True\n",
            "Pretrained Models for Transfer Learning  467Next we must preprocess the images The CNN expects 224  224 images so we need\n",
            "to resize them We also need to run the image through Xceptions prepro\n",
            "cessinput  function\n",
            "def preprocess image label\n",
            "    resizedimage   tfimageresizeimage 224 224\n",
            "    finalimage   kerasapplications xception preprocessinput resizedimage \n",
            "    return finalimage  label\n",
            "Lets apply this preprocessing function to all 3 datasets and lets also shuffle  repeat\n",
            "the training set and add batching  prefetching to all datasets\n",
            "batchsize   32\n",
            "trainset   trainset shuffle1000repeat\n",
            "trainset   trainset mappreprocess batchbatchsize prefetch 1\n",
            "validset   validset mappreprocess batchbatchsize prefetch 1\n",
            "testset   testset mappreprocess batchbatchsize prefetch 1\n",
            "If you want to perform some data augmentation you can just change the preprocess\n",
            "ing function for the training set adding some random transformations to the training\n",
            "images For example use tfimagerandomcrop  to randomly crop the images use\n",
            "tfimagerandomflipleftright  to randomly flip the images horizontally and\n",
            "so on see the notebook for an example\n",
            "Next lets load an Xception model pretrained on ImageNet We exclude the top of the\n",
            "network by setting includetopFalse  this excludes the global average pooling\n",
            "layer and the dense output layer We then add our own global average pooling layer\n",
            "based on the output of the base model followed by a dense output layer with 1 unit\n",
            "per class using the softmax activation function Finally we create the Keras Model \n",
            "basemodel   kerasapplications xception Xception weightsimagenet \n",
            "                                                  includetop False\n",
            "avg  keraslayersGlobalAveragePooling2D basemodel output\n",
            "output  keraslayersDensenclasses  activation softmax avg\n",
            "model  kerasmodelsModelinputsbasemodel input outputsoutput\n",
            "As explained in Chapter 11  its usually a good idea to freeze the weights of the pre\n",
            "trained layers at least at the beginning of training\n",
            "for layer in basemodel layers\n",
            "    layertrainable   False\n",
            "Since our model uses the base models layers directly rather than\n",
            "the basemodel  object itself setting basemodeltrainableFalse\n",
            "would have no effect\n",
            "Finally we can compile the model and start training\n",
            "468  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksoptimizer   kerasoptimizers SGDlr02 momentum 09 decay001\n",
            "modelcompilelosssparsecategoricalcrossentropy  optimizer optimizer \n",
            "              metricsaccuracy \n",
            "history  modelfittrainset \n",
            "                    stepsperepoch int075  datasetsize   batchsize \n",
            "                    validationdata validset \n",
            "                    validationsteps int015  datasetsize   batchsize \n",
            "                    epochs5\n",
            "This will be very slow unless you have a GPU If you do not then\n",
            "you should run this chapters notebook in Colab using a GPU run\n",
            "time its free See the instructions at httpsgithubcomageron\n",
            "handsonml2 \n",
            "After training the model for a few epochs its validation accuracy should reach about\n",
            "7580 and stop making much progress This means that the top layers are now\n",
            "pretty well trained so we are ready to unfreeze all layers or you could try unfreezing\n",
            "just the top ones and continue training dont forget to compile the model when you\n",
            "freeze or unfreeze layers This time we use a much lower learning rate to avoid dam\n",
            "aging the pretrained weights\n",
            "for layer in basemodel layers\n",
            "    layertrainable   True\n",
            "optimizer   kerasoptimizers SGDlr001 momentum 09 decay0001\n",
            "modelcompile\n",
            "history  modelfit\n",
            "It will take a while but this model should reach around 95 accuracy on the test set\n",
            "With that you can start training amazing image classifiers But theres more to com\n",
            "puter vision than just classification For example what if you also want to know where\n",
            "the flower is in the picture Lets look at this now\n",
            "Classification  and Localization\n",
            "Localizing an object in a picture can be expressed as a regression task as discussed in\n",
            "Chapter 10  to predict a bounding box around the object a common approach is to\n",
            "predict the horizontal and vertical coordinates of the objects center as well as its\n",
            "height and width This means we have 4 numbers to predict It does not require much\n",
            "change to the model we just need to add a second dense output layer with 4 units\n",
            "typically on top of the global average pooling layer and it can be trained using the\n",
            "MSE loss\n",
            "basemodel   kerasapplications xception Xception weightsimagenet \n",
            "                                                  includetop False\n",
            "avg  keraslayersGlobalAveragePooling2D basemodel output\n",
            "classoutput   keraslayersDensenclasses  activation softmax avg\n",
            "Classification  and Localization  46922Crowdsourcing in Computer Vision  A Kovashka et al 2016\n",
            "locoutput   keraslayersDense4avg\n",
            "model  kerasmodelsModelinputsbasemodel input\n",
            "                           outputsclassoutput  locoutput \n",
            "modelcompilelosssparsecategoricalcrossentropy  mse\n",
            "              lossweights 08 02  depends on what you care most about\n",
            "              optimizer optimizer  metricsaccuracy \n",
            "But now we have a problem the flowers dataset does not have bounding boxes\n",
            "around the flowers So we need to add them ourselves This is often one of the hard\n",
            "est and most costly part of a Machine Learning project getting the labels Its a good\n",
            "idea to spend time looking for the right tools To annotate images with bounding\n",
            "boxes you may want to use an open source image labeling tool like VGG Image\n",
            "Annotator LabelImg OpenLabeler or ImgLab or perhaps a commercial tool like\n",
            "LabelBox or Supervisely Y ou may also want to consider crowdsourcing platforms\n",
            "such as Amazon Mechanical Turk or CrowdFlower if you have a very large number of\n",
            "images to annotate However it is quite a lot of work to setup a crowdsourcing plat\n",
            "form prepare the form to be sent to the workers to supervise them and ensure the\n",
            "quality of the bounding boxes they produce is good so make sure it is worth the\n",
            "effort if there are just a few thousand images to label and you dont plan to do this\n",
            "frequently it may be preferable to do it yourself Adriana Kovashka et al wrote a very\n",
            "practical paper22 about crowdsourcing in Computer Vision I recommend you check\n",
            "it out even if you do not plan to use crowdsourcing\n",
            "So lets suppose you obtained the bounding boxes for every image in the flowers data\n",
            "set for now we will assume there is a single bounding box per image you then need\n",
            "to create a dataset whose items will be batches of preprocessed images along with\n",
            "their class labels and their bounding boxes Each item should be a tuple of the form\n",
            "images classlabels boundingboxes  Then you are ready to train your\n",
            "model\n",
            "The bounding boxes should be normalized so that the horizontal\n",
            "and vertical coordinates as well as the height and width all range\n",
            "from 0 to 1 Also it is common to predict the square root of the\n",
            "height and width rather than the height and width directly this\n",
            "way a 10 pixel error for a large bounding box will not be penalized\n",
            "as much as a 10 pixel error for a small bounding box\n",
            "The MSE often works fairly well as a cost function to train the model but it is not a\n",
            "great metric to evaluate how well the model can predict bounding boxes The most\n",
            "common metric for this is the Intersection over Union IoU it is the area of overlap\n",
            "between the predicted bounding box and the target bounding box divided by the\n",
            "470  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksarea of their union see Figure 1423  In tfkeras it is implemented by the\n",
            "tfkerasmetricsMeanIoU  class\n",
            "Figure 1423 Intersection over Union IoU Metric for Bounding Boxes\n",
            "Classifying and localizing a single object is nice but what if the images contain multi\n",
            "ple objects as is often the case in the flowers dataset\n",
            "Object Detection\n",
            "The task of classifying and localizing multiple objects in an image is called object\n",
            "detection  Until a few years ago a common approach was to take a CNN that was\n",
            "trained to classify and locate a single object then slide it across the image as shown\n",
            "in Figure 1424  In this example the image was chopped into a 6  8 grid and we\n",
            "show a CNN the thick black rectangle sliding across all 3  3 regions When the\n",
            "CNN was looking at the top left of the image it detected part of the leftmost rose\n",
            "and then it detected that same rose again when it was first shifted one step to the\n",
            "right At the next step it started detecting part of the topmost rose and then it detec\n",
            "ted it again once it was shifted one more step to the right Y ou would then continue to\n",
            "slide the CNN through the whole image looking at all 3  3 regions Moreover since\n",
            "objects can have varying sizes you would also slide the CNN across regions of differ\n",
            "ent sizes For example once you are done with the 3  3 regions you might want to\n",
            "slide the CNN across all 4  4 regions as well\n",
            "Object Detection  471Figure 1424 Detecting Multiple Objects by Sliding a CNN Across the Image\n",
            "This technique is fairly straightforward but as you can see it will detect the same\n",
            "object multiple times at slightly different positions Some postprocessing will then\n",
            "be needed to get rid of all the unnecessary bounding boxes A common approach for\n",
            "this is called nonmax suppression \n",
            "First you need to add an extra objectness  output to your CNN to estimate the\n",
            "probability that a flower is indeed present in the image alternatively you could\n",
            "add a noflower class but this usually does not work as well It must use the\n",
            "sigmoid activation function and you can train it using the binarycrossen\n",
            "tropy  loss Then just get rid of all the bounding boxes for which the objectness\n",
            "score is below some threshold this will drop all the bounding boxes that dont\n",
            "actually contain a flower\n",
            "Second find the bounding box with the highest objectness score and get rid of\n",
            "all the other bounding boxes that overlap a lot with it eg with an IoU greater\n",
            "than 60 For example in Figure 1424  the bounding box with the max object\n",
            "ness score is the thick bounding box over the topmost rose the objectness score\n",
            "is represented by the thickness of the bounding boxes The other bounding box\n",
            "over that same rose overlaps a lot with the max bounding box so we will get rid\n",
            "of it\n",
            "472  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks23Fully Convolutional Networks for Semantic Segmentation  J Long E Shelhamer T Darrell 2015\n",
            "24There is one small exception a convolutional layer using V ALID padding will complain if the input size is\n",
            "smaller than the kernel size\n",
            "Third repeat step two until there are no more bounding boxes to get rid of\n",
            "This simple approach to object detection works pretty well but it requires running\n",
            "the CNN many times so it is quite slow Fortunately there is a much faster way to\n",
            "slide a CNN across an image using a Fully Convolutional Network \n",
            "Fully Convolutional Networks FCNs\n",
            "The idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al for\n",
            "semantic segmentation the task of classifying every pixel in an image according to\n",
            "the class of the object it belongs to They pointed out that you could replace the\n",
            "dense layers at the top of a CNN by convolutional layers To understand this lets look\n",
            "at an example suppose a dense layer with 200 neurons sits on top of a convolutional\n",
            "layer that outputs 100 feature maps each of size 7  7 this is the feature map size not\n",
            "the kernel size Each neuron will compute a weighted sum of all 100  7  7 activa\n",
            "tions from the convolutional layer plus a bias term Now lets see what happens if we\n",
            "replace the dense layer with a convolution layer using 200 filters each 7  7 and with\n",
            "V ALID padding This layer will output 200 feature maps each 1  1 since the kernel\n",
            "is exactly the size of the input feature maps and we are using V ALID padding In\n",
            "other words it will output 200 numbers just like the dense layer did and if you look\n",
            "closely at the computations performed by a convolutional layer you will notice that\n",
            "these numbers will be precisely the same as the dense layer produced The only differ\n",
            "ence is that the dense layers output was a tensor of shape batch size 200 while the\n",
            "convolutional layer will output a tensor of shape batch size 1 1 200\n",
            "To convert a dense layer to a convolutional layer the number of fil\n",
            "ters in the convolutional layer must be equal to the number of units\n",
            "in the dense layer the filter size must be equal to the size of the\n",
            "input feature maps and you must use V ALID padding The stride\n",
            "may be set to 1 or more as we will see shortly\n",
            "Why is this important Well while a dense layer expects a specific input size since it\n",
            "has one weight per input feature a convolutional layer will happily process images of\n",
            "any size24 however it does expect its inputs to have a specific number of channels\n",
            "since each kernel contains a different set of weights for each input channel Since an\n",
            "FCN contains only convolutional layers and pooling layers which have the same\n",
            "property it can be trained and executed on images of any size\n",
            "Object Detection  47325This assumes we used only SAME padding in the network indeed V ALID padding would reduce the size of\n",
            "the feature maps Moreover 448 can be neatly divided by 2 several times until we reach 7 without any round\n",
            "ing error If any layer uses a different stride than 1 or 2 then there may be some rounding error so again the\n",
            "feature maps may end up being smallerFor example suppose we already trained a CNN for flower classification and localiza\n",
            "tion It was trained on 224  224 images and it outputs 10 numbers outputs 0 to 4 are\n",
            "sent through the softmax activation function and this gives the class probabilities\n",
            "one per class output 5 is sent through the logistic activation function and this gives\n",
            "the objectness score outputs 6 to 9 do not use any activation function and they rep\n",
            "resent the bounding boxs center coordinates and its height and width We can now\n",
            "convert its dense layers to convolutional layers In fact we dont even need to retrain\n",
            "it we can just copy the weights from the dense layers to the convolutional layers\n",
            "Alternatively we could have converted the CNN into an FCN before training\n",
            "Now suppose the last convolutional layer before the output layer also called the bot\n",
            "tleneck layer outputs 7  7 feature maps when the network is fed a 224  224 image\n",
            "see the left side of Figure 1425  If we feed the FCN a 448  448 image see the right\n",
            "side of Figure 1425  the bottleneck layer will now output 14  14 feature maps25\n",
            "Since the dense output layer was replaced by a convolutional layer using 10 filters of\n",
            "size 7  7 V ALID padding and stride 1 the output will be composed of 10 features\n",
            "maps each of size 8  8 since 14  7  1  8 In other words the FCN will process\n",
            "the whole image only once and it will output an 8  8 grid where each cell contains 10\n",
            "numbers 5 class probabilities 1 objectness score and 4 bounding box coordinates\n",
            "Its exactly like taking the original CNN and sliding it across the image using 8 steps\n",
            "per row and 8 steps per column to visualize this imagine chopping the original\n",
            "image into a 14  14 grid then sliding a 7  7 window across this grid there will be 8\n",
            " 8  64 possible locations for the window hence 8  8 predictions However the\n",
            "FCN approach is much  more efficient since the network only looks at the image\n",
            "once In fact You Only Look Once  YOLO is the name of a very popular object detec\n",
            "tion architecture\n",
            "474  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks26Y ou Only Look Once Unified RealTime Object Detection  J Redmon S Divvala R Girshick A Farhadi\n",
            "2015\n",
            "27YOLO9000 Better Faster Stronger  J Redmon A Farhadi 2016\n",
            "28YOLOv3 An Incremental Improvement  J Redmon A Farhadi 2018\n",
            "Figure 1425 A Fully Convolutional Network Processing a Small Image left  and a\n",
            "Large One right\n",
            "You Only Look Once YOLO\n",
            "YOLO is an extremely fast and accurate object detection architecture proposed by\n",
            "Joseph Redmon et al in a 2015 paper26 and subsequently improved in 201627\n",
            "YOLOv2 and in 201828 YOLOv3 It is so fast that it can run in realtime on a video\n",
            "check out this nice demo \n",
            "YOLOv3s architecture is quite similar to the one we just discussed but with a few\n",
            "important differences\n",
            "Object Detection  475First it outputs 5 bounding boxes for each grid cell instead of just 1 and each\n",
            "bounding box comes with an objectness score It also outputs 20 class probabili\n",
            "ties per grid cell as it was trained on the PASCAL VOC dataset which contains\n",
            "20 classes Thats a total of 45 numbers per grid cell 5  4 bounding box coordi\n",
            "nates plus 5 objectness scores plus 20 class probabilities\n",
            "Second instead of predicting the absolute coordinates of the bounding box cen\n",
            "ters YOLOv3 predicts an offset relative to the coordinates of the grid cell where\n",
            "0 0 means the top left of that cell and 1 1 means the bottom right For each\n",
            "grid cell YOLOv3 is trained to predict only bounding boxes whose center lies in\n",
            "that cell but the bounding box itself generally extends well beyond the grid cell\n",
            "YOLOv3 applies the logistic activation function to the bounding box coordinates\n",
            "to ensure they remain in the 0 to 1 range\n",
            "Third before training the neural net YOLOv3 finds 5 representative bounding\n",
            "box dimensions called anchor boxes  or bounding box priors  it does this by\n",
            "applying the KMeans algorithm see  to the height and width of the training\n",
            "set bounding boxes For example if the training images contain many pedes\n",
            "trians then one of the anchor boxes will likely have the dimensions of a typical\n",
            "pedestrian Then when the neural net predicts 5 bounding boxes per grid cell it\n",
            "actually predicts how much to rescale each of the anchor boxes For example\n",
            "suppose one anchor box is 100 pixels tall and 50 pixels wide and the network\n",
            "predicts say a vertical rescaling factor of 15 and a horizontal rescaling of 09 for\n",
            "one of the grid cells this will result in a predicted bounding box of size 150  45\n",
            "pixels To be more precise for each grid cell and each anchor box the network\n",
            "predicts the log of the vertical and horizontal rescaling factors Having these pri\n",
            "ors makes the network more likely to predict bounding boxes of the appropriate\n",
            "dimensions and it also speeds up training since it will more quickly learn what\n",
            "reasonable bounding boxes look like\n",
            "Fourth the network is trained using images of different scales every few batches\n",
            "during training the network randomly chooses a new image dimension from\n",
            "330  330 to 608  608 pixels This allows the network to learn to detect objects\n",
            "at different scales Moreover it makes it possible to use YOLOv3 at different\n",
            "scales the smaller scale will be less accurate but faster than the larger scale so\n",
            "you can choose the right tradeoff for your use case\n",
            "There are a few more innovations you might be interested in such as the use of skip\n",
            "connections to recover some of the spatial resolution that is lost in the CNN we will\n",
            "discuss this shortly when we look at semantic segmentation Moreover in the 2016\n",
            "paper the authors introduce the YOLO9000 model that uses hierarchical classifica\n",
            "tion the model predicts a probability for each node in a visual hierarchy called Word\n",
            "Tree This makes it possible for the network to predict with high confidence that an\n",
            "image represents say a dog even though it is unsure what specific type of dog it is\n",
            "476  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksSo I encourage you to go ahead and read all three papers they are quite pleasant to\n",
            "read and it is an excellent example of how Deep Learning systems can be incremen\n",
            "tally improved\n",
            "Mean Average Precision mAP\n",
            "A very common metric used in object detection tasks is the mean Average Precision\n",
            "mAP Mean Average sounds a bit redundant doesnt it To understand this met\n",
            "ric lets go back to two classification metrics we discussed in Chapter 3  precision and\n",
            "recall Remember the tradeoff the higher the recall the lower the precision Y ou can\n",
            "visualize this in a PrecisionRecall curve see Figure 35  To summarize this curve\n",
            "into a single number we could compute its Area Under the Curve AUC But note\n",
            "that the PrecisionRecall curve may contain a few sections where precision actually\n",
            "goes up when recall increases especially at low recall values you can see this at the\n",
            "top left of Figure 35  This is one of the motivations for the mAP metric\n",
            "Suppose the classifier has a 90 precision at 10 recall but a 96 precision at 20\n",
            "recall theres really no tradeoff here it simply makes more sense to use the classifier\n",
            "at 20 recall rather than at 10 recall as you will get both higher recall and higher\n",
            "precision So instead of looking at the precision at 10 recall we should really be\n",
            "looking at the maximum  precision that the classifier can offer with at least  10 recall\n",
            "It would be 96 not 90 So one way to get a fair idea of the models performance is\n",
            "to compute the maximum precision you can get with at least 0 recall then 10\n",
            "recall 20 and so on up to 100 and then calculate the mean of these maximum\n",
            "precisions This is called the Average Precision  AP metric Now when there are more\n",
            "than 2 classes we can compute the AP for each class and then compute the mean AP\n",
            "mAP Thats it\n",
            "However in an object detection systems there is an additional level of complexity\n",
            "what if the system detected the correct class but at the wrong location ie the\n",
            "bounding box is completely off Surely we should not count this as a positive predic\n",
            "tion So one approach is to define an IOU threshold for example we may consider\n",
            "that a prediction is correct only if the IOU is greater than say 05 and the predicted\n",
            "class is correct The corresponding mAP is generally noted mAP05 or mAP50\n",
            "or sometimes just AP50 In some competitions such as the Pascal VOC challenge\n",
            "this is what is done In others such as the COCO competition the mAP is computed\n",
            "for different IOU thresholds 050 055 060  095 and the final metric is the\n",
            "mean of all these mAPs noted AP5095 or AP5000595 Y es thats a mean\n",
            "mean average\n",
            "Several YOLO implementations built using TensorFlow are available on github some\n",
            "with pretrained weights At the time of writing they are based on TensorFlow 1 but\n",
            "by the time you read this TF 2 implementations will certainly be available Moreover\n",
            "other object detection models are available in the TensorFlow Models project many\n",
            "Object Detection  47729SSD Single Shot MultiBox Detector  Wei Liu et al 2015\n",
            "30Faster RCNN Towards RealTime Object Detection with Region Proposal Networks  Shaoqing Ren et al\n",
            "2015with pretrained weights and some have even been ported to TF Hub making them\n",
            "extremely easy to use such as SSD29 and FasterRCNN 30 which are both quite popu\n",
            "lar SSD is also a single shot detection model quite similar to YOLO while Faster R\n",
            "CNN is more complex the image first goes through a CNN and the output is passed\n",
            "to a Region Proposal Network RPN which proposes bounding boxes that are most\n",
            "likely to contain an object and a classifier is run for each bounding box based on the\n",
            "cropped output of the CNN\n",
            "The choice of detection system depends on many factors speed accuracy available\n",
            "pretrained models training time complexity etc The papers contain tables of met\n",
            "rics but there is quite a lot of variability in the testing environments and the technol\n",
            "ogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\n",
            "most people and remain valid for more than a few months\n",
            "Great So we can locate objects by drawing bounding boxes around them But per\n",
            "haps you might want to be a bit more precise Lets see how to go down to the pixel\n",
            "level\n",
            "Semantic Segmentation\n",
            "In semantic segmentation  each pixel is classified according to the class of the object it\n",
            "belongs to eg road car pedestrian building etc as shown in Figure 1426  Note\n",
            "that different objects of the same class are not distinguished For example all the bicy\n",
            "cles on the right side of the segmented image end up as one big lump of pixels The\n",
            "main difficulty in this task is that when images go through a regular CNN they grad\n",
            "ually lose their spatial resolution due to the layers with strides greater than 1 so a\n",
            "regular CNN may end up knowing that theres a person in the image somewhere in\n",
            "the bottom left of the image but it will not be much more precise than that\n",
            "478  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks31This type of layer is sometimes referred to as a deconvolution layer  but it does not perform what mathemati\n",
            "cians call a deconvolution so this name should be avoided\n",
            "Figure 1426 Semantic segmentation\n",
            "Just like for object detection there are many different approaches to tackle this prob\n",
            "lem some quite complex However a fairly simple solution was proposed in the 2015\n",
            "paper by Jonathan Long et al we discussed earlier They start by taking a pretrained\n",
            "CNN and turning into an FCN as discussed earlier The CNN applies a stride of 32 to\n",
            "the input image overall ie if you add up all the strides greater than 1 meaning the\n",
            "last layer outputs feature maps that are 32 times smaller than the input image This is\n",
            "clearly too coarse so they add a single upsampling layer  that multiplies the resolution\n",
            "by 32 There are several solutions available for upsampling increasing the size of an\n",
            "image such as bilinear interpolation but it only works reasonably well up to 4 or\n",
            "8 Instead they used a transposed convolutional layer 31 it is equivalent to first\n",
            "stretching the image by inserting empty rows and columns full of zeros then per\n",
            "forming a regular convolution see Figure 1427  Alternatively some people prefer to\n",
            "think of it as a regular convolutional layer that uses fractional strides eg 12 in\n",
            "Figure 1427  The transposed convolutional layer  can be initialized to perform some\n",
            "thing close to linear interpolation but since it is a trainable layer it will learn to do\n",
            "better during training\n",
            "Semantic Segmentation  479Figure 1427 Upsampling Using a Transpose Convolutional Layer\n",
            "In a transposed convolution layer the stride defines how much the\n",
            "input will be stretched not the size of the filter steps so the larger\n",
            "the stride the larger the output unlike for convolutional layers or\n",
            "pooling layers\n",
            "TensorFlow Convolution Operations\n",
            "TensorFlow also offers a few other kinds of convolutional layers\n",
            "keraslayersConv1D  creates a convolutional layer for 1D inputs such as time\n",
            "series or text sequences of letters or words as we will see in \n",
            "keraslayersConv3D  creates a convolutional layer for 3D inputs such as 3D\n",
            "PET scan\n",
            "Setting the dilationrate  hyperparameter of any convolutional layer to a value\n",
            "of 2 or more creates an trous convolutional layer   trous is French for with\n",
            "holes This is equivalent to using a regular convolutional layer with a filter dila\n",
            "ted by inserting rows and columns of zeros ie holes For example a 1  3 filter\n",
            "equal to 123  may be dilated with a dilation rate  of 4 resulting in a dilated\n",
            "filter  1 0 0 0 2 0 0 0 3  This allows the convolutional layer to\n",
            "480  Chapter 14 Deep Computer Vision Using Convolutional Neural Networkshave a larger receptive field at no computational price and using no extra param\n",
            "eters\n",
            "tfnndepthwiseconv2d  can be used to create a depthwise convolutional layer\n",
            "but you need to create the variables yourself It applies every filter to every\n",
            "individual input channel independently Thus if there are fn filters and fn input\n",
            "channels then this will output fn  fn feature maps\n",
            "This solution is okay but still too imprecise To do better the authors added skip con\n",
            "nections from lower layers for example they upsampled the output image by a factor\n",
            "of 2 instead of 32 and they added the output of a lower layer that had this double\n",
            "resolution Then they upsampled the result by a factor of 16 leading to a total upsam\n",
            "pling factor of 32 see Figure 1428  This recovered some of the spatial resolution\n",
            "that was lost in earlier pooling layers In their best architecture they used a second\n",
            "similar skip connection to recover even finer details from an even lower layer in\n",
            "short the output of the original CNN goes through the following extra steps upscale\n",
            "2 add the output of a lower layer of the appropriate scale upscale 2 add the out\n",
            "put of an even lower layer and finally upscale 8 It is even possible to scale up\n",
            "beyond the size of the original image this can be used to increase the resolution of an\n",
            "image which is a technique called superresolution \n",
            "Figure 1428 Skip layers recover some spatial resolution from lower layers\n",
            "Once again many github repositories provide TensorFlow implementations of\n",
            "semantic segmentation TensorFlow 1 for now and you will even find a pretrained\n",
            "instance segmentation  model in the TensorFlow Models project Instance segmenta\n",
            "tion is similar to semantic segmentation but instead of merging all objects of the\n",
            "same class into one big lump each object is distinguished from the others eg it\n",
            "identifies each individual bicycle At the present they provide multiple implementa\n",
            "tions of the Mask RCNN  architecture which was proposed in a 2017 paper  it\n",
            "extends the Faster RCNN model by additionally producing a pixelmask for each\n",
            "bounding box So not only do you get a bounding box around each object with a set\n",
            "of estimated class probabilities you also get a pixel mask that locates pixels in the\n",
            "bounding box that belong to the object\n",
            "Semantic Segmentation  48132Matrix Capsules with EM Routing  G Hinton S Sabour N Frosst 2018As you can see the field of Deep Computer Vision is vast and moving fast with all\n",
            "sorts of architectures popping out every year all based on Convolutional Neural Net\n",
            "works The progress made in just a few years has been astounding and researchers\n",
            "are now focusing on harder and harder problems such as adversarial learning  which\n",
            "attempts to make the network more resistant to images designed to fool it explaina\n",
            "bility understanding why the network makes a specific classification realistic image\n",
            "generation  which we will come back to in  singleshot learning  a system that can\n",
            "recognize an object after it has seen it just once and much more Some even explore\n",
            "completely novel architectures such as Geoffrey Hintons capsule networks32 I pre\n",
            "sented them in a couple videos  with the corresponding code in a notebook Now on\n",
            "to the next chapter where we will look at how to process sequential data such as time\n",
            "series using Recurrent Neural Networks and Convolutional Neural Networks\n",
            "Exercises\n",
            "1What are the advantages of a CNN over a fully connected DNN for image classi\n",
            "fication\n",
            "2Consider a CNN composed of three convolutional layers each with 3  3 kernels\n",
            "a stride of 2 and SAME padding The lowest layer outputs 100 feature maps the\n",
            "middle one outputs 200 and the top one outputs 400 The input images are RGB\n",
            "images of 200  300 pixels What is the total number of parameters in the CNN\n",
            "If we are using 32bit floats at least how much RAM will this network require\n",
            "when making a prediction for a single instance What about when training on a\n",
            "minibatch of 50 images\n",
            "3If your GPU runs out of memory while training a CNN what are five things you\n",
            "could try to solve the problem\n",
            "4Why would you want to add a max pooling layer rather than a convolutional\n",
            "layer with the same stride\n",
            "5When would you want to add a local response normalization  layer\n",
            "6Can you name the main innovations in AlexNet compared to LeNet5 What\n",
            "about the main innovations in GoogLeNet ResNet SENet and Xception\n",
            "7What is a Fully Convolutional Network How can you convert a dense layer into\n",
            "a convolutional layer\n",
            "8What is the main technical difficulty of semantic segmentation\n",
            "9Build your own CNN from scratch and try to achieve the highest possible accu\n",
            "racy on MNIST\n",
            "482  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks10Use transfer learning for large image classification\n",
            "aCreate a training set containing at least 100 images per class For example you\n",
            "could classify your own pictures based on the location beach mountain city\n",
            "etc or alternatively you can just use an existing dataset eg from Tensor\n",
            "Flow Datasets\n",
            "bSplit it into a training set a validation set and a test set\n",
            "cBuild the input pipeline including the appropriate preprocessing operations\n",
            "and optionally add data augmentation\n",
            "dFinetune a pretrained model on this dataset\n",
            "11Go through TensorFlows DeepDream tutorial  It is a fun way to familiarize your\n",
            "self with various ways of visualizing the patterns learned by a CNN and to gener\n",
            "ate art using Deep Learning\n",
            "Solutions to these exercises are available in \n",
            "Exercises  483About the Author\n",
            "Aurlien Gron  is a Machine Learning consultant A former Googler he led the Y ou\n",
            "Tube video classification team from 2013 to 2016 He was also a founder and CTO of\n",
            "Wifirst from 2002 to 2012 a leading Wireless ISP in France and a founder and CTO\n",
            "of Polyconseil in 2001 the firm that now manages the electric car sharing service\n",
            "Autolib \n",
            "Before this he worked as an engineer in a variety of domains finance JP Morgan and\n",
            "Socit Gnrale defense Canadas DOD and healthcare blood transfusion He\n",
            "published a few technical books on C WiFi and internet architectures and was\n",
            "a Computer Science lecturer in a French engineering school\n",
            "A few fun facts he taught his three children to count in binary with their fingers up\n",
            "to 1023 he studied microbiology and evolutionary genetics before going into soft\n",
            "ware engineering and his parachute didnt open on the second jump\n",
            "Colophon\n",
            "The animal on the cover of HandsOn Machine Learning with ScikitLearn and Ten\n",
            "sorFlow  is the fire salamander  Salamandra salamandra  an amphibian found across\n",
            "most of Europe Its black glossy skin features large yellow spots on the head and\n",
            "back signaling the presence of alkaloid toxins This is a possible source of this\n",
            "amphibians common name contact with these toxins which they can also spray\n",
            "short distances causes convulsions and hyperventilation Either the painful poisons\n",
            "or the moistness of the salamanders skin or both led to a misguided belief that these\n",
            "creatures not only could survive being placed in fire but could extinguish it as well\n",
            "Fire salamanders live in shaded forests hiding in moist crevices and under logs near\n",
            "the pools or other freshwater bodies that facilitate their breeding Though they spend\n",
            "most of their life on land they give birth to their young in water They subsist mostly\n",
            "on a diet of insects spiders slugs and worms Fire salamanders can grow up to a foot\n",
            "in length and in captivity may live as long as 50 years\n",
            "The fire salamanders numbers have been reduced by destruction of their forest habi\n",
            "tat and capture for the pet trade but the greatest threat is the susceptibility of their\n",
            "moisturepermeable skin to pollutants and microbes Since 2014 they have become\n",
            "extinct in parts of the Netherlands and Belgium due to an introduced fungus\n",
            "Many of the animals on OReilly covers are endangered all of them are important to\n",
            "the world To learn more about how you can help go to animalsoreillycom \n",
            "The cover image is from Woods Illustrated Natural History  The cover fonts are URW\n",
            "Typewriter and Guardian Sans The text font is Adobe Minion Pro the heading font\n",
            "is Adobe Myriad Condensed and the code font is Dalton Maags Ubuntu Mono\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: chunk the text with overlapping\n",
        "\n",
        "def chunk_text_with_overlap(text, chunk_size, overlap):\n",
        "  \"\"\"\n",
        "  Chunks the given text with overlapping.\n",
        "\n",
        "  Args:\n",
        "    text: The text to chunk.\n",
        "    chunk_size: The size of each chunk.\n",
        "    overlap: The number of characters to overlap between chunks.\n",
        "\n",
        "  Returns:\n",
        "    A list of text chunks.\n",
        "  \"\"\"\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  while start < len(text):\n",
        "    end = min(start + chunk_size, len(text))\n",
        "    chunks.append(text[start:end])\n",
        "    start += chunk_size - overlap\n",
        "  return chunks\n",
        "\n",
        "# Example usage:\n",
        "chunk_size = 1000\n",
        "overlap = 100\n",
        "chunks = chunk_text_with_overlap(text, chunk_size, overlap)\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPwJj3r0nxVE",
        "outputId": "12bf0da0-e18c-4220-be44-fd5e5518ba8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aurlien GronHandson Machine Learning with\\nScikitLearn Keras and\\nTensorFlow\\nConcepts Tools and Techniques to\\nBuild Intelligent SystemsSECOND EDITION\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing9781492032649\\nLSIHandson Machine Learning with ScikitLearn Keras and TensorFlow\\nby Aurlien Gron\\nCopyright  2019 Aurlien Gron All rights reserved\\nPrinted in the United States of America\\nPublished by OReilly Media Inc 1005 Gravenstein Highway North Sebastopol CA 95472\\nOReilly books may be purchased for educational business or sales promotional use Online editions are\\nalso available for most titles  httporeillycom  For more information contact our corporateinstitutional\\nsales department 8009989938 or corporateoreillycom \\nEditor  Nicole Tache\\nInterior Designer  David FutatoCover Designer  Karen Montgomery\\nIllustrator  Rebecca Demarest\\nJune 2019  Second Edition\\nRevision History for the Early Release\\n20181105 First Release\\n20190124 Second Release\\n20190307 Third Release', ' History for the Early Release\\n20181105 First Release\\n20190124 Second Release\\n20190307 Third Release\\n20190329 Fourth Release\\n20190422 Fifth Release\\nSee httporeillycomcatalogerratacspisbn9781492032649  for release details\\nThe OReilly logo is a registered trademark of OReilly Media Inc Handson Machine Learning with\\nScikitLearn Keras and TensorFlow  the cover image and related trade dress are trademarks of OReilly\\nMedia Inc\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate the publisher and the author disclaim all responsibility\\nfor errors or omissions including without limitation responsibility for damages resulting from the use of\\nor reliance on this work Use of the information and instructions contained in this work is at your own\\nrisk If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others it ', 'ns or describes is subject to open source\\nlicenses or the intellectual property rights of others it is your responsibility to ensure that your use\\nthereof complies with such licenses andor rightsTable of Contents\\nPreface                                                                        xi\\nPart I The Fundamentals of Machine Learning\\n1The Machine Learning Landscape                                              3\\nWhat Is Machine Learning                                                                                           4\\nWhy Use Machine Learning                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervisedUnsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstanceBased V', '                                                                                  15\\nInstanceBased Versus ModelBased Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoorQuality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back      ', '                                                                              30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2EndtoEnd Machine Learning Project                                         37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                            ', ' Picture                                                                                                  39\\niiiFrame the Problem                                                                                                    39\\nSelect a Performance Measure                                                                                  42\\nCheck the Assumptions                                                                                             45\\nGet the Data                                                                                                                    45\\nCreate the Workspace                                                                                                45\\nDownload the Data                                                                                                    49\\nTake a Quick Look at the Data Structure                                                                50\\nCreate a Test Set                                             ', '                                   50\\nCreate a Test Set                                                                                                          54\\nDiscover and Visualize the Data to Gain Insights                                                     58\\nVisualizing Geographical Data                                                                                 59\\nLooking for Correlations                                                                                           62\\nExperimenting with Attribute Combinations                                                        65\\nPrepare the Data for Machine Learning Algorithms                                                66\\nData Cleaning                                                                                                             67\\nHandling Text and Categorical Attributes                                                              69\\nCustom Transformers                                                     ', '                         69\\nCustom Transformers                                                                                                71\\nFeature Scaling                                                                                                            72\\nTransformation Pipelines                                                                                          73\\nSelect and Train a Model                                                                                               75\\nTraining and Evaluating on the Training Set                                                         75\\nBetter Evaluation Using CrossValidation                                                              76\\nFineTune Y our Model                                                                                                  79\\nGrid Search                                                                                                                 79\\nRandomized Search                 ', '                                                               79\\nRandomized Search                                                                                                   81\\nEnsemble Methods                                                                                                     82\\nAnalyze the Best Models and Their Errors                                                             82\\nEvaluate Y our System on the Test Set                                                                      83\\nLaunch Monitor and Maintain Y our System                                                            84\\nTry It Out                                                                                                                       85\\nExercises                                                                                                                          85\\n3Classification                                                               87\\nMNIST                          ', 'ion                                                               87\\nMNIST                                                                                                                             87\\nTraining a Binary Classifier                                                                                          90\\nPerformance Measures                                                                                                  90\\nMeasuring Accuracy Using CrossValidation                                                        91\\nConfusion Matrix                                                                                                       92\\nPrecision and Recall                                                                                                   94\\nPrecisionRecall Tradeoff                                                                                          95\\nThe ROC Curve                                                                                   ', ' 95\\nThe ROC Curve                                                                                                          99\\nMulticlass Classification                                                                                             102\\nError Analysis                                                                                                              104\\niv  Table of ContentsMultilabel Classification                                                                                             108\\nMultioutput Classification                                                                                          109\\nExercises                                                                                                                        110\\n4Training Models                                                           113\\nLinear Regression                                                                                                        114\\nThe Normal Equation     ', '                                                                        114\\nThe Normal Equation                                                                                              116\\nComputational Complexity                                                                                    119\\nGradient Descent                                                                                                         119\\nBatch Gradient Descent                                                                                           123\\nStochastic Gradient Descent                                                                                   126\\nMinibatch Gradient Descent                                                                                 129\\nPolynomial Regression                                                                                                130\\nLearning Curves                                                                                            ', 'g Curves                                                                                                           132\\nRegularized Linear Models                                                                                         136\\nRidge Regression                                                                                                      137\\nLasso Regression                                                                                                      139\\nElastic Net                                                                                                                 142\\nEarly Stopping                                                                                                          142\\nLogistic Regression                                                                                                      144\\nEstimating Probabilities                                                                                          144\\nTraining and Cost Functi', '                                                                        144\\nTraining and Cost Function                                                                                   145\\nDecision Boundaries                                                                                                146\\nSoftmax Regression                                                                                                  149\\nExercises                                                                                                                        153\\n5Support Vector Machines                                                   155\\nLinear SVM Classification                                                                                          155\\nSoft Margin Classification                                                                                       156\\nNonlinear SVM Classification                                                                                   159\\nPolynomia', 'tion                                                                                   159\\nPolynomial Kernel                                                                                                   160\\nAdding Similarity Features                                                                                     161\\nGaussian RBF Kernel                                                                                               162\\nComputational Complexity                                                                                    163\\nSVM Regression                                                                                                           164\\nUnder the Hood                                                                                                           166\\nDecision Function and Predictions                                                                       166\\nTraining Objective                                                                    ', '          166\\nTraining Objective                                                                                                   167\\nQuadratic Programming                                                                                         169\\nThe Dual Problem                                                                                                    170\\nKernelized SVM                                                                                                       171\\nOnline SVMs                                                                                                            174\\nTable of Contents  vExercises                                                                                                                        175\\n6Decision Trees                                                            177\\nTraining and Visualizing a Decision Tree                                                                177\\nMaking Predictions                             ', '                                                 177\\nMaking Predictions                                                                                                     179\\nEstimating Class Probabilities                                                                                   181\\nThe CART Training Algorithm                                                                                 182\\nComputational Complexity                                                                                        183\\nGini Impurity or Entropy                                                                                         183\\nRegularization Hyperparameters                                                                              184\\nRegression                                                                                                                     185\\nInstability                                                                                                            ', '                                                                                                              188\\nExercises                                                                                                                        189\\n7Ensemble Learning and Random Forests                                      191\\nVoting Classifiers                                                                                                         192\\nBagging and Pasting                                                                                                    195\\nBagging and Pasting in ScikitLearn                                                                     196\\nOutofBag Evaluation                                                                                            197\\nRandom Patches and Random Subspaces                                                                198\\nRandom Forests                                                                                     ', '\\nRandom Forests                                                                                                           199\\nExtraTrees                                                                                                                200\\nFeature Importance                                                                                                  200\\nBoosting                                                                                                                        201\\nAdaBoost                                                                                                                   202\\nGradient Boosting                                                                                                    205\\nStacking                                                                                                                         210\\nExercises                                                                                                          ', '                                                                                                                  213\\n8Dimensionality Reduction                                                  215\\nThe Curse of Dimensionality                                                                                     216\\nMain Approaches for Dimensionality Reduction                                                   218\\nProjection                                                                                                                  218\\nManifold Learning                                                                                                   220\\nPCA                                                                                                                                222\\nPreserving the Variance                                                                                          222\\nPrincipal Components                                                                    ', '        222\\nPrincipal Components                                                                                            223\\nProjecting Down to d Dimensions                                                                        224\\nUsing ScikitLearn                                                                                                    224\\nExplained Variance Ratio                                                                                        225\\nChoosing the Right Number of Dimensions                                                       225\\nPCA for Compression                                                                                             226\\nvi  Table of ContentsRandomized PCA                                                                                                    227\\nIncremental PCA                                                                                                     227\\nKernel PCA                                             ', '                                         227\\nKernel PCA                                                                                                                   228\\nSelecting a Kernel and Tuning Hyperparameters                                                229\\nLLE                                                                                                                                 232\\nOther Dimensionality Reduction Techniques                                                         234\\nExercises                                                                                                                        235\\n9Unsupervised Learning Techniques                                          237\\nClustering                                                                                                                      238\\nKMeans                                                                                                                    240\\nLimits of KMeans     ', '                                                                           240\\nLimits of KMeans                                                                                                   250\\nUsing clustering for image segmentation                                                             251\\nUsing Clustering for Preprocessing                                                                       252\\nUsing Clustering for SemiSupervised Learning                                                 254\\nDBSCAN                                                                                                                   256\\nOther Clustering Algorithms                                                                                 259\\nGaussian Mixtures                                                                                                       260\\nAnomaly Detection using Gaussian Mixtures                                                     266\\nSelecting the Number of Clusters ', 'n Mixtures                                                     266\\nSelecting the Number of Clusters                                                                          267\\nBayesian Gaussian Mixture Models                                                                      270\\nOther Anomaly Detection and Novelty Detection Algorithms                        274\\nPart II Neural Networks and Deep Learning\\n10 Introduction to Artificial  Neural Networks with Keras                           277\\nFrom Biological to Artificial Neurons                                                                      278\\nBiological Neurons                                                                                                   279\\nLogical Computations with Neurons                                                                    281\\nThe Perceptron                                                                                                         281\\nMultiLayer Perceptron and Backpropagation  ', '                                                     281\\nMultiLayer Perceptron and Backpropagation                                                    286\\nRegression MLPs                                                                                                      289\\nClassification MLPs                                                                                                 290\\nImplementing MLPs with Keras                                                                                292\\nInstalling TensorFlow 2                                                                                           293\\nBuilding an Image Classifier Using the Sequential API                                     294\\nBuilding a Regression MLP Using the Sequential API                                       303\\nBuilding Complex Models Using the Functional API                                        304\\nBuilding Dynamic Models Using the Subclassing API                                       309\\nSavi', '304\\nBuilding Dynamic Models Using the Subclassing API                                       309\\nSaving and Restoring a Model                                                                                311\\nUsing Callbacks                                                                                                        311\\nTable of Contents  viiVisualization Using TensorBoard                                                                          313\\nFineTuning Neural Network Hyperparameters                                                     315\\nNumber of Hidden Layers                                                                                      319\\nNumber of Neurons per Hidden Layer                                                                 320\\nLearning Rate Batch Size and Other Hyperparameters                                     320\\nExercises                                                                                                                        322\\n', '                                                                                                322\\n11 Training Deep Neural Networks                                             325\\nVanishingExploding Gradients Problems                                                              326\\nGlorot and He Initialization                                                                                   327\\nNonsaturating Activation Functions                                                                     329\\nBatch Normalization                                                                                                333\\nGradient Clipping                                                                                                    338\\nReusing Pretrained Layers                                                                                          339\\nTransfer Learning With Keras                                                                                341\\nUnsupervised Pretrainin', '                                                                         341\\nUnsupervised Pretraining                                                                                       343\\nPretraining on an Auxiliary Task                                                                           344\\nFaster Optimizers                                                                                                         344\\nMomentum Optimization                                                                                      345\\nNesterov Accelerated Gradient                                                                              346\\nAdaGrad                                                                                                                    347\\nRMSProp                                                                                                                   349\\nAdam and Nadam Optimization                                                                     ', '349\\nAdam and Nadam Optimization                                                                           349\\nLearning Rate Scheduling                                                                                       352\\nAvoiding Overfitting Through Regularization                                                        356\\n1 and 2 Regularization                                                                                           356\\nDropout                                                                                                                     357\\nMonteCarlo MC Dropout                                                                                  360\\nMaxNorm Regularization                                                                                      362\\nSummary and Practical Guidelines                                                                           363\\nExercises                                                                                        ', '63\\nExercises                                                                                                                        364\\n12 Custom Models and Training with TensorFlow                                 367\\nA Quick Tour of TensorFlow                                                                                     368\\nUsing TensorFlow like NumPy                                                                                  371\\nTensors and Operations                                                                                          371\\nTensors and NumPy                                                                                                373\\nType Conversions                                                                                                     374\\nVariables                                                                                                                    374\\nOther Data Structures                                                  ', '                         374\\nOther Data Structures                                                                                             375\\nCustomizing Models and Training Algorithms                                                      376\\nCustom Loss Functions                                                                                           376\\nviii  Table of ContentsSaving and Loading Models That Contain Custom Components                    377\\nCustom Activation Functions Initializers Regularizers and Constraints     379\\nCustom Metrics                                                                                                        380\\nCustom Layers                                                                                                          383\\nCustom Models                                                                                                        386\\nLosses and Metrics Based on Model Internals                                               ', '      386\\nLosses and Metrics Based on Model Internals                                                     388\\nComputing Gradients Using Autodiff                                                                   389\\nCustom Training Loops                                                                                          393\\nTensorFlow Functions and Graphs                                                                           396\\nAutograph and Tracing                                                                                           398\\nTF Function Rules                                                                                                    400\\n13 Loading and Preprocessing Data with TensorFlow                              403\\nThe Data API                                                                                                                404\\nChaining Transformations                                                                                      40', 'nsformations                                                                                      405\\nShuffling the Data                                                                                                    406\\nPreprocessing the Data                                                                                            409\\nPutting Everything Together                                                                                  410\\nPrefetching                                                                                                                411\\nUsing the Dataset With tfkeras                                                                              413\\nThe TFRecord Format                                                                                                414\\nCompressed TFRecord Files                                                                                   415\\nA Brief Introduction to Protocol Buffers                                   ', '                     415\\nA Brief Introduction to Protocol Buffers                                                              415\\nTensorFlow Protobufs                                                                                             416\\nLoading and Parsing Examples                                                                              418\\nHandling Lists of Lists Using the SequenceExample  Protobuf                          419\\nThe Features API                                                                                                         420\\nCategorical Features                                                                                                 421\\nCrossed Categorical Features                                                                                 421\\nEncoding Categorical Features Using OneHot Vectors                                    422\\nEncoding Categorical Features Using Embeddings                                            423\\nUsing Feat', 'ding Categorical Features Using Embeddings                                            423\\nUsing Feature Columns for Parsing                                                                      426\\nUsing Feature Columns in Y our Models                                                               426\\nTF Transform                                                                                                               428\\nThe TensorFlow Datasets TFDS Project                                                                429\\n14 Deep Computer Vision Using Convolutional Neural Networks                    431\\nThe Architecture of the Visual Cortex                                                                     432\\nConvolutional Layer                                                                                                    434\\nFilters                                                                                                                         436\\nStacking Multiple Fe', '                                                                            436\\nStacking Multiple Feature Maps                                                                             437\\nTensorFlow Implementation                                                                                  439\\nTable of Contents  ixMemory Requirements                                                                                           441\\nPooling Layer                                                                                                                442\\nTensorFlow Implementation                                                                                  444\\nCNN Architectures                                                                                                      446\\nLeNet5                                                                                                                      449\\nAlexNet                                                                 ', '                        449\\nAlexNet                                                                                                                      450\\nGoogLeNet                                                                                                                452\\nVGGNet                                                                                                                     456\\nResNet                                                                                                                        457\\nXception                                                                                                                    459\\nSENet                                                                                                                         461\\nImplementing a ResNet34 CNN Using Keras                                                        464\\nUsing Pretrained Models From Keras                                                                      ', 'g Pretrained Models From Keras                                                                      465\\nPretrained Models for Transfer Learning                                                                 467\\nClassification and Localization                                                                                  469\\nObject Detection                                                                                                          471\\nFully Convolutional Networks FCNs                                                                 473\\nY ou Only Look Once YOLO                                                                                475\\nSemantic Segmentation                                                                                               478\\nExercises                                                                                                                        482\\nx  Table of Contents1Available on Hintons home page at httpwwwcstorontoeduhinton \\n', '              482\\nx  Table of Contents1Available on Hintons home page at httpwwwcstorontoeduhinton \\n2Despite the fact that Y ann Lecuns deep convolutional neural networks had worked well for image recognition\\nsince the 1990s although they were not as general purposePreface\\nThe Machine Learning Tsunami\\nIn 2006 Geoffrey Hinton et al published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with stateoftheart precision\\n98 They branded this technique Deep Learning  Training a deep neural net\\nwas widely considered impossible at the time2 and most researchers had abandoned\\nthe idea since the 1990s This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible but capable of mindblowing achievements that no other Machine Learning\\nML technique could hope to match with the help of tremendous computing power\\nand great amounts of data This enthusiasm soon extended to ma', 'the help of tremendous computing power\\nand great amounts of data This enthusiasm soon extended to many other areas of\\nMachine Learning\\nFastforward 10 years and Machine Learning has conquered the industry it is now at\\nthe heart of much of the magic in todays hightech products ranking your web\\nsearch results powering your smartphones speech recognition recommending vid\\neos and beating the world champion at the game of Go Before you know it it will be\\ndriving your car\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty\\nPerhaps you would like to give your homemade robot a brain of its own Make it rec\\nognize faces Or learn to walk around\\nxiOr maybe your company has tons of data user logs financial data production data\\nmachine sensor data hotline stats HR reports etc and more than likely you could\\nunearth some hidden gems if you just knew where to look for example\\nSegment customers and find the best marketing strategy for', 'f you just knew where to look for example\\nSegment customers and find the best marketing strategy for each group\\nRecommend products for each client based on what similar clients bought\\nDetect which transactions are likely to be fraudulent\\nForecast next years revenue\\nAnd more\\nWhatever the reason you have decided to learn Machine Learning and implement it\\nin your projects Great idea\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning Its goal\\nis to give you the concepts the intuitions and the tools you need to actually imple\\nment programs capable of learning from data \\nWe will cover a large number of techniques from the simplest and most commonly\\nused such as linear regression to some of the Deep Learning techniques that regu\\nlarly win competitions\\nRather than implementing our own toy versions of each algorithm we will be using\\nactual productionready Python frameworks\\nScikitLearn  is very easy to use yet it implements many Machine Learning algo\\nr', 'dy Python frameworks\\nScikitLearn  is very easy to use yet it implements many Machine Learning algo\\nrithms efficiently so it makes for a great entry point to learn Machine Learning\\nTensorFlow  is a more complex library for distributed numerical computation It\\nmakes it possible to train and run very large neural networks efficiently by dis\\ntributing the computations across potentially hundreds of multiGPU servers\\nTensorFlow was created at Google and supports many of their largescale\\nMachine Learning applications It was open sourced in November 2015\\nKeras  is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks It can run on top of either TensorFlow Theano or Micro\\nsoft Cognitive Toolkit formerly known as CNTK TensorFlow comes with its\\nown implementation of this API called tfkeras  which provides support for some\\nadvanced TensorFlow features eg to efficiently load data\\nThe book favors a handson approach growing an intuitive understanding of\\nMachine Lea', 'ently load data\\nThe book favors a handson approach growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory\\nWhile you can read this book without picking up your laptop we highly recommend\\nxii  Prefaceyou experiment with the code examples available online as Jupyter notebooks at\\nhttpsgithubcomageronhandsonml2 \\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Pythons main scientific libraries in particular NumPy  Pandas  and\\nMatplotlib \\nAlso if you care about whats under the hood you should have a reasonable under\\nstanding of collegelevel math as well calculus linear algebra probabilities and sta\\ntistics\\nIf you dont know Python yet httplearnpythonorg  is a great place to start The offi\\ncial tutorial on pythonorg  is also quite good\\nIf you have never used Jupyter Chapter 2  will guide you through installation and the\\nbasics it is a great tool to have in your toolb', 'apter 2  will guide you through installation and the\\nbasics it is a great tool to have in your toolbox\\nIf you are not familiar with Pythons scientific libraries the provided Jupyter note\\nbooks include a few tutorials There is also a quick math tutorial for linear algebra\\nRoadmap\\nThis book is organized in two parts Part I The Fundamentals of Machine Learning \\ncovers the following topics\\nWhat is Machine Learning What problems does it try to solve What are the\\nmain categories and fundamental concepts of Machine Learning systems\\nThe main steps in a typical Machine Learning project\\nLearning by fitting a model to data\\nOptimizing a cost function\\nHandling cleaning and preparing data\\nSelecting and engineering features\\nSelecting a model and tuning hyperparameters using crossvalidation\\nThe main challenges of Machine Learning in particular underfitting and overfit\\nting the biasvariance tradeoff\\nReducing the dimensionality of the training data to fight the curse of dimension\\nality\\nOther unsupervise', 'cing the dimensionality of the training data to fight the curse of dimension\\nality\\nOther unsupervised learning techniques including clustering density estimation\\nand anomaly detection\\nPreface  xiiiThe most common learning algorithms Linear and Polynomial Regression\\nLogistic Regression kNearest Neighbors Support Vector Machines Decision\\nTrees Random Forests and Ensemble methods\\nxiv  PrefacePart II Neural Networks and Deep Learning  covers the following topics\\nWhat are neural nets What are they good for\\nBuilding and training neural nets using TensorFlow and Keras\\nThe most important neural net architectures feedforward neural nets convolu\\ntional nets recurrent nets long shortterm memory LSTM nets autoencoders\\nand generative adversarial networks GANs\\nTechniques for training deep neural nets\\nScaling neural networks for large datasets\\nLearning strategies with Reinforcement Learning\\nHandling uncertainty with Bayesian Deep Learning\\nThe first part is based mostly on ScikitLearn while the second', 'certainty with Bayesian Deep Learning\\nThe first part is based mostly on ScikitLearn while the second part uses TensorFlow\\nand Keras\\nDont jump into deep waters too hastily while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning you\\nshould master the fundamentals first Moreover most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods discussed in Part I  Deep Learn\\ning is best suited for complex problems such as image recognition\\nspeech recognition or natural language processing provided you\\nhave enough data computing power and patience\\nOther Resources\\nMany resources are available to learn about Machine Learning Andrew Ngs ML\\ncourse on Coursera  and Geoffrey Hintons course on neural networks and Deep\\nLearning  are amazing although they both require a significant time investment\\nthink months\\nThere are also many interesting websites about Machine Learning including of\\ncourse ScikitLearns exceptional User Guide  Y', 'teresting websites about Machine Learning including of\\ncourse ScikitLearns exceptional User Guide  Y ou may also enjoy Dataquest  which\\nprovides very nice interactive tutorials and ML blogs such as those listed on Quora \\nFinally the Deep Learning website  has a good list of resources to learn more\\nOf course there are also many other introductory books about Machine Learning in\\nparticular\\nJoel Grus Data Science from Scratch  OReilly This book presents the funda\\nmentals of Machine Learning and implements some of the main algorithms in\\npure Python from scratch as the name suggests\\nPreface  xvStephen Marsland Machine Learning An Algorithmic Perspective  Chapman and\\nHall This book is a great introduction to Machine Learning covering a wide\\nrange of topics in depth with code examples in Python also from scratch but\\nusing NumPy\\nSebastian Raschka Python Machine Learning  Packt Publishing Also a great\\nintroduction to Machine Learning this book leverages Python open source libra\\nries Pylearn 2 a', 'great\\nintroduction to Machine Learning this book leverages Python open source libra\\nries Pylearn 2 and Theano\\nFranois Chollet Deep Learning with Python  Manning A very practical book\\nthat covers a large range of topics in a clear and concise way as you might expect\\nfrom the author of the excellent Keras library It favors code examples over math\\nematical theory\\nY aser S AbuMostafa Malik MagdonIsmail and HsuanTien Lin Learning from\\nData  AMLBook A rather theoretical approach to ML this book provides deep\\ninsights in particular on the biasvariance tradeoff see Chapter 4 \\nStuart Russell and Peter Norvig Artificial  Intelligence A Modern Approach 3rd\\nEdition  Pearson This is a great and huge book covering an incredible amount\\nof topics including Machine Learning It helps put ML into perspective\\nFinally a great way to learn is to join ML competition websites such as Kagglecom\\nthis will allow you to practice your skills on realworld problems with help and\\ninsights from some of the best ML pro', 'ou to practice your skills on realworld problems with help and\\ninsights from some of the best ML professionals out there\\nConventions Used in This Book\\nThe following typographical conventions are used in this book\\nItalic\\nIndicates new terms URLs email addresses filenames and file extensions\\nConstant width\\nUsed for program listings as well as within paragraphs to refer to program ele\\nments such as variable or function names databases data types environment\\nvariables statements and keywords\\nConstant width bold\\nShows commands or other text that should be typed literally by the user\\nConstant width italic\\nShows text that should be replaced with usersupplied values or by values deter\\nmined by context\\nxvi  PrefaceThis element signifies a tip or suggestion\\nThis element signifies a general note\\nThis element indicates a warning or caution\\nCode Examples\\nSupplemental material code examples exercises etc is available for download at\\nhttpsgithubcomageronhandsonml2  It is mostly composed of Jupyter no', 'etc is available for download at\\nhttpsgithubcomageronhandsonml2  It is mostly composed of Jupyter notebooks\\nSome of the code examples in the book leave out some repetitive sections or details\\nthat are obvious or unrelated to Machine Learning This keeps the focus on the\\nimportant parts of the code and it saves space to cover more topics However if you\\nwant the full code examples they are all available in the Jupyter notebooks\\nNote that when the code examples display some outputs then these code examples\\nare shown with Python prompts   and  as in a Python shell to clearly distin\\nguish the code from the outputs For example this code defines the square  func\\ntion then it computes and displays the square of 3\\n def squarex\\n     return x  2\\n\\n result  square3\\n result\\n9\\nWhen code does not display anything prompts are not used However the result may\\nsometimes be shown as a comment like this\\ndef squarex\\n    return x  2\\nresult  square3   result is 9\\nPreface  xviiUsing Code Examples\\nThis book is he', 'uarex\\n    return x  2\\nresult  square3   result is 9\\nPreface  xviiUsing Code Examples\\nThis book is here to help you get your job done In general if example code is offered\\nwith this book you may use it in your programs and documentation Y ou do not\\nneed to contact us for permission unless youre reproducing a significant portion of\\nthe code For example writing a program that uses several chunks of code from this\\nbook does not require permission Selling or distributing a CDROM of examples\\nfrom OReilly books does require permission Answering a question by citing this\\nbook and quoting example code does not require permission Incorporating a signifi\\ncant amount of example code from this book into your products documentation does\\nrequire permission\\nWe appreciate but do not require attribution An attribution usually includes the\\ntitle author publisher and ISBN For example  HandsOn Machine Learning with\\nScikitLearn Keras and TensorFlow  by Aurlien Gron OReilly Copyright 2019\\nAurlien Gron 978149', 'ng with\\nScikitLearn Keras and TensorFlow  by Aurlien Gron OReilly Copyright 2019\\nAurlien Gron 9781492032649  If you feel your use of code examples falls out\\nside fair use or the permission given above feel free to contact us at permis\\nsionsoreillycom \\nOReilly Safari\\nSafari  formerly Safari Books Online is a membershipbased\\ntraining and reference platform for enterprise government\\neducators and individuals\\nMembers have access to thousands of books training videos Learning Paths interac\\ntive tutorials and curated playlists from over 250 publishers including OReilly\\nMedia Harvard Business Review Prentice Hall Professional AddisonWesley Profes\\nsional Microsoft Press Sams Que Peachpit Press Adobe Focal Press Cisco Press\\nJohn Wiley  Sons Syngress Morgan Kaufmann IBM Redbooks Packt Adobe\\nPress FT Press Apress Manning New Riders McGrawHill Jones  Bartlett and\\nCourse Technology among others\\nFor more information please visit httporeillycomsafari \\nHow to Contact Us\\nPlease address comments and que', 'ore information please visit httporeillycomsafari \\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher\\nOReilly Media Inc\\n1005 Gravenstein Highway North\\nSebastopol CA 95472\\n8009989938 in the United States or Canada\\nxviii  Preface7078290515 international or local\\n7078290104 fax\\nWe have a web page for this book where we list errata examples and any additional\\ninformation Y ou can access this page at httpbitlyhandsonmachinelearning\\nwithscikitlearnandtensorflow  or httpshomlinfooreilly \\nTo comment or ask technical questions about this book send email to bookques\\ntionsoreillycom \\nFor more information about our books courses conferences and news see our web\\nsite at httpwwworeillycom \\nFind us on Facebook httpfacebookcomoreilly\\nFollow us on Twitter httptwittercomoreillymedia\\nWatch us on Y ouTube httpwwwyoutubecomoreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives\\n1Cover additional topics additional unsupervised learni', 'This second edition has five main objectives\\n1Cover additional topics additional unsupervised learning techniques including\\nclustering anomaly detection density estimation and mixture models addi\\ntional techniques for training deep nets including selfnormalized networks\\nadditional computer vision techniques including the Xception SENet object\\ndetection with YOLO and semantic segmentation using RCNN handling\\nsequences using CNNs including WaveNet natural language processing using\\nRNNs CNNs and Transformers generative adversarial networks deploying Ten\\nsorFlow models and more\\n2Update the book to mention some of the latest results from Deep Learning\\nresearch\\n3Migrate all TensorFlow chapters to TensorFlow 2 and use TensorFlows imple\\nmentation of the Keras API called tfkeras whenever possible to simplify the\\ncode examples\\n4Update the code examples to use the latest version of ScikitLearn NumPy Pan\\ndas Matplotlib and other libraries\\n5Clarify some sections and fix some errors thanks to plenty', 'y Pan\\ndas Matplotlib and other libraries\\n5Clarify some sections and fix some errors thanks to plenty of great feedback\\nfrom readers\\nSome chapters were added others were rewritten and a few were reordered Table P1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters\\nPreface  xixTable P1 Chapter mapping between 1st and 2nd edition\\n1st Ed chapter 2nd Ed Chapter  Changes 2nd Ed Title\\n1 1 10 The Machine Learning Landscape\\n2 2 10 EndtoEnd Machine Learning Project\\n3 3 10 Classification\\n4 4 10 Training Models\\n5 5 10 Support Vector Machines\\n6 6 10 Decision Trees\\n7 7 10 Ensemble Learning and Random Forests\\n8 8 10 Dimensionality Reduction\\nNA 9 100 new Unsupervised Learning Techniques\\n10 10 75 Introduction to Artificial  Neural Networks with Keras\\n11 11 50 Training Deep Neural Networks\\n9 12 100 rewritten Custom Models and Training with TensorFlow\\nPart of 12 13 100 rewritten Loading and Preprocessing Data with TensorFlow\\n13 14 50 Deep Computer Vision Using Convolutional N', 'n Loading and Preprocessing Data with TensorFlow\\n13 14 50 Deep Computer Vision Using Convolutional Neural Networks\\nPart of 14 15 75 Processing Sequences Using RNNs and CNNs\\nPart of 14 16 90 Natural Language Processing with RNNs and Attention\\n15 17 75 Autoencoders and GANs\\n16 18 75 Reinforcement Learning\\nPart of 12 19 100 rewritten Deploying your TensorFlow Models\\nMore specifically here are the main changes for each 2nd edition chapter other than\\nclarifications corrections and code updates\\nChapter 1\\nAdded a section on handling mismatch between the training set and the vali\\ndation  test sets\\nChapter 2\\nAdded how to compute a confidence interval\\nImproved the installation instructions eg for Windows\\nIntroduced the upgraded OneHotEncoder  and the new ColumnTransformer \\nChapter 4\\nExplained the need for training instances to be Independent and Identically\\nDistributed IID\\nChapter 7\\nAdded a short section about XGBoost\\nxx  PrefaceChapter 9  new chapter including\\nClustering with KMeans how to choo', 'section about XGBoost\\nxx  PrefaceChapter 9  new chapter including\\nClustering with KMeans how to choose the number of clusters how to use it\\nfor dimensionality reduction semisupervised learning image segmentation\\nand more\\nThe DBSCAN clustering algorithm and an overview of other clustering algo\\nrithms available in ScikitLearn\\nGaussian mixture models the ExpectationMaximization EM algorithm\\nBayesian variational inference and how mixture models can be used for clus\\ntering density estimation anomaly detection and novelty detection\\nOverview of other anomaly detection and novelty detection algorithms\\nChapter 10 mostly new\\nAdded an introduction to the Keras API including all its APIs Sequential\\nFunctional and Subclassing persistence and callbacks including the Tensor\\nBoard  callback\\nChapter 11 many changes\\nIntroduced selfnormalizing nets the SELU activation function and Alpha\\nDropout\\nIntroduced selfsupervised learning\\nAdded Nadam optimization\\nAdded MonteCarlo Dropout\\nAdded a note about the ris', 'selfsupervised learning\\nAdded Nadam optimization\\nAdded MonteCarlo Dropout\\nAdded a note about the risks of adaptive optimization methods\\nUpdated the practical guidelines\\nChapter 12  completely rewritten chapter including\\nA tour of TensorFlow 2\\nTensorFlows lowerlevel Python API\\nWriting custom loss functions metrics layers models\\nUsing autodifferentiation and creating custom training algorithms\\nTensorFlow Functions and graphs including tracing and autograph\\nChapter 13  new chapter including\\nThe Data API\\nLoadingStoring data efficiently using TFRecords\\nThe Features API including an introduction to embeddings\\nAn overview of TF Transform and TF Datasets\\nMoved the lowlevel implementation of the neural network to the exercises\\nPreface  xxiRemoved details about queues and readers that are now superseded by the\\nData API\\nChapter 14\\nAdded Xception and SENet architectures\\nAdded a Keras implementation of ResNet34\\nShowed how to use pretrained models using Keras\\nAdded an endtoend transfer learning exam', 'of ResNet34\\nShowed how to use pretrained models using Keras\\nAdded an endtoend transfer learning example\\nAdded classification and localization\\nIntroduced Fully Convolutional Networks FCNs\\nIntroduced object detection using the YOLO architecture\\nIntroduced semantic segmentation using RCNN\\nChapter 15\\nAdded an introduction to Wavenet\\nMoved the EncoderDecoder architecture and Bidirectional RNNs to Chapter\\n16\\nChapter 16\\nExplained how to use the Data API to handle sequential data\\nShowed an endtoend example of text generation using a Character RNN\\nusing both a stateless and a stateful RNN\\nShowed an endtoend example of sentiment analysis using an LSTM\\nExplained masking in Keras\\nShowed how to reuse pretrained embeddings using TF Hub\\nShowed how to build an EncoderDecoder for Neural Machine Translation\\nusing TensorFlow Addonsseq2seq\\nIntroduced beam search\\nExplained attention mechanisms\\nAdded a short overview of visual attention and a note on explainability\\nIntroduced the fully attentionbased Transf', 'overview of visual attention and a note on explainability\\nIntroduced the fully attentionbased Transformer architecture including posi\\ntional embeddings and multihead attention\\nAdded an overview of recent language models 2018\\nChapters 17 18 and 19 coming soon\\nxxii  Preface3Deep Learning with Python  Franois Chollet 2017Acknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience I received so many messages from readers many asking ques\\ntions some kindly pointing out errata and most sending me encouraging words I\\ncannot express how grateful I am to all these readers for their tremendous support\\nThank you all so very much Please do not hesitate to file issues on github  if you find\\nerrors in the code examples or just to ask questions or to submit errata  if you find\\nerrors in the text Some readers also shared how this book helped them get their first\\njob or how it helped them solve a concrete problem they were working on I fi', 'ed them get their first\\njob or how it helped them solve a concrete problem they were working on I find\\nsuch feedback incredibly motivating If you find this book helpful I would love it if\\nyou could share your story with me either privately eg via LinkedIn  or publicly\\neg in an Amazon review \\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care In particular I would like to thank Fran\\nois Chollet for reviewing all the chapters based on Keras  TensorFlow and giving\\nme some great indepth feedback Since Keras is one of the main additions to this 2nd\\nedition having its author review the book was invaluable I highly recommend Fran\\noiss excellent book Deep Learning with Python3 it has the conciseness clarity and\\ndepth of the Keras library itself Big thanks as well to Ankur Patel who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback\\nThis book also benefited from plenty of help from members of the ', 'ion and gave me excellent feedback\\nThis book also benefited from plenty of help from members of the TensorFlow team\\nin particular Martin Wicke who tirelessly answered dozens of my questions and dis\\npatched the rest to the right people including Alexandre Passos Allen Lavoie Andr\\nSusano Pinto Anna Revinskaya Anthony Platanios Clemens Mewald Dan Moldo\\nvan Daniel Dobson Dustin Tran Edd WilderJames Goldie Gadde Jiri Simsa Kar\\nmel Allison Nick Felt Paige Bailey Pete Warden who also reviewed the 1st edition\\nRyan Sepassi Sandeep Gupta Sean Morgan Todd Wang Tom OMalley William\\nChargin and Yuefeng Zhou all of whom were tremendously helpful A huge thank\\nyou to all of you and to all other members of the TensorFlow team Not just for your\\nhelp but also for making such a great library\\nBig thanks to Haesun Park who gave me plenty of excellent feedback and caught sev\\neral errors while he was writing the Korean translation of the 1st edition of this book\\nHe also translated the Jupyter notebooks to Kore', ' Korean translation of the 1st edition of this book\\nHe also translated the Jupyter notebooks to Korean not to mention TensorFlows\\ndocumentation I do not speak Korean but judging by the quality of his feedback all\\nhis translations must be truly excellent Moreover he kindly contributed some of the\\nsolutions to the exercises in this book\\nPreface  xxiiiMany thanks as well to OReillys fantastic staff in particular Nicole Tache who gave\\nme insightful feedback always cheerful encouraging and helpful I could not dream\\nof a better editor Big thanks to Michele Cronin as well who was very helpful and\\npatient at the start of this 2nd edition Thanks to Marie Beaugureau Ben Lorica Mike\\nLoukides and Laurel Ruma for believing in this project and helping me define its\\nscope Thanks to Matt Hacker and all of the Atlas team for answering all my technical\\nquestions regarding formatting asciidoc and LaTeX and thanks to Rachel Mona\\nghan Nick Adams and all of the production team for their final review and the', ' thanks to Rachel Mona\\nghan Nick Adams and all of the production team for their final review and their\\nhundreds of corrections\\nI would also like to thank my former Google colleagues in particular the Y ouTube\\nvideo classification team for teaching me so much about Machine Learning I could\\nnever have started the first edition without them Special thanks to my personal ML\\ngurus Clment Courbet Julien Dubois Mathias Kende Daniel Kitachewsky James\\nPack Alexander Pak Anosh Raj Vitor Sessak Wiktor Tomczak Ingrid von Glehn\\nRich Washington and everyone I worked with at Y ouTube and in the amazing Goo\\ngle research teams in Mountain View All these people are just as nice and helpful as\\nthey are bright and thats saying a lot\\nI will never forget the kind people who reviewed the 1st edition of this book including\\nDavid Andrzejewski Eddy Hung Grgoire Mesnil Iain Smears Ingrid von Glehn\\nJustin Francis Karim Matrah Lukas Biewald Michel Tessier Salim Smaoune Vin\\ncent Guilbeau and of course my dear broth', 'arim Matrah Lukas Biewald Michel Tessier Salim Smaoune Vin\\ncent Guilbeau and of course my dear brother Sylvain\\nLast but not least I am infinitely grateful to my beloved wife Emmanuelle and to our\\nthree wonderful children Alexandre Rmi and Gabrielle for encouraging me to\\nwork hard on this book as well as for their insatiable curiosity explaining some of\\nthe most difficult concepts in this book to my wife and children helped me clarify my\\nthoughts and directly improved many parts of this book Plus they keep bringing me\\ncookies and coffee What more can one dream of\\nxxiv  PrefacePART I\\nThe Fundamentals of\\nMachine LearningCHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 1 in the final\\nrelease of the book\\nWhen most people hear Machine Learning  they picture ', ' be Chapter 1 in the final\\nrelease of the book\\nWhen most people hear Machine Learning  they picture a robot a dependable but\\nler or a deadly Terminator depending on who you ask But Machine Learning is not\\njust a futuristic fantasy its already here In fact it has been around for decades in\\nsome specialized applications such as Optical Character Recognition  OCR But the\\nfirst ML application that really became mainstream improving the lives of hundreds\\nof millions of people took over the world back in the 1990s it was the spam filter \\nNot exactly a selfaware Skynet but it does technically qualify as Machine Learning\\nit has actually learned so well that you seldom need to flag an email as spam any\\nmore It was followed by hundreds of ML applications that now quietly power hun\\ndreds of products and features that you use regularly from better recommendations\\nto voice search\\nWhere does Machine Learning start and where does it end What exactly does it\\nmean for a machine to learn  something If I', 'earning start and where does it end What exactly does it\\nmean for a machine to learn  something If I download a copy of Wikipedia has my\\ncomputer really learned something Is it suddenly smarter In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it\\nThen before we set out to explore the Machine Learning continent we will take a\\nlook at the map and learn about the main regions and the most notable landmarks\\nsupervised versus unsupervised learning online versus batch learning instance\\nbased versus modelbased learning Then we will look at the workflow of a typical ML\\nproject discuss the main challenges you may face and cover how to evaluate and\\nfinetune a Machine Learning system\\n3This chapter introduces a lot of fundamental concepts and jargon that every data\\nscientist should know by heart It will be a highlevel overview the only chapter\\nwithout much code all rather simple but you should make sure everything is\\ncrystalclear to you before continu', 'uch code all rather simple but you should make sure everything is\\ncrystalclear to you before continuing to the rest of the book So grab a coffee and lets\\nget started\\nIf you already know all the Machine Learning basics you may want\\nto skip directly to Chapter 2  If you are not sure try to answer all\\nthe questions listed at the end of the chapter before moving on\\nWhat Is Machine Learning\\nMachine Learning is the science and art of programming computers so they can\\nlearn from data \\nHere is a slightly more general definition\\nMachine Learning is the field of study that gives computers the ability to learn\\nwithout being explicitly programmed\\nArthur Samuel 1959\\nAnd a more engineeringoriented one\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P  if its performance on T as measured by P  improves\\nwith experience E\\nTom Mitchell 1997\\nFor example your spam filter is a Machine Learning program that can learn to flag\\nspam given examples ', 'r example your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails eg flagged by users and examples of regular\\nnonspam also called ham emails The examples that the system uses to learn are\\ncalled the training set  Each training example is called a training instance  or sample \\nIn this case the task T is to flag spam for new emails the experience E is the training\\ndata  and the performance measure P needs to be defined for example you can use\\nthe ratio of correctly classified emails This particular performance measure is called\\naccuracy  and it is often used in classification tasks\\nIf you just download a copy of Wikipedia your computer has a lot more data but it is\\nnot suddenly better at any task Thus it is not Machine Learning\\nWhy Use Machine Learning\\nConsider how you would write a spam filter using traditional programming techni\\nques  Figure 11 \\n4  Chapter 1 The Machine Learning Landscape1First you would look at what spam typically looks', 're 11 \\n4  Chapter 1 The Machine Learning Landscape1First you would look at what spam typically looks like Y ou might notice that\\nsome words or phrases such as 4U  credit card  free  and amazing tend to\\ncome up a lot in the subject Perhaps you would also notice a few other patterns\\nin the senders name the emails body and so on\\n2Y ou would write a detection algorithm for each of the patterns that you noticed\\nand your program would flag emails as spam if a number of these patterns are\\ndetected\\n3Y ou would test your program and repeat steps 1 and 2 until it is good enough\\nFigure 11 The traditional approach\\nSince the problem is not trivial your program will likely become a long list of com\\nplex rulespretty hard to maintain\\nIn contrast a spam filter based on Machine Learning techniques automatically learns\\nwhich words and phrases are good predictors of spam by detecting unusually fre\\nquent patterns of words in the spam examples compared to the ham examples\\nFigure 12  The program is much shor', 'terns of words in the spam examples compared to the ham examples\\nFigure 12  The program is much shorter easier to maintain and most likely more\\naccurate\\nWhy Use Machine Learning  5Figure 12 Machine Learning approach\\nMoreover if spammers notice that all their emails containing 4U are blocked they\\nmight start writing For U instead A spam filter using traditional programming\\ntechniques would need to be updated to flag For U emails If spammers keep work\\ning around your spam filter you will need to keep writing new rules forever\\nIn contrast a spam filter based on Machine Learning techniques automatically noti\\nces that For U has become unusually frequent in spam flagged by users and it starts\\nflagging them without your intervention  Figure 13 \\nFigure 13 Automatically adapting to change\\nAnother area where Machine Learning shines is for problems that either are too com\\nplex for traditional approaches or have no known algorithm For example consider \\nspeech recognition say you want to start simp', 'aches or have no known algorithm For example consider \\nspeech recognition say you want to start simple and write a program capable of dis\\ntinguishing the words one and two  Y ou might notice that the word two starts\\nwith a highpitch sound T so you could hardcode an algorithm that measures\\nhighpitch sound intensity and use that to distinguish ones and twos Obviously this\\ntechnique will not scale to thousands of words spoken by millions of very different\\n6  Chapter 1 The Machine Learning Landscapepeople in noisy environments and in dozens of languages The best solution at least\\ntoday is to write an algorithm that learns by itself given many example recordings\\nfor each word\\nFinally Machine Learning can help humans learn  Figure 14  ML algorithms can be\\ninspected to see what they have learned although for some algorithms this can be\\ntricky For instance once the spam filter has been trained on enough spam it can\\neasily be inspected to reveal the list of words and combinations of words that ', 'n enough spam it can\\neasily be inspected to reveal the list of words and combinations of words that it\\nbelieves are the best predictors of spam Sometimes this will reveal unsuspected cor\\nrelations or new trends and thereby lead to a better understanding of the problem\\nApplying ML techniques to dig into large amounts of data can help discover patterns\\nthat were not immediately apparent This is called data mining \\nFigure 14 Machine Learning can help humans learn\\nTo summarize Machine Learning is great for\\nProblems for which existing solutions require a lot of handtuning or long lists of\\nrules one Machine Learning algorithm can often simplify code and perform bet\\nter\\nComplex problems for which there is no good solution at all using a traditional\\napproach the best Machine Learning techniques can find a solution\\nFluctuating environments a Machine Learning system can adapt to new data\\nGetting insights about complex problems and large amounts of data\\nWhy Use Machine Learning  7Types of Machine', 'nsights about complex problems and large amounts of data\\nWhy Use Machine Learning  7Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on\\nWhether or not they are trained with human supervision supervised unsuper\\nvised semisupervised and Reinforcement Learning\\nWhether or not they can learn incrementally on the fly online versus batch\\nlearning\\nWhether they work by simply comparing new data points to known data points\\nor instead detect patterns in the training data and build a predictive model much\\nlike scientists do instancebased versus modelbased learning\\nThese criteria are not exclusive you can combine them in any way you like For\\nexample a stateoftheart spam filter may learn on the fly using a deep neural net\\nwork model trained using examples of spam and ham this makes it an online model\\nbased supervised learning system\\nLets look at each of these criteria a bit more closely\\nSuperv', 'model\\nbased supervised learning system\\nLets look at each of these criteria a bit more closely\\nSupervisedUnsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training There are four major categories supervised\\nlearning unsupervised learning semisupervised learning and Reinforcement Learn\\ning\\nSupervised learning\\nIn supervised learning  the training data you feed to the algorithm includes the desired\\nsolutions called labels  Figure 15 \\nFigure 15 A labeled training set for supervised learning eg spam classification\\n8  Chapter 1 The Machine Learning Landscape1Fun fact this oddsounding name is a statistics term introduced by Francis Galton while he was studying the\\nfact that the children of tall people tend to be shorter than their parents Since children were shorter he called\\nthis regression to the mean  This name was then applied to the methods he used to analyze correlations\\nbetween variables\\nA typical supervised ', ' then applied to the methods he used to analyze correlations\\nbetween variables\\nA typical supervised learning task is classification  The spam filter is a good example\\nof this it is trained with many example emails along with their class  spam or ham\\nand it must learn how to classify new emails\\nAnother typical task is to predict a target  numeric value such as the price of a car\\ngiven a set of features  mileage age brand etc called predictors  This sort of task is \\ncalled regression  Figure 16 1 To train the system you need to give it many examples\\nof cars including both their predictors and their labels ie their prices\\nIn Machine Learning an attribute  is a data type eg Mileage\\nwhile a feature  has several meanings depending on the context but\\ngenerally means an attribute plus its value eg Mileage \\n15000 Many people use the words attribute  and feature  inter\\nchangeably though\\nFigure 16 Regression\\nNote that some regression algorithms can be used for classification as well and vice\\nvers', 'Regression\\nNote that some regression algorithms can be used for classification as well and vice\\nversa For example Logistic Regression  is commonly used for classification as it can\\noutput a value that corresponds to the probability of belonging to a given class eg\\n20 chance of being spam\\nTypes of Machine Learning Systems  92Some neural network architectures can be unsupervised such as autoencoders and restricted Boltzmann\\nmachines They can also be semisupervised such as in deep belief networks and unsupervised pretrainingHere are some of the most important supervised learning algorithms covered in this\\nbook\\nkNearest Neighbors\\nLinear Regression\\nLogistic Regression\\nSupport Vector Machines SVMs\\nDecision Trees and Random Forests\\nNeural networks2\\nUnsupervised learning\\nIn unsupervised learning  as you might guess the training data is unlabeled\\nFigure 17  The system tries to learn without a teacher\\nFigure 17 An unlabeled training set for unsupervised learning\\nHere are some of the most importa', 'cher\\nFigure 17 An unlabeled training set for unsupervised learning\\nHere are some of the most important unsupervised learning algorithms most of\\nthese are covered in Chapter 8  and Chapter 9 \\nClustering\\nKMeans\\nDBSCAN\\nHierarchical Cluster Analysis HCA\\nAnomaly detection and novelty detection\\nOneclass SVM\\nIsolation Forest\\n10  Chapter 1 The Machine Learning LandscapeVisualization and dimensionality reduction\\nPrincipal Component Analysis PCA\\nKernel PCA\\nLocallyLinear Embedding LLE\\ntdistributed Stochastic Neighbor Embedding tSNE\\nAssociation rule learning\\nApriori\\nEclat\\nFor example say you have a lot of data about your blogs visitors Y ou may want to\\nrun a clustering  algorithm to try to detect groups of similar visitors  Figure 18  At\\nno point do you tell the algorithm which group a visitor belongs to it finds those\\nconnections without your help For example it might notice that 40 of your visitors\\nare males who love comic books and generally read your blog in the evening while\\n20 are young scif', 's\\nare males who love comic books and generally read your blog in the evening while\\n20 are young scifi lovers who visit during the weekends and so on If you use a\\nhierarchical clustering  algorithm it may also subdivide each group into smaller\\ngroups This may help you target your posts for each group\\nFigure 18 Clustering\\nVisualization  algorithms are also good examples of unsupervised learning algorithms\\nyou feed them a lot of complex and unlabeled data and they output a 2D or 3D rep\\nresentation of your data that can easily be plotted  Figure 19  These algorithms try\\nto preserve as much structure as they can eg trying to keep separate clusters in the\\ninput space from overlapping in the visualization so you can understand how the\\ndata is organized and perhaps identify unsuspected patterns\\nTypes of Machine Learning Systems  113Notice how animals are rather well separated from vehicles how horses are close to deer but far from birds\\nand so on Figure reproduced with permission from Socher G', 'orses are close to deer but far from birds\\nand so on Figure reproduced with permission from Socher Ganjoo Manning and Ng 2013 TSNE visual\\nization of the semantic word space \\nFigure 19 Example of a tSNE visualization highlighting semantic clusters3\\nA related task is dimensionality reduction  in which the goal is to simplify the data\\nwithout losing too much information One way to do this is to merge several correla\\nted features into one For example a cars mileage may be very correlated with its age\\nso the dimensionality reduction algorithm will merge them into one feature that rep\\nresents the cars wear and tear This is called feature extraction \\nIt is often a good idea to try to reduce the dimension of your train\\ning data using a dimensionality reduction algorithm before you\\nfeed it to another Machine Learning algorithm such as a super\\nvised learning algorithm It will run much faster the data will take\\nup less disk and memory space and in some cases it may also per\\nform better\\nY et anoth', 'ata will take\\nup less disk and memory space and in some cases it may also per\\nform better\\nY et another important unsupervised task is anomaly detection for example detect\\ning unusual credit card transactions to prevent fraud catching manufacturing defects\\nor automatically removing outliers from a dataset before feeding it to another learn\\ning algorithm The system is shown mostly normal instances during training so it\\nlearns to recognize them and when it sees a new instance it can tell whether it looks\\n12  Chapter 1 The Machine Learning Landscape4Thats when the system works perfectly In practice it often creates a few clusters per person and sometimes\\nmixes up two people who look alike so you need to provide a few labels per person and manually clean up\\nsome clusterslike a normal one or whether it is likely an anomaly see Figure 110  A very similar\\ntask is novelty detection  the difference is that novelty detection algorithms expect to\\nsee only normal data during training while anomaly ', 'e is that novelty detection algorithms expect to\\nsee only normal data during training while anomaly detection algorithms are usually\\nmore tolerant they can often perform well even with a small percentage of outliers in\\nthe training set\\nFigure 110 Anomaly detection\\nFinally another common unsupervised task is association rule learning  in which the\\ngoal is to dig into large amounts of data and discover interesting relations between\\nattributes For example suppose you own a supermarket Running an association rule\\non your sales logs may reveal that people who purchase barbecue sauce and potato\\nchips also tend to buy steak Thus you may want to place these items close to each \\nother\\nSemisupervised learning\\nSome algorithms can deal with partially labeled training data usually a lot of unla\\nbeled data and a little bit of labeled data This is called semisupervised learning\\nFigure 111 \\nSome photohosting services such as Google Photos are good examples of this Once\\nyou upload all your family photo', 'sting services such as Google Photos are good examples of this Once\\nyou upload all your family photos to the service it automatically recognizes that the\\nsame person A shows up in photos 1 5 and 11 while another person B shows up in\\nphotos 2 5 and 7 This is the unsupervised part of the algorithm clustering Now all\\nthe system needs is for you to tell it who these people are Just one label per person4\\nand it is able to name everyone in every photo which is useful for searching photos\\nTypes of Machine Learning Systems  13Figure 111 Semisupervised learning\\nMost semisupervised learning algorithms are combinations of unsupervised and\\nsupervised algorithms For example deep belief networks  DBNs are based on unsu\\npervised components called restricted Boltzmann machines  RBMs stacked on top of\\none another RBMs are trained sequentially in an unsupervised manner and then the\\nwhole system is finetuned using supervised learning techniques\\nReinforcement Learning\\nReinforcement Learning  is a very dif', 'ed using supervised learning techniques\\nReinforcement Learning\\nReinforcement Learning  is a very different beast The learning system called an agent\\nin this context can observe the environment select and perform actions and get\\nrewards  in return or penalties  in the form of negative rewards as in Figure 112  It\\nmust then learn by itself what is the best strategy called a policy  to get the most\\nreward over time A policy defines what action the agent should choose when it is in a\\ngiven situation\\n14  Chapter 1 The Machine Learning LandscapeFigure 112 Reinforcement Learning\\nFor example many robots implement Reinforcement Learning algorithms to learn\\nhow to walk DeepMinds AlphaGo program is also a good example of Reinforcement\\nLearning it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go It learned its winning policy by analyzing millions of games and\\nthen playing many games against itself Note that learning was turned off during the\\ngames against the', 'hen playing many games against itself Note that learning was turned off during the\\ngames against the champion AlphaGo was just applying the policy it had learned\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data\\nBatch learning\\nIn batch learning  the system is incapable of learning incrementally it must be trained\\nusing all the available data This will generally take a lot of time and computing\\nresources so it is typically done offline First the system is trained and then it is\\nlaunched into production and runs without learning anymore it just applies what it\\nhas learned This is called offline  learning \\nIf you want a batch learning system to know about new data such as a new type of\\nspam you need to train a new version of the system from scratch on the full dataset\\nnot just the new data but also the old data then stop the old system and replace it\\nwith the new one\\nFor', 'just the new data but also the old data then stop the old system and replace it\\nwith the new one\\nFortunately the whole process of training evaluating and launching a Machine\\nLearning system can be automated fairly easily as shown in Figure 13  so even a\\nTypes of Machine Learning Systems  15batch learning system can adapt to change Simply update the data and train a new\\nversion of the system from scratch as often as needed\\nThis solution is simple and often works fine but training using the full set of data can\\ntake many hours so you would typically train a new system only every 24 hours or\\neven just weekly If your system needs to adapt to rapidly changing data eg to pre\\ndict stock prices then you need a more reactive solution\\nAlso training on the full set of data requires a lot of computing resources CPU\\nmemory space disk space disk IO network IO etc If you have a lot of data and\\nyou automate your system to train from scratch every day it will end up costing you a\\nlot of money If the am', 'mate your system to train from scratch every day it will end up costing you a\\nlot of money If the amount of data is huge it may even be impossible to use a batch\\nlearning algorithm\\nFinally if your system needs to be able to learn autonomously and it has limited\\nresources eg a smartphone application or a rover on Mars then carrying around\\nlarge amounts of training data and taking up a lot of resources to train for hours\\nevery day is a showstopper\\nFortunately a better option in all these cases is to use algorithms that are capable of\\nlearning incrementally\\nOnline learning\\nIn online learning  you train the system incrementally by feeding it data instances\\nsequentially either individually or by small groups called minibatches  Each learning\\nstep is fast and cheap so the system can learn about new data on the fly as it arrives\\nsee Figure 113 \\nFigure 113 Online learning\\nOnline learning is great for systems that receive data as a continuous flow eg stock\\nprices and need to adapt to change rap', 't for systems that receive data as a continuous flow eg stock\\nprices and need to adapt to change rapidly or autonomously It is also a good option\\n16  Chapter 1 The Machine Learning Landscapeif you have limited computing resources once an online learning system has learned\\nabout new data instances it does not need them anymore so you can discard them\\nunless you want to be able to roll back to a previous state and replay the data This\\ncan save a huge amount of space\\nOnline learning algorithms can also be used to train systems on huge datasets that\\ncannot fit in one machines main memory this is called outofcore  learning The\\nalgorithm loads part of the data runs a training step on that data and repeats the\\nprocess until it has run on all of the data see Figure 114 \\nOutofcore learning is usually done offline ie not on the live\\nsystem so online learning  can be a confusing name Think of it as\\nincremental learning \\nFigure 114 Using online learning to handle huge datasets\\nOne important parame', '\\nincremental learning \\nFigure 114 Using online learning to handle huge datasets\\nOne important parameter of online learning systems is how fast they should adapt to\\nchanging data this is called the learning rate  If you set a high learning rate then your\\nsystem will rapidly adapt to new data but it will also tend to quickly forget the old\\ndata you dont want a spam filter to flag only the latest kinds of spam it was shown\\nConversely if you set a low learning rate the system will have more inertia that is it\\nwill learn more slowly but it will also be less sensitive to noise in the new data or to\\nsequences of nonrepresentative data points outliers\\nA big challenge with online learning is that if bad data is fed to the system the sys\\ntems performance will gradually decline If we are talking about a live system your\\nclients will notice For example bad data could come from a malfunctioning sensor\\non a robot or from someone spamming a search engine to try to rank high in search\\nTypes of Machine', 'r\\non a robot or from someone spamming a search engine to try to rank high in search\\nTypes of Machine Learning Systems  17results To reduce this risk you need to monitor your system closely and promptly\\nswitch learning off and possibly revert to a previously working state if you detect a\\ndrop in performance Y ou may also want to monitor the input data and react to\\nabnormal data eg using an anomaly detection algorithm\\nInstanceBased Versus ModelBased Learning\\nOne more way to categorize Machine Learning systems is by how they generalize \\nMost Machine Learning tasks are about making predictions This means that given a\\nnumber of training examples the system needs to be able to generalize to examples it\\nhas never seen before Having a good performance measure on the training data is\\ngood but insufficient the true goal is to perform well on new instances\\nThere are two main approaches to generalization instancebased learning and\\nmodelbased learning\\nInstancebased learning\\nPossibly the most trivia', 'ation instancebased learning and\\nmodelbased learning\\nInstancebased learning\\nPossibly the most trivial form of learning is simply to learn by heart If you were to\\ncreate a spam filter this way it would just flag all emails that are identical to emails\\nthat have already been flagged by usersnot the worst solution but certainly not the\\nbest\\nInstead of just flagging emails that are identical to known spam emails your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails This requires a measure of similarity  between two emails A very basic simi\\nlarity measure between two emails could be to count the number of words they have\\nin common The system would flag an email as spam if it has many words in com\\nmon with a known spam email\\nThis is called instancebased learning  the system learns the examples by heart then\\ngeneralizes to new cases by comparing them to the learned examples or a subset of\\nthem using a similarity measure For example in Figure 115  ', 'm to the learned examples or a subset of\\nthem using a similarity measure For example in Figure 115  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class\\n18  Chapter 1 The Machine Learning LandscapeFigure 115 Instancebased learning\\nModelbased learning\\nAnother way to generalize from a set of examples is to build a model of these exam\\nples then use that model to make predictions  This is called modelbased learning\\nFigure 116 \\nFigure 116 Modelbased learning\\nFor example suppose you want to know if money makes people happy so you down\\nload the Better Life Index  data from the OECDs website  as well as stats about GDP\\nper capita from the IMFs website  Then you join the tables and sort by GDP per cap\\nita Table 11  shows an excerpt of what you get\\nTypes of Machine Learning Systems  195By convention the Greek letter  theta is frequently used to represent model parametersTable 11 Does money make people happier\\nCountry GDP per c', 'equently used to represent model parametersTable 11 Does money make people happier\\nCountry GDP per capita USD Life satisfaction\\nHungary 12240 49\\nKorea 27195 58\\nFrance 37675 65\\nAustralia 50962 73\\nUnited States 55805 72\\nLets plot the data for a few random countries  Figure 117 \\nFigure 117 Do you see a trend here\\nThere does seem to be a trend here Although the data is noisy  ie partly random it\\nlooks like life satisfaction goes up more or less linearly as the countrys GDP per cap\\nita increases So you decide to model life satisfaction as a linear function of GDP per\\ncapita This step is called model selection  you selected a linear model  of life satisfac\\ntion with just one attribute GDP per capita  Equation 11 \\nEquation 11 A simple linear model\\nlifesatisfaction  01 GDPpercapita\\nThis model has two model parameters  0 and 15 By tweaking these parameters you\\ncan make your model represent any linear function as shown in Figure 118 \\n20  Chapter 1 The Machine Learning LandscapeFigure 118 A few p', 'near function as shown in Figure 118 \\n20  Chapter 1 The Machine Learning LandscapeFigure 118 A few possible linear models\\nBefore you can use your model you need to define the parameter values 0 and 1\\nHow can you know which values will make your model perform best To answer this\\nquestion you need to specify a performance measure Y ou can either define a utility\\nfunction  or fitness  function  that measures how good  your model is or you can define\\na cost function  that measures how bad it is For linear regression problems people\\ntypically use a cost function that measures the distance between the linear models\\npredictions and the training examples the objective is to minimize this distance\\nThis is where the Linear Regression algorithm comes in you feed it your training\\nexamples and it finds the parameters that make the linear model fit best to your data\\nThis is called training  the model In our case the algorithm finds that the optimal\\nparameter values are 0  485 and 1  491  105\\nNow the', 'In our case the algorithm finds that the optimal\\nparameter values are 0  485 and 1  491  105\\nNow the model fits the training data as closely as possible for a linear model as you\\ncan see in Figure 119 \\nFigure 119 The linear model that fits the training data best\\nTypes of Machine Learning Systems  216The preparecountrystats  functions definition is not shown here see this chapters Jupyter notebook if\\nyou want all the gory details Its just boring Pandas code that joins the life satisfaction data from the OECD\\nwith the GDP per capita data from the IMF\\n7Its okay if you dont understand all the code yet we will present ScikitLearn in the following chaptersY ou are finally ready to run the model to make predictions For example say you\\nwant to know how happy Cypriots are and the OECD data does not have the answer\\nFortunately you can use your model to make a good prediction you look up Cypruss\\nGDP per capita find 22587 and then apply your model and find that life satisfac\\ntion is likely to be s', 'P per capita find 22587 and then apply your model and find that life satisfac\\ntion is likely to be somewhere around 485  22587  491  105  596\\nTo whet your appetite Example 11  shows the Python code that loads the data pre\\npares it6 creates a scatterplot for visualization and then trains a linear model and\\nmakes a prediction7\\nExample 11 Training and running a linear model using ScikitLearn\\nimport matplotlibpyplot  as plt\\nimport numpy as np\\nimport pandas as pd\\nimport sklearnlinearmodel\\n Load the data\\noecdbli   pdreadcsv oecdbli2015csv  thousands \\ngdppercapita   pdreadcsv gdppercapitacsv thousands delimiter t\\n                             encoding latin1  navalues na\\n Prepare the data\\ncountrystats   preparecountrystats oecdbli  gdppercapita \\nX  npccountrystats GDP per capita \\ny  npccountrystats Life satisfaction \\n Visualize the data\\ncountrystats plotkindscatter  xGDP per capita  yLife satisfaction \\npltshow\\n Select a linear model\\nmodel  sklearnlinearmodel LinearRegression \\n Train the model\\n', 'action \\npltshow\\n Select a linear model\\nmodel  sklearnlinearmodel LinearRegression \\n Train the model\\nmodelfitX y\\n Make a prediction for Cyprus\\nXnew  22587   Cyprus GDP per capita\\nprintmodelpredictXnew  outputs  596242338\\n22  Chapter 1 The Machine Learning LandscapeIf you had used an instancebased learning algorithm instead you\\nwould have found that Slovenia has the closest GDP per capita to\\nthat of Cyprus 20732 and since the OECD data tells us that\\nSlovenians life satisfaction is 57 you would have predicted a life\\nsatisfaction of 57 for Cyprus If you zoom out a bit and look at the\\ntwo next closest countries you will find Portugal and Spain with\\nlife satisfactions of 51 and 65 respectively Averaging these three\\nvalues you get 577 which is pretty close to your modelbased pre\\ndiction This simple algorithm is called kNearest Neighbors  regres\\nsion in this example k  3\\nReplacing the Linear Regression model with kNearest Neighbors\\nregression in the previous code is as simple as replacing thes', 'ression model with kNearest Neighbors\\nregression in the previous code is as simple as replacing these two\\nlines\\nimport sklearnlinearmodel\\nmodel  sklearnlinearmodel LinearRegression \\nwith these two\\nimport sklearnneighbors\\nmodel  sklearnneighbors KNeighborsRegressor nneighbors 3\\nIf all went well your model will make good predictions If not you may need to use\\nmore attributes employment rate health air pollution etc get more or better qual\\nity training data or perhaps select a more powerful model eg a Polynomial Regres\\nsion model\\nIn summary\\nY ou studied the data\\nY ou selected a model\\nY ou trained it on the training data ie the learning algorithm searched for the\\nmodel parameter values that minimize a cost function\\nFinally you applied the model to make predictions on new cases this is called\\ninference  hoping that this model will generalize well\\nThis is what a typical Machine Learning project looks like In Chapter 2  you will\\nexperience this firsthand by going through an endtoend project\\nW', 't looks like In Chapter 2  you will\\nexperience this firsthand by going through an endtoend project\\nWe have covered a lot of ground so far you now know what Machine Learning is\\nreally about why it is useful what some of the most common categories of ML sys\\ntems are and what a typical project workflow looks like Now lets look at what can go\\nwrong in learning and prevent you from making accurate predictions\\nTypes of Machine Learning Systems  23Main Challenges of Machine Learning\\nIn short since your main task is to select a learning algorithm and train it on some\\ndata the two things that can go wrong are bad algorithm and bad data  Lets start\\nwith examples of bad data\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is all it takes is for you to point to an apple and\\nsay apple possibly repeating this procedure a few times Now the child is able to\\nrecognize apples in all sorts of colors and shapes Genius\\nMachine Learning is not quite there yet it takes a lot of d', 'in all sorts of colors and shapes Genius\\nMachine Learning is not quite there yet it takes a lot of data for most Machine Learn\\ning algorithms to work properly Even for very simple problems you typically need\\nthousands of examples and for complex problems such as image or speech recogni\\ntion you may need millions of examples unless you can reuse parts of an existing\\nmodel\\n24  Chapter 1 The Machine Learning Landscape8For example knowing whether to write to  two  or too depending on the context\\n9Figure reproduced with permission from Banko and Brill 2001 Learning Curves for Confusion Set Disam\\nbiguation \\n10The Unreasonable Effectiveness of Data  Peter Norvig et al 2009The Unreasonable Effectiveness  of Data\\nIn a famous paper  published in 2001 Microsoft researchers Michele Banko and Eric\\nBrill showed that very different Machine Learning algorithms including fairly simple\\nones performed almost identically well on a complex problem of natural language\\ndisambiguation8 once they were given en', 'st identically well on a complex problem of natural language\\ndisambiguation8 once they were given enough data as you can see in Figure 120 \\nFigure 120 The importance of data versus algorithms9\\nAs the authors put it these results suggest that we may want to reconsider the trade\\noff between spending time and money on algorithm development versus spending it\\non corpus development \\nThe idea that data matters more than algorithms for complex problems was further\\npopularized by Peter Norvig et al in a paper titled The Unreasonable Effectiveness\\nof Data  published in 200910 It should be noted however that small and medium\\nsized datasets are still very common and it is not always easy or cheap to get extra\\ntraining data so dont abandon algorithms just yet\\nMain Challenges of Machine Learning  25Nonrepresentative Training Data\\nIn order to generalize well it is crucial that your training data be representative of the\\nnew cases you want to generalize to This is true whether you use instancebased\\nl', 'presentative of the\\nnew cases you want to generalize to This is true whether you use instancebased\\nlearning or modelbased learning\\nFor example the set of countries we used earlier for training the linear model was not\\nperfectly representative a few countries were missing Figure 121  shows what the\\ndata looks like when you add the missing countries\\nFigure 121 A more representative training sample\\nIf you train a linear model on this data you get the solid line while the old model is\\nrepresented by the dotted line As you can see not only does adding a few missing\\ncountries significantly alter the model but it makes it clear that such a simple linear\\nmodel is probably never going to work well It seems that very rich countries are not\\nhappier than moderately rich countries in fact they seem unhappier and conversely\\nsome poor countries seem happier than many rich countries\\nBy using a nonrepresentative training set we trained a model that is unlikely to make\\naccurate predictions especially fo', 'entative training set we trained a model that is unlikely to make\\naccurate predictions especially for very poor and very rich countries\\nIt is crucial to use a training set that is representative of the cases you want to general\\nize to This is often harder than it sounds if the sample is too small you will have\\nsampling noise  ie nonrepresentative data as a result of chance but even very large\\nsamples can be nonrepresentative if the sampling method is flawed This is called\\nsampling bias \\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi\\ndential election in 1936 which pitted Landon against Roosevelt the Literary Digest\\nconducted a very large poll sending mail to about 10 million people It got 24 million\\nanswers and predicted with high confidence that Landon would get 57 of the votes\\n26  Chapter 1 The Machine Learning LandscapeInstead Roosevelt won with 62 of the votes The flaw was in the Literary Digest s\\nsampling method\\nFirst', 'stead Roosevelt won with 62 of the votes The flaw was in the Literary Digest s\\nsampling method\\nFirst to obtain the addresses to send the polls to the Literary Digest  used tele\\nphone directories lists of magazine subscribers club membership lists and the\\nlike All of these lists tend to favor wealthier people who are more likely to vote\\nRepublican hence Landon\\nSecond less than 25 of the people who received the poll answered Again this\\nintroduces a sampling bias by ruling out people who dont care much about poli\\ntics people who dont like the Literary Digest  and other key groups This is a spe\\ncial type of sampling bias called nonresponse bias \\nHere is another example say you want to build a system to recognize funk music vid\\neos One way to build your training set is to search funk music on Y ouTube and use\\nthe resulting videos But this assumes that Y ouTubes search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube In reality the\\nsearch result', 'videos that are representative of all the funk music videos on Y ouTube In reality the\\nsearch results are likely to be biased toward popular artists and if you live in Brazil\\nyou will get a lot of funk carioca videos which sound nothing like James Brown\\nOn the other hand how else can you get a large training set\\nPoorQuality Data\\nObviously if your training data is full of errors outliers and noise eg due to poor\\nquality measurements it will make it harder for the system to detect the underlying\\npatterns so your system is less likely to perform well It is often well worth the effort\\nto spend time cleaning up your training data The truth is most data scientists spend\\na significant part of their time doing just that For example\\nIf some instances are clearly outliers it may help to simply discard them or try to\\nfix the errors manually\\nIf some instances are missing a few features eg 5 of your customers did not\\nspecify their age you must decide whether you want to ignore this attribute alto\\ng', 'customers did not\\nspecify their age you must decide whether you want to ignore this attribute alto\\ngether ignore these instances fill in the missing values eg with the median\\nage or train one model with the feature and one model without it and so on\\nIrrelevant Features\\nAs the saying goes garbage in garbage out Y our system will only be capable of learn\\ning if the training data contains enough relevant features and not too many irrelevant\\nones A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on This process called feature engineering  involves\\nMain Challenges of Machine Learning  27Feature selection  selecting the most useful features to train on among existing\\nfeatures\\nFeature extraction  combining existing features to produce a more useful one as\\nwe saw earlier dimensionality reduction algorithms can help\\nCreating new features by gathering new data\\nNow that we have looked at many examples of bad data lets look at a couple o', 's by gathering new data\\nNow that we have looked at many examples of bad data lets look at a couple of exam\\nples of bad algorithms\\nOverfitting  the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off Y ou might be\\ntempted to say that all taxi drivers in that country are thieves Overgeneralizing is\\nsomething that we humans do all too often and unfortunately machines can fall into\\nthe same trap if we are not careful In Machine Learning this is called overfitting  it\\nmeans that the model performs well on the training data but it does not generalize\\nwell\\nFigure 122  shows an example of a highdegree polynomial life satisfaction model\\nthat strongly overfits the training data Even though it performs much better on the\\ntraining data than the simple linear model would you really trust its predictions\\nFigure 122 Overfitting  the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data\\nbut if the training set is noisy or if ', ' deep neural networks can detect subtle patterns in the data\\nbut if the training set is noisy or if it is too small which introduces sampling noise\\nthen the model is likely to detect patterns in the noise itself Obviously these patterns\\nwill not generalize to new instances For example say you feed your life satisfaction\\nmodel many more attributes including uninformative ones such as the countrys\\nname In that case a complex model may detect patterns like the fact that all coun\\ntries in the training data with a w in their name have a life satisfaction greater than 7\\nNew Zealand 73 Norway 74 Sweden 72 and Switzerland 75 How confident\\n28  Chapter 1 The Machine Learning Landscapeare you that the Wsatisfaction rule generalizes to Rwanda or Zimbabwe Obviously\\nthis pattern occurred in the training data by pure chance but the model has no way\\nto tell whether a pattern is real or simply the result of noise in the data\\nOverfitting happens when the model is too complex relative to the\\namount and n', ' of noise in the data\\nOverfitting happens when the model is too complex relative to the\\namount and noisiness of the training data The possible solutions\\nare\\nTo simplify the model by selecting one with fewer parameters\\neg a linear model rather than a highdegree polynomial\\nmodel by reducing the number of attributes in the training\\ndata or by constraining the model\\nTo gather more training data\\nTo reduce the noise in the training data eg fix data errors\\nand remove outliers\\nConstraining a model to make it simpler and reduce the risk of overfitting is called\\nregularization  For example the linear model we defined earlier has two parameters\\n0 and 1 This gives the learning algorithm two degrees of freedom  to adapt the model\\nto the training data it can tweak both the height  0 and the slope  1 of the line If\\nwe forced 1  0 the algorithm would have only one degree of freedom and would\\nhave a much harder time fitting the data properly all it could do is move the line up\\nor down to get as close a', ' harder time fitting the data properly all it could do is move the line up\\nor down to get as close as possible to the training instances so it would end up\\naround the mean A very simple model indeed If we allow the algorithm to modify 1\\nbut we force it to keep it small then the learning algorithm will effectively have some\\nwhere in between one and two degrees of freedom It will produce a simpler model\\nthan with two degrees of freedom but more complex than with just one Y ou want to\\nfind the right balance between fitting the training data perfectly and keeping the\\nmodel simple enough to ensure that it will generalize well\\nFigure 123  shows three models the dotted line represents the original model that\\nwas trained with a few countries missing the dashed line is our second model trained\\nwith all countries and the solid line is a linear model trained with the same data as\\nthe first model but with a regularization constraint Y ou can see that regularization\\nforced the model to have a small', 't with a regularization constraint Y ou can see that regularization\\nforced the model to have a smaller slope which fits a bit less the training data that the\\nmodel was trained on but actually allows it to generalize better to new examples\\nMain Challenges of Machine Learning  29Figure 123 Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper\\nparameter  A hyperparameter is a parameter of a learning algorithm not of the\\nmodel As such it is not affected by the learning algorithm itself it must be set prior\\nto training and remains constant during training If you set the regularization hyper\\nparameter to a very large value you will get an almost flat model a slope close to\\nzero the learning algorithm will almost certainly not overfit the training data but it\\nwill be less likely to find a good solution Tuning hyperparameters is an important\\npart of building a Machine Learning system you will see a detailed example in ', 'eters is an important\\npart of building a Machine Learning system you will see a detailed example in the\\nnext chapter\\nUnderfitting  the Training Data\\nAs you might guess underfitting  is the opposite of overfitting it occurs when your\\nmodel is too simple to learn the underlying structure of the data For example a lin\\near model of life satisfaction is prone to underfit reality is just more complex than\\nthe model so its predictions are bound to be inaccurate even on the training exam\\nples\\nThe main options to fix this problem are\\nSelecting a more powerful model with more parameters\\nFeeding better features to the learning algorithm feature engineering\\nReducing the constraints on the model eg reducing the regularization hyper\\nparameter\\nStepping Back\\nBy now you already know a lot about Machine Learning However we went through\\nso many concepts that you may be feeling a little lost so lets step back and look at the\\nbig picture\\n30  Chapter 1 The Machine Learning LandscapeMachine Learning is about', 'ck and look at the\\nbig picture\\n30  Chapter 1 The Machine Learning LandscapeMachine Learning is about making machines get better at some task by learning\\nfrom data instead of having to explicitly code rules\\nThere are many different types of ML systems supervised or not batch or online\\ninstancebased or modelbased and so on\\nIn a ML project you gather data in a training set and you feed the training set to\\na learning algorithm If the algorithm is modelbased it tunes some parameters to\\nfit the model to the training set ie to make good predictions on the training set\\nitself and then hopefully it will be able to make good predictions on new cases\\nas well If the algorithm is instancebased it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure\\nThe system will not perform well if your training set is too small or if the data is\\nnot representative noisy or polluted with irrelevant features garbage in garbage\\nout ', 'if the data is\\nnot representative noisy or polluted with irrelevant features garbage in garbage\\nout Lastly your model needs to be neither too simple in which case it will\\nunderfit nor too complex in which case it will overfit\\nTheres just one last important topic to cover once you have trained a model you\\ndont want to just hope it generalizes to new cases Y ou want to evaluate it and fine\\ntune it if necessary Lets see how\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases One way to do that is to put your model in production and moni\\ntor how well it performs This works well but if your model is horribly bad your\\nusers will complainnot the best idea\\nA better option is to split your data into two sets the training set  and the test set  As\\nthese names imply you train your model using the training set and you test it using\\nthe test set The error rate on new cases is called the generalization error  or outof\\nsampl', 'it using\\nthe test set The error rate on new cases is called the generalization error  or outof\\nsample error  and by evaluating your model on the test set you get an estimate of this\\nerror This value tells you how well your model will perform on instances it has never\\nseen before\\nIf the training error is low ie your model makes few mistakes on the training set\\nbut the generalization error is high it means that your model is overfitting the train\\ning data\\nIt is common to use 80 of the data for training and hold out  20\\nfor testing However this depends on the size of the dataset if it\\ncontains 10 million instances then holding out 1 means your test\\nset will contain 100000 instances thats probably more than\\nenough to get a good estimate of the generalization error\\nTesting and Validating  31Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough just use a test set Now suppose you are hesi\\ntating between two models say a linear model and a polynomial model how can\\ny', 'w suppose you are hesi\\ntating between two models say a linear model and a polynomial model how can\\nyou decide One option is to train both and compare how well they generalize using\\nthe test set\\nNow suppose that the linear model generalizes better but you want to apply some \\nregularization to avoid overfitting The question is how do you choose the value of\\nthe regularization hyperparameter One option is to train 100 different models using\\n100 different values for this hyperparameter Suppose you find the best hyperparame\\nter value that produces a model with the lowest generalization error say just 5 error\\nSo you launch this model into production but unfortunately it does not perform as\\nwell as expected and produces 15 errors What just happened\\nThe problem is that you measured the generalization error multiple times on the test\\nset and you adapted the model and hyperparameters to produce the best model for\\nthat particular set  This means that the model is unlikely to perform as well on ne', 'e best model for\\nthat particular set  This means that the model is unlikely to perform as well on new\\ndata\\nA common solution to this problem is called holdout validation  you simply hold out\\npart of the training set to evaluate several candidate models and select the best one\\nThe new heldout set is called the validation set  or sometimes the development set  or\\ndev set  More specifically you train multiple models with various hyperparameters\\non the reduced training set ie the full training set minus the validation set and\\nyou select the model that performs best on the validation set After this holdout vali\\ndation process you train the best model on the full training set including the valida\\ntion set and this gives you the final model Lastly you evaluate this final model on\\nthe test set to get an estimate of the generalization error\\nThis solution usually works quite well However if the validation set is too small then\\nmodel evaluations will be imprecise you may end up selecting a subopt', 'lidation set is too small then\\nmodel evaluations will be imprecise you may end up selecting a suboptimal model by\\nmistake Conversely if the validation set is too large then the remaining training set\\nwill be much smaller than the full training set Why is this bad Well since the final\\nmodel will be trained on the full training set it is not ideal to compare candidate\\nmodels trained on a much smaller training set It would be like selecting the fastest\\nsprinter to participate in a marathon One way to solve this problem is to perform\\nrepeated crossvalidation  using many small validation sets Each model is evaluated\\nonce per validation set after it is trained on the rest of the data By averaging out all\\nthe evaluations of a model we get a much more accurate measure of its performance\\nHowever there is a drawback the training time is multiplied by the number of valida\\ntion sets\\n32  Chapter 1 The Machine Learning Landscape11The Lack of A Priori Distinctions Between Learning Algorithms  D Wolpe', 'e Machine Learning Landscape11The Lack of A Priori Distinctions Between Learning Algorithms  D Wolpert 1996Data Mismatch\\nIn some cases it is easy to get a large amount of data for training but it is not per\\nfectly representative of the data that will be used in production For example suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter\\nmine their species Y ou can easily download millions of pictures of flowers on the\\nweb but they wont be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device Perhaps you only have 10000 representative\\npictures ie actually taken with the app In this case the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production so they should be composed exclusively\\nof representative pictures you can shuffle them and put half in the validation set and\\nhalf in the test set making sure that', 'es you can shuffle them and put half in the validation set and\\nhalf in the test set making sure that no duplicates or nearduplicates end up in both\\nsets After training your model on the web pictures if you observe that the perfor\\nmance of your model on the validation set is disappointing you will not know\\nwhether this is because your model has overfit the training set or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures One sol\\nution is to hold out part of the training pictures from the web in yet another set that\\nAndrew Ng calls the traindev set  After the model is trained on the training set not\\non the traindev set you can evaluate it on the traindev set if it performs well then\\nthe model is not overfitting the training set so if performs poorly on the validation\\nset the problem must come from the data mismatch Y ou can try to tackle this prob\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by', 'b\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app and then retraining the model Conversely if the\\nmodel performs poorly on the traindev set then the model must have overfit the\\ntraining set so you should try to simplify or regularize the model get more training\\ndata and clean up the training data as discussed earlier\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances How\\never to decide what data to discard and what data to keep you must make assump\\ntions  For example a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise which can safely be ignored\\nIn a famous 1996 paper 11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data then there is no reason to prefer', 'onstrated that if you make absolutely\\nno assumption about the data then there is no reason to prefer one model over any\\nother This is called the No Free Lunch  NFL theorem For some datasets the best\\nTesting and Validating  33model is a linear model while for other datasets it is a neural network There is no\\nmodel that is a priori  guaranteed to work better hence the name of the theorem The\\nonly way to know for sure which model is best is to evaluate them all Since this is not\\npossible in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models For example for simple tasks you may evalu\\nate linear models with various levels of regularization and for a complex problem you\\nmay evaluate various neural networks\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning In the next chapters we will dive deeper and write more code but before we\\ndo make sure you know how to answer the following questions\\n1', 'eper and write more code but before we\\ndo make sure you know how to answer the following questions\\n1How would you define Machine Learning\\n2Can you name four types of problems where it shines\\n3What is a labeled training set\\n4What are the two most common supervised tasks\\n5Can you name four common unsupervised tasks\\n6What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains\\n7What type of algorithm would you use to segment your customers into multiple\\ngroups\\n8Would you frame the problem of spam detection as a supervised learning prob\\nlem or an unsupervised learning problem\\n9What is an online learning system\\n10What is outofcore learning\\n11What type of learning algorithm relies on a similarity measure to make predic\\ntions\\n12What is the difference between a model parameter and a learning algorithms\\nhyperparameter\\n13What do modelbased learning algorithms search for What is the most common\\nstrategy they use to succeed How do they make predictions', 'orithms search for What is the most common\\nstrategy they use to succeed How do they make predictions\\n14Can you name four of the main challenges in Machine Learning\\n15If your model performs great on the training data but generalizes poorly to new\\ninstances what is happening Can you name three possible solutions\\n16What is a test set and why would you want to use it\\n34  Chapter 1 The Machine Learning Landscape17What is the purpose of a validation set\\n18What can go wrong if you tune hyperparameters using the test set\\n19What is repeated crossvalidation and why would you prefer it to using a single\\nvalidation set\\nSolutions to these exercises are available in \\nExercises  351The example project is completely fictitious the goal is just to illustrate the main steps of a Machine Learning\\nproject not to learn anything about the real estate business\\nCHAPTER 2\\nEndtoEnd Machine Learning Project\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he ', 'arly Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 2 in the final\\nrelease of the book\\nIn this chapter you will go through an example project end to end pretending to be a\\nrecently hired data scientist in a real estate company1 Here are the main steps you will\\ngo through\\n1Look at the big picture\\n2Get the data\\n3Discover and visualize the data to gain insights\\n4Prepare the data for Machine Learning algorithms\\n5Select a model and train it\\n6Finetune your model\\n7Present your solution\\n8Launch monitor and maintain your system\\n372The original dataset appeared in R Kelley Pace and Ronald Barry Sparse Spatial Autoregressions  Statistics\\n Probability Letters  33 no 3 1997 291297Working with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nrealworld data not just artif', 'learning about Machine Learning it is best to actually experiment with\\nrealworld data not just artificial datasets Fortunately there are thousands of open\\ndatasets to choose from ranging across all sorts of domains Here are a few places\\nyou can look to get data\\nPopular open data repositories\\nUC Irvine Machine Learning Repository\\nKaggle datasets\\nAmazons AWS datasets\\nMeta portals they list open data repositories\\nhttpdataportalsorg\\nhttpopendatamonitoreu\\nhttpquandlcom\\nOther pages listing many popular open data repositories\\nWikipedias list of Machine Learning datasets\\nQuoracom question\\nDatasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos\\nitory2 see Figure 21  This dataset was based on data from the 1990 California cen\\nsus It is not exactly recent you could still afford a nice house in the Bay Area at the\\ntime but it has many qualities for learning so we will pretend it is recent data We\\nalso added a categorical attribute and removed a few ', 'arning so we will pretend it is recent data We\\nalso added a categorical attribute and removed a few features for teaching purposes\\n38  Chapter 2 EndtoEnd Machine Learning ProjectFigure 21 California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen\\nsus data This data has metrics such as the population median income median hous\\ning price and so on for each block group in California Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data a block\\ngroup typically has a population of 600 to 3000 people We will just call them dis\\ntricts for short\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district given all the other metrics\\nSince you are a wellorganized data scientist the first thing you do\\nis to pull out your Machine Learning project checklist', 'ganized data scientist the first thing you do\\nis to pull out your Machine Learning project checklist Y ou can\\nstart with the one in  it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs\\nIn this chapter we will go through many checklist items but we will\\nalso skip a few either because they are selfexplanatory or because\\nthey will be discussed in later chapters\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective building a\\nmodel is probably not the end goal How does the company expect to use and benefit\\nLook at the Big Picture  393A piece of information fed to a Machine Learning system is often called a signal  in reference to Shannons\\ninformation theory you want a high signalnoise ratiofrom this model This is important because it will determine how you frame the\\nproblem what algorithms you will select what performance measure you will use to\\nevaluate your model and how much effort you should', 'l select what performance measure you will use to\\nevaluate your model and how much effort you should spend tweaking it\\nY our boss answers that your models output a prediction of a districts median hous\\ning price will be fed to another Machine Learning system see Figure 22  along\\nwith many other signals 3 This downstream system will determine whether it is worth\\ninvesting in a given area or not Getting this right is critical as it directly affects reve\\nnue\\nFigure 22 A Machine Learning pipeline for real estate investments\\nPipelines\\nA sequence of data processing components  is called a data pipeline  Pipelines are very\\ncommon in Machine Learning systems since there is a lot of data to manipulate and\\nmany data transformations to apply\\nComponents typically run asynchronously Each component pulls in a large amount\\nof data processes it and spits out the result in another data store and then some time\\nlater the next component in the pipeline pulls this data and spits out its own output\\nand so ', 'e time\\nlater the next component in the pipeline pulls this data and spits out its own output\\nand so on Each component is fairly selfcontained the interface between components\\nis simply the data store This makes the system quite simple to grasp with the help of\\na data flow graph and different teams can focus on different components Moreover\\nif a component breaks down the downstream components can often continue to run\\nnormally at least for a while by just using the last output from the broken compo\\nnent This makes the architecture quite robust\\n40  Chapter 2 EndtoEnd Machine Learning ProjectOn the other hand a broken component can go unnoticed for some time if proper\\nmonitoring is not implemented The data gets stale and the overall systems perfor\\nmance drops\\nThe next question to ask is what the current solution looks like if any It will often\\ngive you a reference performance as well as insights on how to solve the problem\\nY our boss answers that the district housing prices are currently ', 'ights on how to solve the problem\\nY our boss answers that the district housing prices are currently estimated manually\\nby experts a team gathers uptodate information about a district and when they\\ncannot get the median housing price they estimate it using complex rules\\nThis is costly and timeconsuming and their estimates are not great in cases where\\nthey manage to find out the actual median housing price they often realize that their\\nestimates were off by more than 20 This is why the company thinks that it would\\nbe useful to train a model to predict a districts median housing price given other data\\nabout that district The census data looks like a great dataset to exploit for this pur\\npose since it includes the median housing prices of thousands of districts as well as\\nother data\\nOkay with all this information you are now ready to start designing your system\\nFirst you need to frame the problem is it supervised unsupervised or Reinforce\\nment Learning Is it a classification task a regress', 'oblem is it supervised unsupervised or Reinforce\\nment Learning Is it a classification task a regression task or something else Should\\nyou use batch learning or online learning techniques Before you read on pause and\\ntry to answer these questions for yourself\\nHave you found the answers Lets see it is clearly a typical supervised learning task\\nsince you are given labeled  training examples each instance comes with the expected\\noutput ie the districts median housing price Moreover it is also a typical regres\\nsion task since you are asked to predict a value More specifically this is a multiple\\nregression  problem since the system will use multiple features to make a prediction it\\nwill use the districts population the median income etc It is also a univariate\\nregression  problem since we are only trying to predict a single value for each district\\nIf we were trying to predict multiple values per district it would be a multivariate\\nregression  problem Finally there is no continuous flow of da', 'er district it would be a multivariate\\nregression  problem Finally there is no continuous flow of data coming in the system\\nthere is no particular need to adjust to changing data rapidly and the data is small\\nenough to fit in memory so plain batch learning should do just fine\\nIf the data was huge you could either split your batch learning\\nwork across multiple servers using the MapReduce  technique or\\nyou could use an online learning technique instead\\nLook at the Big Picture  41Select a Performance Measure\\nY our next step is to select a performance measure A typical performance measure for\\nregression problems is the Root Mean Square Error RMSE It gives an idea of how\\nmuch error the system typically makes in its predictions with a higher weight for\\nlarge errors Equation 21  shows the mathematical formula to compute the RMSE\\nEquation 21 Root Mean Square Error RMSE\\nRMSE Xh1\\nm\\ni 1m\\nhxiyi2\\n42  Chapter 2 EndtoEnd Machine Learning Project4Recall that the transpose operator flips a column vecto', ' Chapter 2 EndtoEnd Machine Learning Project4Recall that the transpose operator flips a column vector into a row vector and vice versaNotations\\nThis equation introduces several very common Machine Learning notations that we\\nwill use throughout this book\\nm is the number of instances in the dataset you are measuring the RMSE on\\nFor example if you are evaluating the RMSE on a validation set of 2000 dis\\ntricts then m  2000\\nxi is a vector of all the feature values excluding the label of the ith instance in\\nthe dataset and yi is its label the desired output value for that instance\\nFor example if the first district in the dataset is located at longitude 11829\\nlatitude 3391 and it has 1416 inhabitants with a median income of 38372\\nand the median house value is 156400 ignoring the other features for now\\nthen\\nx1118  29\\n33  91\\n1 416\\n38 372\\nand\\ny1 156 400\\nX is a matrix containing all the feature values excluding labels of all instances in\\nthe dataset There is one row per instance and the ith row i', 'ues excluding labels of all instances in\\nthe dataset There is one row per instance and the ith row is equal to the transpose\\nof xi noted  xiT4\\nFor example if the first district is as just described then the matrix X looks\\nlike this\\nXx1T\\nx2T\\n\\nx1999 T\\nx2000 T118  29 33  91 1 416 38 372\\n   \\nLook at the Big Picture  43h is your systems prediction function also called a hypothesis  When your system\\nis given an instances feature vector xi it outputs a predicted value i  hxi\\nfor that instance   is pronounced yhat\\nFor example if your system predicts that the median housing price in the first\\ndistrict is 158400 then 1  hx1  158400 The prediction error for this\\ndistrict is 1  y1  2000\\nRMSE Xh is the cost function measured on the set of examples using your\\nhypothesis h\\nWe use lowercase italic font for scalar values such as m or yi and function names\\nsuch as h lowercase bold font for vectors such as xi and uppercase bold font for\\nmatrices such as X\\nEven though the RMSE is generally the preferred p', ' xi and uppercase bold font for\\nmatrices such as X\\nEven though the RMSE is generally the preferred performance measure for regression\\ntasks in some contexts you may prefer to use another function For example suppose\\nthat there are many outlier districts In that case you may consider using the Mean\\nAbsolute Error  also called the Average Absolute Deviation see Equation 22 \\nEquation 22 Mean Absolute Error\\nMAE Xh1\\nm\\ni 1m\\nhxiyi\\nBoth the RMSE and the MAE are ways to measure the distance between two vectors\\nthe vector of predictions and the vector of target values Various distance measures\\nor norms  are possible\\nComputing the root of a sum of squares RMSE corresponds to the Euclidean\\nnorm  it is the notion of distance you are familiar with It is also called the 2\\nnorm  noted   2 or just   \\nComputing the sum of absolutes MAE corresponds to the 1 norm  noted   1\\nIt is sometimes called the Manhattan norm  because it measures the distance\\nbetween two points in a city if you can only travel along', 'tan norm  because it measures the distance\\nbetween two points in a city if you can only travel along orthogonal city blocks\\nMore generally the k norm  of a vector v containing n elements is defined as\\n  kv0kv1kvnk1\\nk 0 just gives the number of nonzero ele\\nments in the vector and  gives the maximum absolute value in the vector\\nThe higher the norm index the more it focuses on large values and neglects small\\nones This is why the RMSE is more sensitive to outliers than the MAE But when\\n44  Chapter 2 EndtoEnd Machine Learning Project5The latest version of Python 3 is recommended Python 27 may work too but it is now deprecated all major\\nscientific libraries are dropping support for it so you should migrate to Python 3 as soon as possibleoutliers are exponentially rare like in a bellshaped curve the RMSE performs\\nvery well and is generally preferred\\nCheck the Assumptions\\nLastly it is good practice to list and verify the assumptions that were made so far by\\nyou or others this can catch serious', 'ice to list and verify the assumptions that were made so far by\\nyou or others this can catch serious issues early on For example the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem and we assume that these prices are going to be used as such But what if the\\ndownstream system actually converts the prices into categories eg cheap \\nmedium  or expensive and then uses those categories instead of the prices them\\nselves In this case getting the price perfectly right is not important at all your sys\\ntem just needs to get the category right If thats so then the problem should have\\nbeen framed as a classification task not a regression task Y ou dont want to find this\\nout after working on a regression system for months\\nFortunately after talking with the team in charge of the downstream system you are\\nconfident that they do indeed need the actual prices not just categories Great Y oure\\nall set the lights are green and you can start coding now\\n', 'l prices not just categories Great Y oure\\nall set the lights are green and you can start coding now\\nGet the Data\\nIts time to get your hands dirty Dont hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook The full Jupyter note\\nbook is available at httpsgithubcomageronhandsonml2 \\nCreate the Workspace\\nFirst you will need to have Python installed It is probably already installed on your\\nsystem If not you can get it at httpswwwpythonorg 5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets Open a terminal and type the following commands after the  prompts\\n export MLPATHHOMEml       You can change the path if you prefer\\n mkdir p MLPATH\\nY ou will need a number of Python modules Jupyter NumPy Pandas Matplotlib and\\nScikitLearn If you already have Jupyter running with all these modules installed\\nyou can safely skip to Download the Data on page 49 If you dont have them yet\\nthere are many ways to install them and t', ' to Download the Data on page 49 If you dont have them yet\\nthere are many ways to install them and their dependencies Y ou can use your sys\\nGet the Data  456We will show the installation steps using pip in a bash shell on a Linux or MacOS system Y ou may need to\\nadapt these commands to your own system On Windows we recommend installing Anaconda instead\\n7If you want to upgrade pip for all users on your machine rather than just your own user you should remove\\nthe user  option and make sure you have administrator rights eg by adding sudo  before the whole com\\nmand on Linux or MacOSX\\n8Alternative tools include venv very similar to virtualenv and included in the standard library virtualenv\\nwrapper provides extra functionalities on top of virtualenv pyenv allows easy switching between Python\\nversions and pipenv a great packaging tool by the same author as the popular requests  library built on top\\nof pip virtualenv and moretems packaging system eg aptget on Ubuntu or MacPorts or HomeBrew on\\n', ' top\\nof pip virtualenv and moretems packaging system eg aptget on Ubuntu or MacPorts or HomeBrew on\\nMacOS install a Scientific Python distribution such as Anaconda and use its packag\\ning system or just use Pythons own packaging system pip which is included by\\ndefault with the Python binary installers since Python 2796 Y ou can check to see if\\npip is installed by typing the following command\\n python3 m pip version\\npip 1902 from libpython36sitepackages python 36\\nY ou should make sure you have a recent version of pip installed To upgrade the pip\\nmodule type7\\n python3 m pip install user U pip\\nCollecting pip\\n\\nSuccessfully installed pip1902\\nCreating an Isolated Environment\\nIf you would like to work in an isolated environment which is strongly recom\\nmended so you can work on different projects without having conflicting library ver\\nsions install virtualenv8 by running the following pip command again if you want\\nvirtualenv to be installed for all users on your machine remove user  and run this', 'again if you want\\nvirtualenv to be installed for all users on your machine remove user  and run this\\ncommand with administrator rights\\n python3 m pip install user U virtualenv\\nCollecting virtualenv\\n\\nSuccessfully installed virtualenv\\nNow you can create an isolated Python environment by typing\\n cd MLPATH\\n virtualenv env\\nUsing base prefix \\nNew python executable in mlenvbinpython36\\nAlso creating executable in mlenvbinpython\\nInstalling setuptools pip wheeldone\\n46  Chapter 2 EndtoEnd Machine Learning Project9Note that Jupyter can handle multiple versions of Python and even many other languages such as R or\\nOctaveNow every time you want to activate this environment just open a terminal and type\\n cd MLPATH\\n source envbinactivate  on Linux or MacOSX\\n envScriptsactivate   on Windows\\nTo deactivate this environment just type deactivate  While the environment is\\nactive any package you install using pip will be installed in this isolated environment\\nand Python will only have access to these packages', 'ip will be installed in this isolated environment\\nand Python will only have access to these packages if you also want access to the sys\\ntems packages you should create the environment using virtualenvs systemsite\\npackages  option Check out virtualenvs documentation for more information\\nNow you can install all the required modules and their dependencies using this sim\\nple pip command if you are not using a virtualenv you will need the user  option\\nor administrator rights\\n python3 m pip install U jupyter matplotlib numpy pandas scipy scikitlearn\\nCollecting jupyter\\n  Downloading jupyter100py2py3noneanywhl\\nCollecting matplotlib\\n  \\nTo check your installation try to import every module like this\\n python3 c import jupyter matplotlib numpy pandas scipy sklearn\\nThere should be no output and no error Now you can fire up Jupyter by typing\\n jupyter notebook\\nI 1524 NotebookApp Serving notebooks from local directory ml\\nI 1524 NotebookApp 0 active kernels\\nI 1524 NotebookApp The Jupyter Notebook is ru', 'local directory ml\\nI 1524 NotebookApp 0 active kernels\\nI 1524 NotebookApp The Jupyter Notebook is running at httplocalhost8888\\nI 1524 NotebookApp Use ControlC to stop this server and shut down all\\nkernels twice to skip confirmation\\nA Jupyter server is now running in your terminal listening to port 8888 Y ou can visit\\nthis server by opening your web browser to httplocalhost8888  this usually hap\\npens automatically when the server starts Y ou should see your empty workspace\\ndirectory containing only the env directory if you followed the preceding virtualenv\\ninstructions\\nNow create a new Python notebook by clicking on the New button and selecting the\\nappropriate Python version9 see Figure 23 \\nThis does three things first it creates a new notebook file called Untitledipynb  in\\nyour workspace second it starts a Jupyter Python kernel to run this notebook and\\nGet the Data  47third it opens this notebook in a new tab Y ou should start by renaming this note\\nbook to Housing this will automatical', ' notebook in a new tab Y ou should start by renaming this note\\nbook to Housing this will automatically rename the file to Housingipynb  by click\\ning Untitled and typing the new name\\nFigure 23 Your workspace in Jupyter\\nA notebook contains a list of cells Each cell can contain executable code or formatted\\ntext Right now the notebook contains only one empty code cell labeled In 1  Try\\ntyping printHello world  in the cell and click on the play button see\\nFigure 24  or press ShiftEnter This sends the current cell to this notebooks Python\\nkernel which runs it and returns the output The result is displayed below the cell\\nand since we reached the end of the notebook a new cell is automatically created Go\\nthrough the User Interface Tour from Jupyters Help menu to learn the basics\\nFigure 24 Hello world Python notebook\\n48  Chapter 2 EndtoEnd Machine Learning Project10Y ou might also need to check legal constraints such as private fields that should never be copied to unsafe\\ndatastores\\n11In a real', 'egal constraints such as private fields that should never be copied to unsafe\\ndatastores\\n11In a real project you would save this code in a Python file but for now you can just write it in your Jupyter\\nnotebookDownload the Data\\nIn typical environments your data would be available in a relational database or\\nsome other common datastore and spread across multiple tablesdocumentsfiles To\\naccess it you would first need to get your credentials and access authorizations10 and\\nfamiliarize yourself with the data schema In this project however things are much\\nsimpler you will just download a single compressed file housingtgz  which contains a\\ncommaseparated value CSV file called housingcsv  with all the data\\nY ou could use your web browser to download it and run tar xzf housingtgz  to\\ndecompress the file and extract the CSV file but it is preferable to create a small func\\ntion to do that It is useful in particular if data changes regularly as it allows you to\\nwrite a small script that you can ru', 'ful in particular if data changes regularly as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data or\\nyou can set up a scheduled job to do that automatically at regular intervals Auto\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines\\nHere is the function to fetch the data11\\nimport os\\nimport tarfile\\nfrom sixmoves  import urllib\\nDOWNLOADROOT   httpsrawgithubusercontentcomageronhandsonml2master\\nHOUSINGPATH   ospathjoindatasets  housing \\nHOUSINGURL   DOWNLOADROOT   datasetshousinghousingtgz\\ndef fetchhousingdata housingurl HOUSINGURL  housingpath HOUSINGPATH \\n    if not ospathisdirhousingpath \\n        osmakedirs housingpath \\n    tgzpath   ospathjoinhousingpath  housingtgz \\n    urllibrequesturlretrieve housingurl  tgzpath \\n    housingtgz   tarfileopentgzpath \\n    housingtgz extractall pathhousingpath \\n    housingtgz close\\nNow when you call fetchhousingdata  it creates a datasetshousing ', 'hhousingpath \\n    housingtgz close\\nNow when you call fetchhousingdata  it creates a datasetshousing  directory in\\nyour workspace downloads the housingtgz  file and extracts the housingcsv  from it in\\nthis directory\\nNow lets load the data using Pandas Once again you should write a small function to\\nload the data\\nGet the Data  49import pandas as pd\\ndef loadhousingdata housingpath HOUSINGPATH \\n    csvpath   ospathjoinhousingpath  housingcsv \\n    return pdreadcsv csvpath \\nThis function returns a Pandas DataFrame object containing all the data\\nTake a Quick Look at the Data Structure\\nLets take a look at the top five rows using the DataFrames head  method see\\nFigure 25 \\nFigure 25 Top five rows in the dataset\\nEach row represents one district There are 10 attributes you can see the first 6 in the\\nscreenshot longitude  latitude  housingmedianage  totalrooms  totalbed\\nrooms  population  households  medianincome  medianhousevalue  and\\noceanproximity \\nThe info  method is useful to get a quick descr', ' medianincome  medianhousevalue  and\\noceanproximity \\nThe info  method is useful to get a quick description of the data in particular the\\ntotal number of rows and each attributes type and number of nonnull values see\\nFigure 26 \\n50  Chapter 2 EndtoEnd Machine Learning ProjectFigure 26 Housing info\\nThere are 20640 instances in the dataset which means that it is fairly small by\\nMachine Learning standards but its perfect to get started Notice that the totalbed\\nrooms  attribute has only 20433 nonnull values meaning that 207 districts are miss\\ning this feature We will need to take care of this later\\nAll attributes are numerical except the oceanproximity  field Its type is object  so it\\ncould hold any kind of Python object but since you loaded this data from a CSV file\\nyou know that it must be a text attribute When you looked at the top five rows you\\nprobably noticed that the values in the oceanproximity  column were repetitive\\nwhich means that it is probably a categorical attribute Y ou can f', 'proximity  column were repetitive\\nwhich means that it is probably a categorical attribute Y ou can find out what cate\\ngories exist and how many districts belong to each category by using the\\nvaluecounts  method\\n housingoceanproximity valuecounts \\n1H OCEAN     9136\\nINLAND        6551\\nNEAR OCEAN    2658\\nNEAR BAY      2290\\nISLAND           5\\nName oceanproximity dtype int64\\nLets look at the other fields The describe  method shows a summary of the\\nnumerical attributes  Figure 27 \\nGet the Data  5112The standard deviation is generally denoted  the Greek letter sigma and it is the square root of the var\\niance  which is the average of the squared deviation from the mean When a feature has a bellshaped normal\\ndistribution  also called a Gaussian distribution  which is very common the 6895997 rule applies about\\n68 of the values fall within 1 of the mean 95 within 2 and 997 within 3\\nFigure 27 Summary of each numerical attribute\\nThe count  mean  min and max rows are selfexplanatory Note that the nu', 'y of each numerical attribute\\nThe count  mean  min and max rows are selfexplanatory Note that the null values are\\nignored so for example count  of totalbedrooms  is 20433 not 20640 The std\\nrow shows the standard deviation  which measures how dispersed the values are12\\nThe 25 50 and 75 rows show the corresponding percentiles  a percentile indi\\ncates the value below which a given percentage of observations in a group of observa\\ntions falls For example 25 of the districts have a housingmedianage  lower than\\n18 while 50 are lower than 29 and 75 are lower than 37 These are often called the\\n25th percentile or 1st quartile  the median and the 75th percentile or 3rd quartile\\nAnother quick way to get a feel of the type of data you are dealing with is to plot a \\nhistogram for each numerical attribute A histogram shows the number of instances\\non the vertical axis that have a given value range on the horizontal axis Y ou can\\neither plot this one attribute at a time or you can call the hist  method', 'e horizontal axis Y ou can\\neither plot this one attribute at a time or you can call the hist  method on the\\nwhole dataset and it will plot a histogram for each numerical attribute see\\nFigure 28  For example you can see that slightly over 800 districts have a\\nmedianhousevalue  equal to about 100000\\nmatplotlib  inline    only in a Jupyter notebook\\nimport matplotlibpyplot  as plt\\nhousinghistbins50 figsize2015\\npltshow\\n52  Chapter 2 EndtoEnd Machine Learning ProjectThe hist  method relies on Matplotlib which in turn relies on a\\nuserspecified graphical backend to draw on your screen So before\\nyou can plot anything you need to specify which backend Matplot\\nlib should use The simplest option is to use Jupyters magic com\\nmand matplotlib inline  This tells Jupyter to set up Matplotlib\\nso it uses Jupyters own backend Plots are then rendered within the\\nnotebook itself Note that calling show  is optional in a Jupyter\\nnotebook as Jupyter will automatically display plots when a cell is\\nexecuted\\nFigur', 'onal in a Jupyter\\nnotebook as Jupyter will automatically display plots when a cell is\\nexecuted\\nFigure 28 A histogram for each numerical attribute\\nNotice a few things in these histograms\\n1First the median income attribute does not look like it is expressed in US dollars\\nUSD After checking with the team that collected the data you are told that the\\ndata has been scaled and capped at 15 actually 150001 for higher median\\nincomes and at 05 actually 04999 for lower median incomes The numbers\\nrepresent roughly tens of thousands of dollars eg 3 actually means about\\n30000 Working with preprocessed attributes is common in Machine Learning\\nGet the Data  53and it is not necessarily a problem but you should try to understand how the\\ndata was computed\\n2The housing median age and the median house value were also capped The lat\\nter may be a serious problem since it is your target attribute your labels Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit\\nY ou need to check', ' our\\nMachine Learning algorithms may learn that prices never go beyond that limit\\nY ou need to check with your client team the team that will use your systems out\\nput to see if this is a problem or not If they tell you that they need precise pre\\ndictions even beyond 500000 then you have mainly two options\\naCollect proper labels for the districts whose labels were capped\\nbRemove those districts from the training set and also from the test set since\\nyour system should not be evaluated poorly if it predicts values beyond\\n500000\\n3These attributes have very different scales We will discuss this later in this chap\\nter when we explore feature scaling\\n4Finally many histograms are tail heavy  they extend much farther to the right of\\nthe median than to the left This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns We will try transforming these attributes\\nlater on to have more bellshaped distributions\\nHopefully you now have a better understanding of the kind of da', 'o have more bellshaped distributions\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith\\nWait Before you look at the data any further you need to create a\\ntest set put it aside and never look at it\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage After all\\nyou have only taken a quick glance at the data and surely you should learn a whole\\nlot more about it before you decide what algorithms to use right This is true but\\nyour brain is an amazing pattern detection system which means that it is highly\\nprone to overfitting if you look at the test set you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model When you estimate the generalization error using the test\\nset your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected This is called data snooping  bias\\nCreating a test set i', 'm that will not\\nperform as well as expected This is called data snooping  bias\\nCreating a test set is theoretically quite simple just pick some instances randomly\\ntypically 20 of the dataset or less if your dataset is very large and set them aside\\n54  Chapter 2 EndtoEnd Machine Learning Project13In this book when a code example contains a mix of code and outputs as is the case here it is formatted like\\nin the Python interpreter for better readability the code lines are prefixed with  or  for indented\\nblocks and the outputs have no prefix\\n14Y ou will often see people set the random seed to 42 This number has no special property other than to be\\nThe Answer to the Ultimate Question of Life the Universe and Everythingimport numpy as np\\ndef splittraintest data testratio \\n    shuffledindices   nprandompermutation lendata\\n    testsetsize   intlendata  testratio \\n    testindices   shuffledindices testsetsize \\n    trainindices   shuffledindices testsetsize \\n    return datailoctrainindices  data', 's testsetsize \\n    trainindices   shuffledindices testsetsize \\n    return datailoctrainindices  datailoctestindices \\nY ou can then use this function like this13\\n trainset  testset   splittraintest housing 02\\n lentrainset \\n16512\\n lentestset \\n4128\\nWell this works but it is not perfect if you run the program again it will generate a\\ndifferent test set Over time you or your Machine Learning algorithms will get to\\nsee the whole dataset which is what you want to avoid\\nOne solution is to save the test set on the first run and then load it in subsequent\\nruns Another option is to set the random number generators seed eg npran\\ndomseed42 14 before calling nprandompermutation  so that it always generates\\nthe same shuffled indices\\nBut both these solutions will break next time you fetch an updated dataset A com\\nmon solution is to use each instances identifier to decide whether or not it should go\\nin the test set assuming instances have a unique and immutable identifier For\\nexample you could compute ', 'he test set assuming instances have a unique and immutable identifier For\\nexample you could compute a hash of each instances identifier and put that instance\\nin the test set if the hash is lower or equal to 20 of the maximum hash value This\\nensures that the test set will remain consistent across multiple runs even if you\\nrefresh the dataset The new test set will contain 20 of the new instances but it will\\nnot contain any instance that was previously in the training set Here is a possible\\nimplementation\\nfrom zlib import crc32\\ndef testsetcheck identifier  testratio \\n    return crc32npint64identifier   0xffffffff   testratio   232\\ndef splittraintestbyid data testratio  idcolumn \\n    ids  dataidcolumn \\nGet the Data  5515The location information is actually quite coarse and as a result many districts will have the exact same ID so\\nthey will end up in the same set test or train This introduces some unfortunate sampling bias    intestset   idsapplylambda id testsetcheck id testratio \\n    retu', ' some unfortunate sampling bias    intestset   idsapplylambda id testsetcheck id testratio \\n    return datalocintestset  datalocintestset \\nUnfortunately the housing dataset does not have an identifier column The simplest\\nsolution is to use the row index as the ID\\nhousingwithid   housingresetindex     adds an index column\\ntrainset  testset   splittraintestbyid housingwithid  02 index\\nIf you use the row index as a unique identifier you need to make sure that new data\\ngets appended to the end of the dataset and no row ever gets deleted If this is not\\npossible then you can try to use the most stable features to build a unique identifier\\nFor example a districts latitude and longitude are guaranteed to be stable for a few\\nmillion years so you could combine them into an ID like so15\\nhousingwithid id  housinglongitude   1000  housinglatitude \\ntrainset  testset   splittraintestbyid housingwithid  02 id\\nScikitLearn provides a few functions to split datasets into multiple subsets in various\\nways ', ' 02 id\\nScikitLearn provides a few functions to split datasets into multiple subsets in various\\nways The simplest function is traintestsplit  which does pretty much the same\\nthing as the function splittraintest  defined earlier with a couple of additional\\nfeatures First there is a randomstate  parameter that allows you to set the random\\ngenerator seed as explained previously and second you can pass it multiple datasets\\nwith an identical number of rows and it will split them on the same indices this is\\nvery useful for example if you have a separate DataFrame for labels\\nfrom sklearnmodelselection  import traintestsplit\\ntrainset  testset   traintestsplit housing testsize 02 randomstate 42\\nSo far we have considered purely random sampling methods This is generally fine if\\nyour dataset is large enough especially relative to the number of attributes but if it\\nis not you run the risk of introducing a significant sampling bias When a survey\\ncompany decides to call 1000 people to ask them a few q', 'ng a significant sampling bias When a survey\\ncompany decides to call 1000 people to ask them a few questions they dont just pick\\n1000 people randomly in a phone book They try to ensure that these 1000 people\\nare representative of the whole population For example the US population is com\\nposed of 513 female and 487 male so a wellconducted survey in the US would\\ntry to maintain this ratio in the sample 513 female and 487 male This is called strati\\nfied sampling  the population is divided into homogeneous subgroups called strata \\nand the right number of instances is sampled from each stratum to guarantee that the\\ntest set is representative of the overall population If they used purely random sam\\npling there would be about 12 chance of sampling a skewed test set with either less\\nthan 49 female or more than 54 female Either way the survey results would be\\nsignificantly biased\\n56  Chapter 2 EndtoEnd Machine Learning ProjectSuppose you chatted with experts who told you that the median income ', 'dtoEnd Machine Learning ProjectSuppose you chatted with experts who told you that the median income is a very\\nimportant attribute to predict median housing prices Y ou may want to ensure that\\nthe test set is representative of the various categories of incomes in the whole dataset\\nSince the median income is a continuous numerical attribute you first need to create\\nan income category attribute Lets look at the median income histogram more closely\\nback in Figure 28  most median income values are clustered around 15 to 6 ie\\n1500060000 but some median incomes go far beyond 6 It is important to have\\na sufficient number of instances in your dataset for each stratum or else the estimate\\nof the stratums importance may be biased This means that you should not have too\\nmany strata and each stratum should be large enough The following code uses the\\npdcut  function to create an income category attribute with 5 categories labeled\\nfrom 1 to 5 category 1 ranges from 0 to 15 ie less than 15000 category', 'ute with 5 categories labeled\\nfrom 1 to 5 category 1 ranges from 0 to 15 ie less than 15000 category 2 from\\n15 to 3 and so on\\nhousingincomecat   pdcuthousingmedianincome \\n                               bins0 15 30 45 6 npinf\\n                               labels1 2 3 4 5\\nThese income categories are represented in Figure 29 \\nhousingincomecat hist\\nFigure 29 Histogram of income categories\\nNow you are ready to do stratified sampling based on the income category For this\\nyou can use ScikitLearns StratifiedShuffleSplit  class\\nfrom sklearnmodelselection  import StratifiedShuffleSplit\\nsplit  StratifiedShuffleSplit nsplits 1 testsize 02 randomstate 42\\nfor trainindex  testindex  in splitsplithousing housingincomecat \\n    strattrainset   housingloctrainindex \\n    strattestset   housingloctestindex \\nGet the Data  57Lets see if this worked as expected Y ou can start by looking at the income category\\nproportions in the test set\\n strattestset incomecat valuecounts   lenstrattestset \\n3    0350533\\n2   ', 'proportions in the test set\\n strattestset incomecat valuecounts   lenstrattestset \\n3    0350533\\n2    0318798\\n4    0176357\\n5    0114583\\n1    0039729\\nName incomecat dtype float64\\nWith similar code you can measure the income category proportions in the full data\\nset Figure 210  compares the income category proportions in the overall dataset in\\nthe test set generated with stratified sampling and in a test set generated using purely\\nrandom sampling As you can see the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset whereas the\\ntest set generated using purely random sampling is quite skewed\\nFigure 210 Sampling bias comparison of stratified  versus purely random sampling\\nNow you should remove the incomecat  attribute so the data is back to its original\\nstate\\nfor set in strattrainset  strattestset \\n    setdropincomecat  axis1 inplaceTrue\\nWe spent quite a bit of time on test set generation for a good reason this is an oft', 'is1 inplaceTrue\\nWe spent quite a bit of time on test set generation for a good reason this is an often\\nneglected but critical part of a Machine Learning project Moreover many of these\\nideas will be useful later when we discuss crossvalidation Now its time to move on\\nto the next stage exploring the data\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating Now the goal is to go a little bit more in depth\\nFirst make sure you have put the test set aside and you are only exploring the train\\ning set Also if the training set is very large you may want to sample an exploration\\n58  Chapter 2 EndtoEnd Machine Learning Projectset to make manipulations easy and fast In our case the set is quite small so you can\\njust work directly on the full set Lets create a copy so you can play with it without\\nharming the training set\\nhousing  strattrainset copy\\nVisualizing Geographical Data\\n', ' with it without\\nharming the training set\\nhousing  strattrainset copy\\nVisualizing Geographical Data\\nSince there is geographical information latitude and longitude it is a good idea to\\ncreate a scatterplot of all districts to visualize the data  Figure 211 \\nhousingplotkindscatter  xlongitude  ylatitude \\nFigure 211 A geographical scatterplot of the data\\nThis looks like California all right but other than that it is hard to see any particular\\npattern Setting the alpha  option to 01 makes it much easier to visualize the places\\nwhere there is a high density of data points  Figure 212 \\nhousingplotkindscatter  xlongitude  ylatitude  alpha01\\nDiscover and Visualize the Data to Gain Insights  5916If you are reading this in grayscale grab a red pen and scribble over most of the coastline from the Bay Area\\ndown to San Diego as you might expect Y ou can add a patch of yellow around Sacramento as well\\nFigure 212 A better visualization highlighting highdensity areas\\nNow thats much better you can clea', '\\nFigure 212 A better visualization highlighting highdensity areas\\nNow thats much better you can clearly see the highdensity areas namely the Bay\\nArea and around Los Angeles and San Diego plus a long line of fairly high density in\\nthe Central Valley in particular around Sacramento and Fresno\\nMore generally our brains are very good at spotting patterns on pictures but you\\nmay need to play around with visualization parameters to make the patterns stand\\nout\\nNow lets look at the housing prices  Figure 213  The radius of each circle represents\\nthe districts population option s and the color represents the price option c We\\nwill use a predefined color map option cmap  called jet which ranges from blue\\nlow values to red high prices16\\nhousingplotkindscatter  xlongitude  ylatitude  alpha04\\n    shousingpopulation 100 labelpopulation  figsize107\\n    cmedianhousevalue  cmappltgetcmap jet colorbar True\\n\\npltlegend\\n60  Chapter 2 EndtoEnd Machine Learning ProjectFigure 213 California housing prices\\nDis', 'e\\n\\npltlegend\\n60  Chapter 2 EndtoEnd Machine Learning ProjectFigure 213 California housing prices\\nDiscover and Visualize the Data to Gain Insights  61This image tells you that the housing prices are very much related to the location\\neg close to the ocean and to the population density as you probably knew already\\nIt will probably be useful to use a clustering algorithm to detect the main clusters and\\nadd new features that measure the proximity to the cluster centers The ocean prox\\nimity attribute may be useful as well although in Northern California the housing\\nprices in coastal districts are not too high so it is not a simple rule\\nLooking for Correlations\\nSince the dataset is not too large you can easily compute the standard correlation\\ncoefficient  also called Pearsons r  between every pair of attributes using the corr\\nmethod\\ncorrmatrix   housingcorr\\nNow lets look at how much each attribute correlates with the median house value\\n corrmatrix medianhousevalue sortvalues ascending False\\nm', 'ute correlates with the median house value\\n corrmatrix medianhousevalue sortvalues ascending False\\nmedianhousevalue    1000000\\nmedianincome         0687170\\ntotalrooms           0135231\\nhousingmedianage    0114220\\nhouseholds            0064702\\ntotalbedrooms        0047865\\npopulation           0026699\\nlongitude            0047279\\nlatitude             0142826\\nName medianhousevalue dtype float64\\nThe correlation coefficient ranges from 1 to 1 When it is close to 1 it means that\\nthere is a strong positive correlation for example the median house value tends to go\\nup when the median income goes up When the coefficient is close to 1 it means\\nthat there is a strong negative correlation you can see a small negative correlation\\nbetween the latitude and the median house value ie prices have a slight tendency to\\ngo down when you go north Finally coefficients close to zero mean that there is no\\nlinear correlation Figure 214  shows various plots along with the correlation coeffi\\ncient between their h', ' correlation Figure 214  shows various plots along with the correlation coeffi\\ncient between their horizontal and vertical axes\\n62  Chapter 2 EndtoEnd Machine Learning ProjectFigure 214 Standard correlation coefficient  of various datasets source Wikipedia\\npublic domain image\\nThe correlation coefficient only measures linear correlations if x\\ngoes up then y generally goes updown It may completely miss\\nout on nonlinear relationships eg if x is close to zero then y gen\\nerally goes up Note how all the plots of the bottom row have a\\ncorrelation coefficient equal to zero despite the fact that their axes\\nare clearly not independent these are examples of nonlinear rela\\ntionships Also the second row shows examples where the correla\\ntion coefficient is equal to 1 or 1 notice that this has nothing to\\ndo with the slope For example your height in inches has a correla\\ntion coefficient of 1 with your height in feet or in nanometers\\nAnother way to check for correlation between attributes is to use Pan', 'eight in feet or in nanometers\\nAnother way to check for correlation between attributes is to use Pandas\\nscattermatrix  function which plots every numerical attribute against every other\\nnumerical attribute Since there are now 11 numerical attributes you would get 112 \\n121 plots which would not fit on a page so lets just focus on a few promising\\nattributes that seem most correlated with the median housing value  Figure 215 \\nfrom pandasplotting  import scattermatrix\\nattributes   medianhousevalue  medianincome  totalrooms \\n              housingmedianage \\nscattermatrix housingattributes  figsize12 8\\nDiscover and Visualize the Data to Gain Insights  63Figure 215 Scatter matrix\\nThe main diagonal top left to bottom right would be full of straight lines if Pandas\\nplotted each variable against itself which would not be very useful So instead Pandas\\ndisplays a histogram of each attribute other options are available see Pandas docu\\nmentation for more details\\nThe most promising attribute to predic', 'ions are available see Pandas docu\\nmentation for more details\\nThe most promising attribute to predict the median house value is the median\\nincome so lets zoom in on their correlation scatterplot  Figure 216 \\nhousingplotkindscatter  xmedianincome  ymedianhousevalue \\n             alpha01\\nThis plot reveals a few things First the correlation is indeed very strong you can\\nclearly see the upward trend and the points are not too dispersed Second the price\\ncap that we noticed earlier is clearly visible as a horizontal line at 500000 But this\\nplot reveals other less obvious straight lines a horizontal line around 450000\\nanother around 350000 perhaps one around 280000 and a few more below that\\nY ou may want to try removing the corresponding districts to prevent your algorithms\\nfrom learning to reproduce these data quirks\\n64  Chapter 2 EndtoEnd Machine Learning ProjectFigure 216 Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave ', 's median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm and you found interesting\\ncorrelations between attributes in particular with the target attribute Y ou also\\nnoticed that some attributes have a tailheavy distribution so you may want to trans\\nform them eg by computing their logarithm Of course your mileage will vary\\nconsiderably with each project but the general ideas are similar\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations For example the\\ntotal number of rooms in a district is not very useful if you dont know how many\\nhouseholds there are What you really want is the number of rooms per household\\nSimilarly the total number of bedrooms by itself is not v', ' want is the number of rooms per household\\nSimilarly the total number of bedrooms by itself is not very useful you probably\\nwant to compare it to the number of rooms And the population per household also\\nseems like an interesting attribute combination to look at Lets create these new\\nattributes\\nhousingroomsperhousehold   housingtotalrooms housinghouseholds \\nhousingbedroomsperroom   housingtotalbedrooms housingtotalrooms \\nhousingpopulationperhousehold housingpopulation housinghouseholds \\nAnd now lets look at the correlation matrix again\\n corrmatrix   housingcorr\\n corrmatrix medianhousevalue sortvalues ascending False\\nmedianhousevalue          1000000\\nDiscover and Visualize the Data to Gain Insights  65medianincome               0687160\\nroomsperhousehold         0146285\\ntotalrooms                 0135097\\nhousingmedianage          0114110\\nhouseholds                  0064506\\ntotalbedrooms              0047689\\npopulationperhousehold   0021985\\npopulation                 0026920\\nlongitude    ', '           0047689\\npopulationperhousehold   0021985\\npopulation                 0026920\\nlongitude                  0047432\\nlatitude                   0142724\\nbedroomsperroom          0259984\\nName medianhousevalue dtype float64\\nHey not bad The new bedroomsperroom  attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms Apparently\\nhouses with a lower bedroomroom ratio tend to be more expensive The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrictobviously the larger the houses the more expensive they are\\nThis round of exploration does not have to be absolutely thorough the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea\\nsonably good prototype But this is an iterative process once you get a prototype up\\nand running you can analyze its output to gain more insights and come back to this\\nexploration step\\nPrepare the Data for Machine Learning A', 'to gain more insights and come back to this\\nexploration step\\nPrepare the Data for Machine Learning Algorithms\\nIts time to prepare the data for your Machine Learning algorithms Instead of just\\ndoing this manually you should write functions to do that for several good reasons\\nThis will allow you to reproduce these transformations easily on any dataset eg\\nthe next time you get a fresh dataset\\nY ou will gradually build a library of transformation functions that you can reuse\\nin future projects\\nY ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms\\nThis will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best\\nBut first lets revert to a clean training set by copying strattrainset  once again\\nand lets separate the predictors and the labels since we dont necessarily want to apply\\nthe same transformations to the predictors and the target values note that drop  \\ncreates a c', ' apply\\nthe same transformations to the predictors and the target values note that drop  \\ncreates a copy of the data and does not affect strattrainset \\nhousing  strattrainset dropmedianhousevalue  axis1\\nhousinglabels   strattrainset medianhousevalue copy\\n66  Chapter 2 EndtoEnd Machine Learning ProjectData Cleaning\\nMost Machine Learning algorithms cannot work with missing features so lets create\\na few functions to take care of them Y ou noticed earlier that the totalbedrooms\\nattribute has some missing values so lets fix this Y ou have three options\\nGet rid of the corresponding districts\\nGet rid of the whole attribute\\nSet the values to some value zero the mean the median etc\\nY ou can accomplish these easily using DataFrames dropna  drop  and fillna\\nmethods\\nhousingdropnasubsettotalbedrooms      option 1\\nhousingdroptotalbedrooms  axis1        option 2\\nmedian  housingtotalbedrooms median   option 3\\nhousingtotalbedrooms fillnamedian inplaceTrue\\nIf you choose option 3 you should compute the me', 'ion 3\\nhousingtotalbedrooms fillnamedian inplaceTrue\\nIf you choose option 3 you should compute the median value on the training set and\\nuse it to fill the missing values in the training set but also dont forget to save the\\nmedian value that you have computed Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system and also once the system goes\\nlive to replace missing values in new data\\nScikitLearn provides a handy class to take care of missing values SimpleImputer \\nHere is how to use it First you need to create a SimpleImputer  instance specifying\\nthat you want to replace each attributes missing values with the median of that\\nattribute\\nfrom sklearnimpute  import SimpleImputer\\nimputer  SimpleImputer strategy median \\nSince the median can only be computed on numerical attributes we need to create a\\ncopy of the data without the text attribute oceanproximity \\nhousingnum   housingdropoceanproximity  axis1\\nNow you can fit the imputer  instance to ', 'anproximity \\nhousingnum   housingdropoceanproximity  axis1\\nNow you can fit the imputer  instance to the training data using the fit  method\\nimputerfithousingnum \\nThe imputer  has simply computed the median of each attribute and stored the result\\nin its statistics  instance variable Only the totalbedrooms  attribute had missing\\nvalues but we cannot be sure that there wont be any missing values in new data after\\nthe system goes live so it is safer to apply the imputer  to all the numerical attributes\\n imputerstatistics\\narray 11851  3426  29  21195  433  1164  408  35409\\nPrepare the Data for Machine Learning Algorithms  6717For more details on the design principles see  API design for machine learning software experiences from\\nthe scikitlearn project  L Buitinck G Louppe M Blondel F Pedregosa A Mller et al 2013 housingnum medianvalues\\narray 11851  3426  29  21195  433  1164  408  35409\\nNow you can use this trained imputer  to transform the training set by replacing\\nmissing values by the l', 'you can use this trained imputer  to transform the training set by replacing\\nmissing values by the learned medians\\nX  imputertransform housingnum \\nThe result is a plain NumPy array containing the transformed features If you want to\\nput it back into a Pandas DataFrame its simple\\nhousingtr   pdDataFrame X columnshousingnum columns\\nScikitLearn Design\\nScikitLearns API is remarkably well designed The main design principles  are17\\nConsistency  All objects share a consistent and simple interface\\nEstimators  Any object that can estimate some parameters based on a dataset\\nis called an estimator  eg an imputer  is an estimator The estimation itself is\\nperformed by the fit  method and it takes only a dataset as a parameter or\\ntwo for supervised learning algorithms the second dataset contains the\\nlabels Any other parameter needed to guide the estimation process is con\\nsidered a hyperparameter such as an imputer s strategy  and it must be set\\nas an instance variable generally via a constructor para', 's an imputer s strategy  and it must be set\\nas an instance variable generally via a constructor parameter\\nTransformers  Some estimators such as an imputer  can also transform a\\ndataset these are called transformers  Once again the API is quite simple the\\ntransformation is performed by the transform  method with the dataset to\\ntransform as a parameter It returns the transformed dataset This transforma\\ntion generally relies on the learned parameters as is the case for an imputer \\nAll transformers also have a convenience method called fittransform  \\nthat is equivalent to calling fit  and then transform  but sometimes\\nfittransform  is optimized and runs much faster\\nPredictors  Finally some estimators are capable of making predictions given a\\ndataset they are called predictors  For example the LinearRegression  model \\nin the previous chapter was a predictor it predicted life satisfaction given a\\ncountrys GDP per capita A predictor has a predict  method that takes a\\ndataset of new instances ', 'n a\\ncountrys GDP per capita A predictor has a predict  method that takes a\\ndataset of new instances and returns a dataset of corresponding predictions It\\nalso has a score  method that measures the quality of the predictions given\\n68  Chapter 2 EndtoEnd Machine Learning Project18Some predictors also provide methods to measure the confidence of their predictions\\n19This class is available since ScikitLearn 020 If you use an earlier version please consider upgrading or use\\nPandas Seriesfactorize  methoda test set and the corresponding labels in the case of supervised learning\\nalgorithms18\\nInspection  All the estimators hyperparameters are accessible directly via public\\ninstance variables eg imputerstrategy  and all the estimators learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix eg imputerstatistics \\nNonproliferation of classes  Datasets are represented as NumPy arrays or SciPy\\nsparse matrices instead of homemade classes Hyperparameters are just', 'sented as NumPy arrays or SciPy\\nsparse matrices instead of homemade classes Hyperparameters are just regular\\nPython strings or numbers\\nComposition  Existing building blocks are reused as much as possible For\\nexample it is easy to create a Pipeline  estimator from an arbitrary sequence of\\ntransformers followed by a final estimator as we will see\\nSensible defaults  ScikitLearn provides reasonable default values for most\\nparameters making it easy to create a baseline working system quickly\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute oceanproximity  because it is a text\\nattribute so we cannot compute its median\\n housingcat   housingoceanproximity \\n housingcat head10\\n      oceanproximity\\n17606       1H OCEAN\\n18632       1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        1H OCEAN\\n19480          INLAND\\n8879        1H OCEAN\\n13685          INLAND\\n4937        1H OCEAN\\n4861        1H OCEAN\\nMost Machine Learning algorithms prefer to work wit', 'INLAND\\n4937        1H OCEAN\\n4861        1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway so lets con\\nvert these categories from text to numbers For this we can use ScikitLearns Ordina\\nlEncoder  class19\\n from sklearnpreprocessing  import OrdinalEncoder\\n ordinalencoder   OrdinalEncoder \\nPrepare the Data for Machine Learning Algorithms  6920Before ScikitLearn 020 it could only encode integer categorical values but since 020 it can also handle\\nother types of inputs including text categorical inputs housingcatencoded   ordinalencoder fittransform housingcat \\n housingcatencoded 10\\narray0\\n       0\\n       4\\n       1\\n       0\\n       1\\n       0\\n       1\\n       0\\n       0\\nY ou can get the list of categories using the categories  instance variable It is a list\\ncontaining a 1D array of categories for each categorical attribute in this case a list\\ncontaining a single array since there is just one categorical attribute\\n ordinalencoder categories\\narray1H OCEAN INLAND ISLAND', 'since there is just one categorical attribute\\n ordinalencoder categories\\narray1H OCEAN INLAND ISLAND NEAR BAY NEAR OCEAN\\n       dtypeobject\\nOne issue with this representation is that ML algorithms will assume that two nearby\\nvalues are more similar than two distant values This may be fine in some cases eg\\nfor ordered categories such as bad  average  good  excellent but it is obviously\\nnot the case for the oceanproximity  column for example categories 0 and 4 are\\nclearly more similar than categories 0 and 1 To fix this issue a common solution is\\nto create one binary attribute per category one attribute equal to 1 when the category\\nis 1H OCEAN and 0 otherwise another attribute equal to 1 when the category is\\nINLAND and 0 otherwise and so on This is called onehot encoding  because\\nonly one attribute will be equal to 1 hot while the others will be 0 cold The new\\nattributes are sometimes called dummy  attributes ScikitLearn provides a OneHotEn\\ncoder  class to convert categorical values into', 'ed dummy  attributes ScikitLearn provides a OneHotEn\\ncoder  class to convert categorical values into onehot vectors20\\n from sklearnpreprocessing  import OneHotEncoder\\n catencoder   OneHotEncoder \\n housingcat1hot   catencoder fittransform housingcat \\n housingcat1hot\\n16512x5 sparse matrix of type class numpyfloat64\\n  with 16512 stored elements in Compressed Sparse Row format\\nNotice that the output is a SciPy sparse matrix  instead of a NumPy array This is very\\nuseful when you have categorical attributes with thousands of categories After one\\nhot encoding we get a matrix with thousands of columns and the matrix is full of\\nzeros except for a single 1 per row Using up tons of memory mostly to store zeros\\nwould be very wasteful so instead a sparse matrix only stores the location of the non\\n70  Chapter 2 EndtoEnd Machine Learning Project21See SciPys documentation for more details\\nzero elements Y ou can use it mostly like a normal 2D array21 but if you really want to\\nconvert it to a dense NumP', 'Y ou can use it mostly like a normal 2D array21 but if you really want to\\nconvert it to a dense NumPy array just call the toarray  method\\n housingcat1hot toarray\\narray1 0 0 0 0\\n       1 0 0 0 0\\n       0 0 0 0 1\\n       \\n       0 1 0 0 0\\n       1 0 0 0 0\\n       0 0 0 1 0\\nOnce again you can get the list of categories using the encoders categories\\ninstance variable\\n catencoder categories\\narray1H OCEAN INLAND ISLAND NEAR BAY NEAR OCEAN\\n       dtypeobject\\nIf a categorical attribute has a large number of possible categories\\neg country code profession species etc then onehot encod\\ning will result in a large number of input features This may slow\\ndown training and degrade performance If this happens you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories for example you could replace the\\noceanproximity  feature with the distance to the ocean similarly\\na country code could be replaced with the countrys population and\\nGDP per capita Alternatively you', 'y\\na country code could be replaced with the countrys population and\\nGDP per capita Alternatively you could replace each category\\nwith a learnable low dimensional vector called an embedding  Each\\ncategorys representation would be learned during training this is\\nan example of representation learning  see Chapter 13  and  for\\nmore details\\nCustom Transformers\\nAlthough ScikitLearn provides many useful transformers you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes Y ou will want your transformer to work seamlessly with ScikitLearn func\\ntionalities such as pipelines and since ScikitLearn relies on duck typing not inher\\nitance all you need is to create a class and implement three methods fit\\nreturning self  transform  and fittransform  Y ou can get the last one for\\nfree by simply adding TransformerMixin  as a base class Also if you add BaseEstima\\ntor as a base class and avoid args  and kargs  in your constructor you will get\\ntwo extra ', 'aseEstima\\ntor as a base class and avoid args  and kargs  in your constructor you will get\\ntwo extra methods  getparams  and setparams  that will be useful for auto\\nPrepare the Data for Machine Learning Algorithms  71matic hyperparameter tuning For example here is a small transformer class that adds\\nthe combined attributes we discussed earlier\\nfrom sklearnbase  import BaseEstimator  TransformerMixin\\nroomsix  bedroomsix  populationix  householdsix   3 4 5 6\\nclass CombinedAttributesAdder BaseEstimator  TransformerMixin \\n    def init self addbedroomsperroom   True  no args or kargs\\n        selfaddbedroomsperroom   addbedroomsperroom\\n    def fitself X yNone\\n        return self   nothing else to do\\n    def transform self X yNone\\n        roomsperhousehold   X roomsix   X householdsix \\n        populationperhousehold   X populationix   X householdsix \\n        if selfaddbedroomsperroom \\n            bedroomsperroom   X bedroomsix   X roomsix \\n            return npcX roomsperhousehold  populationp', '  bedroomsperroom   X bedroomsix   X roomsix \\n            return npcX roomsperhousehold  populationperhousehold \\n                         bedroomsperroom \\n        else\\n            return npcX roomsperhousehold  populationperhousehold \\nattradder   CombinedAttributesAdder addbedroomsperroom False\\nhousingextraattribs   attradder transform housingvalues\\nIn this example the transformer has one hyperparameter addbedroomsperroom \\nset to True  by default it is often helpful to provide sensible defaults This hyperpara\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not More generally you can add a hyperparameter\\nto gate any data preparation step that you are not 100 sure about The more you\\nautomate these data preparation steps the more combinations you can automatically\\ntry out making it much more likely that you will find a great combination and sav\\ning you a lot of time\\nFeature Scaling\\nOne of the most important transformations you', 'bination and sav\\ning you a lot of time\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling  With few exceptions Machine Learning algorithms dont perform well when\\nthe input numerical attributes have very different scales This is the case for the hous\\ning data the total number of rooms ranges from about 6 to 39320 while the median\\nincomes only range from 0 to 15 Note that scaling the target values is generally not\\nrequired\\nThere are two common ways to get all attributes to have the same scale minmax\\nscaling  and standardization \\nMinmax scaling many people call this normalization  is quite simple values are\\nshifted and rescaled so that they end up ranging from 0 to 1 We do this by subtract\\ning the min value and dividing by the max minus the min ScikitLearn provides a\\n72  Chapter 2 EndtoEnd Machine Learning Projecttransformer called MinMaxScaler  for this It has a featurerange  hyperparameter\\nthat lets you change the range if you don', 'nMaxScaler  for this It has a featurerange  hyperparameter\\nthat lets you change the range if you dont want 01 for some reason\\nStandardization is quite different first it subtracts the mean value so standardized\\nvalues always have a zero mean and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance Unlike minmax scaling standardization\\ndoes not bound values to a specific range which may be a problem for some algo\\nrithms eg neural networks often expect an input value ranging from 0 to 1 How\\never standardization is much less affected by outliers For example suppose a district\\nhad a median income equal to 100 by mistake Minmax scaling would then crush\\nall the other values from 015 down to 0015 whereas standardization would not be\\nmuch affected ScikitLearn provides a transformer called StandardScaler  for stand\\nardization\\nAs with all the transformations it is important to fit the scalers to\\nthe training data only not to the full dataset including th', 'ns it is important to fit the scalers to\\nthe training data only not to the full dataset including the test set\\nOnly then can you use them to transform the training set and the\\ntest set and new data\\nTransformation Pipelines\\nAs you can see there are many data transformation steps that need to be executed in\\nthe right order Fortunately ScikitLearn provides the Pipeline  class to help with\\nsuch sequences of transformations Here is a small pipeline for the numerical\\nattributes\\nfrom sklearnpipeline  import Pipeline\\nfrom sklearnpreprocessing  import StandardScaler\\nnumpipeline   Pipeline \\n        imputer  SimpleImputer strategy median \\n        attribsadder  CombinedAttributesAdder \\n        stdscaler  StandardScaler \\n    \\nhousingnumtr   numpipeline fittransform housingnum \\nThe Pipeline  constructor takes a list of nameestimator pairs defining a sequence of\\nsteps All but the last estimator must be transformers ie they must have a\\nfittransform  method The names can be anything you like as long as', ' transformers ie they must have a\\nfittransform  method The names can be anything you like as long as they are\\nunique and dont contain double underscores   they will come in handy later for\\nhyperparameter tuning\\nWhen you call the pipelines fit  method it calls fittransform  sequentially on\\nall transformers passing the output of each call as the parameter to the next call until\\nit reaches the final estimator for which it just calls the fit  method\\nPrepare the Data for Machine Learning Algorithms  7322Just like for pipelines the name can be anything as long as it does not contain double underscoresThe pipeline exposes the same methods as the final estimator In this example the last\\nestimator is a StandardScaler  which is a transformer so the pipeline has a trans\\nform  method that applies all the transforms to the data in sequence and of course\\nalso a fittransform  method which is the one we used\\nSo far we have handled the categorical columns and the numerical columns sepa\\nrately It would ', ' used\\nSo far we have handled the categorical columns and the numerical columns sepa\\nrately It would be more convenient to have a single transformer able to handle all col\\numns applying the appropriate transformations to each column In version 020\\nScikitLearn introduced the ColumnTransformer  for this purpose and the good news\\nis that it works great with Pandas DataFrames Lets use it to apply all the transforma\\ntions to the housing data\\nfrom sklearncompose  import ColumnTransformer\\nnumattribs   listhousingnum \\ncatattribs   oceanproximity \\nfullpipeline   ColumnTransformer \\n        num numpipeline  numattribs \\n        cat OneHotEncoder  catattribs \\n    \\nhousingprepared   fullpipeline fittransform housing\\nHere is how this works first we import the ColumnTransformer  class next we get the\\nlist of numerical column names and the list of categorical column names and we\\nconstruct a ColumnTransformer  The constructor requires a list of tuples where each\\ntuple contains a name22 a transformer and ', 'mer  The constructor requires a list of tuples where each\\ntuple contains a name22 a transformer and a list of names or indices of columns\\nthat the transformer should be applied to In this example we specify that the numer\\nical columns should be transformed using the numpipeline  that we defined earlier\\nand the categorical columns should be transformed using a OneHotEncoder  Finally\\nwe apply this ColumnTransformer  to the housing data it applies each transformer to\\nthe appropriate columns and concatenates the outputs along the second axis the\\ntransformers must return the same number of rows\\nNote that the OneHotEncoder  returns a sparse matrix while the numpipeline  returns\\na dense matrix When there is such a mix of sparse and dense matrices the Colum\\nnTransformer  estimates the density of the final matrix ie the ratio of nonzero\\ncells and it returns a sparse matrix if the density is lower than a given threshold by\\ndefault sparsethreshold03  In this example it returns a dense matrix And\\n', ' than a given threshold by\\ndefault sparsethreshold03  In this example it returns a dense matrix And\\nthats it We have a preprocessing pipeline that takes the full housing data and applies\\nthe appropriate transformations to each column\\n74  Chapter 2 EndtoEnd Machine Learning ProjectInstead of a transformer you can specify the string drop  if you\\nwant the columns to be dropped Or you can specify pass\\nthrough  if you want the columns to be left untouched By default\\nthe remaining columns ie the ones that were not listed will be\\ndropped but you can set the remainder  hyperparameter to any\\ntransformer or to passthrough  if you want these columns to be\\nhandled differently\\nIf you are using ScikitLearn 019 or earlier you can use a thirdparty library such as\\nsklearnpandas  or roll out your own custom transformer to get the same function\\nality as the ColumnTransformer  Alternatively you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs but yo', 'FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs but you\\ncannot specify different columns for each transformer they all apply to the whole\\ndata It is possible to work around this limitation using a custom transformer for col\\numn selection see the Jupyter notebook for an example\\nSelect and Train a Model\\nAt last Y ou framed the problem you got the data and explored it you sampled a\\ntraining set and a test set and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically Y ou are now ready\\nto select and train a Machine Learning model\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps things are now going to be\\nmuch simpler than you might think Lets first train a Linear Regression model like\\nwe did in the previous chapter\\nfrom sklearnlinearmodel  import LinearRegression\\nlinreg  LinearRegression \\nlinregfithousingprepared  housinglabels \\nDone', 'del  import LinearRegression\\nlinreg  LinearRegression \\nlinregfithousingprepared  housinglabels \\nDone Y ou now have a working Linear Regression model Lets try it out on a few\\ninstances from the training set\\n somedata   housingiloc5\\n somelabels   housinglabels iloc5\\n somedataprepared   fullpipeline transform somedata \\n printPredictions  linregpredictsomedataprepared \\nPredictions  2106446045  3177688069  2109564333  592189888  1897475584\\n printLabels  listsomelabels \\nLabels 2866000 3406000 1969000 463000 2545000\\nSelect and Train a Model  75It works although the predictions are not exactly accurate eg the first prediction is\\noff by close to 40 Lets measure this regression models RMSE on the whole train\\ning set using ScikitLearns meansquarederror  function\\n from sklearnmetrics  import meansquarederror\\n housingpredictions   linregpredicthousingprepared \\n linmse  meansquarederror housinglabels  housingpredictions \\n linrmse   npsqrtlinmse\\n linrmse\\n6862819819848922\\nOkay this is better than noth', ' housingpredictions \\n linrmse   npsqrtlinmse\\n linrmse\\n6862819819848922\\nOkay this is better than nothing but clearly not a great score most districts\\nmedianhousingvalues  range between 120000 and 265000 so a typical predic\\ntion error of 68628 is not very satisfying This is an example of a model underfitting\\nthe training data When this happens it can mean that the features do not provide\\nenough information to make good predictions or that the model is not powerful\\nenough As we saw in the previous chapter the main ways to fix underfitting are to\\nselect a more powerful model to feed the training algorithm with better features or\\nto reduce the constraints on the model This model is not regularized so this rules\\nout the last option Y ou could try to add more features eg the log of the popula\\ntion but first lets try a more complex model to see how it does\\nLets train a DecisionTreeRegressor  This is a powerful model capable of finding\\ncomplex nonlinear relationships in the data Decision Trees ', 's is a powerful model capable of finding\\ncomplex nonlinear relationships in the data Decision Trees are presented in more\\ndetail in Chapter 6  The code should look familiar by now\\nfrom sklearntree  import DecisionTreeRegressor\\ntreereg   DecisionTreeRegressor \\ntreereg fithousingprepared  housinglabels \\nNow that the model is trained lets evaluate it on the training set\\n housingpredictions   treereg predicthousingprepared \\n treemse   meansquarederror housinglabels  housingpredictions \\n treermse   npsqrttreemse \\n treermse\\n00\\nWait what No error at all Could this model really be absolutely perfect Of course\\nit is much more likely that the model has badly overfit the data How can you be sure\\nAs we saw earlier you dont want to touch the test set until you are ready to launch a\\nmodel you are confident about so you need to use part of the training set for train\\ning and part for model validation\\nBetter Evaluation Using CrossValidation\\nOne way to evaluate the Decision Tree model would be to use th', 'tter Evaluation Using CrossValidation\\nOne way to evaluate the Decision Tree model would be to use the traintestsplit\\nfunction to split the training set into a smaller training set and a validation set then\\n76  Chapter 2 EndtoEnd Machine Learning Projecttrain your models against the smaller training set and evaluate them against the vali\\ndation set Its a bit of work but nothing too difficult and it would work fairly well\\nA great alternative is to use ScikitLearns Kfold crossvalidation  feature The follow\\ning code randomly splits the training set into 10 distinct subsets called folds  then it\\ntrains and evaluates the Decision Tree model 10 times picking a different fold for\\nevaluation every time and training on the other 9 folds The result is an array con\\ntaining the 10 evaluation scores\\nfrom sklearnmodelselection  import crossvalscore\\nscores  crossvalscore treereg  housingprepared  housinglabels \\n                         scoringnegmeansquarederror  cv10\\ntreermsescores   npsqrtscores\\nSci', 'glabels \\n                         scoringnegmeansquarederror  cv10\\ntreermsescores   npsqrtscores\\nScikitLearns crossvalidation features expect a utility function\\ngreater is better rather than a cost function lower is better so\\nthe scoring function is actually the opposite of the MSE ie a neg\\native value which is why the preceding code computes scores\\nbefore calculating the square root\\nLets look at the results\\n def displayscores scores\\n     printScores  scores\\n     printMean scoresmean\\n     printStandard deviation  scoresstd\\n\\n displayscores treermsescores \\nScores 7019433680785 6685516363941 7243258244769 7075873896782\\n 7111588230639 7558514172901 7026286139133 702736325285\\n 7536687952553 7123165726027\\nMean 7140768766037929\\nStandard deviation 24394345041191004\\nNow the Decision Tree doesnt look as good as it did earlier In fact it seems to per\\nform worse than the Linear Regression model Notice that crossvalidation allows\\nyou to get not only an estimate of the performance of your model but ', 'ce that crossvalidation allows\\nyou to get not only an estimate of the performance of your model but also a measure\\nof how precise this estimate is ie its standard deviation The Decision Tree has a\\nscore of approximately 71407 generally 2439 Y ou would not have this information\\nif you just used one validation set But crossvalidation comes at the cost of training\\nthe model several times so it is not always possible\\nLets compute the same scores for the Linear Regression model just to be sure\\n linscores   crossvalscore linreg housingprepared  housinglabels \\n                              scoringnegmeansquarederror  cv10\\n\\n linrmsescores   npsqrtlinscores \\n displayscores linrmsescores \\nSelect and Train a Model  77Scores 6678273843989 66960118071   7034795244419 7473957052552\\n 6803113388938 7119384183426 6496963056405 6828161137997\\n 7155291566558 6766510082067\\nMean 6905246136345083\\nStandard deviation 2731674001798348\\nThats right the Decision Tree model is overfitting so badly that it performs ', 'ation 2731674001798348\\nThats right the Decision Tree model is overfitting so badly that it performs worse\\nthan the Linear Regression model\\nLets try one last model now the RandomForestRegressor  As we will see in Chap\\nter 7  Random Forests work by training many Decision Trees on random subsets of\\nthe features then averaging out their predictions Building a model on top of many\\nother models is called Ensemble Learning  and it is often a great way to push ML algo\\nrithms even further We will skip most of the code since it is essentially the same as\\nfor the other models\\n from sklearnensemble  import RandomForestRegressor\\n forestreg   RandomForestRegressor \\n forestreg fithousingprepared  housinglabels \\n \\n forestrmse\\n18603515021376355\\n displayscores forestrmsescores \\nScores 4951980364233 474619115823  5002902762854 5232528068953\\n 4930839426421 5344637892622 486348036574  4758573832311\\n 5349010699751 500215852922 \\nMean 50182303100336096\\nStandard deviation 20970810550985693\\nWow this is much bet', '99751 500215852922 \\nMean 50182303100336096\\nStandard deviation 20970810550985693\\nWow this is much better Random Forests look very promising However note that\\nthe score on the training set is still much lower than on the validation sets meaning\\nthat the model is still overfitting the training set Possible solutions for overfitting are\\nto simplify the model constrain it ie regularize it or get a lot more training data\\nHowever before you dive much deeper in Random Forests you should try out many\\nother models from various categories of Machine Learning algorithms several Sup\\nport Vector Machines with different kernels possibly a neural network etc without\\nspending too much time tweaking the hyperparameters The goal is to shortlist a few\\ntwo to five promising models\\n78  Chapter 2 EndtoEnd Machine Learning ProjectY ou should save every model you experiment with so you can\\ncome back easily to any model you want Make sure you save both\\nthe hyperparameters and the trained parameters as well as t', 'y model you want Make sure you save both\\nthe hyperparameters and the trained parameters as well as the\\ncrossvalidation scores and perhaps the actual predictions as well\\nThis will allow you to easily compare scores across model types\\nand compare the types of errors they make Y ou can easily save\\nScikitLearn models by using Pythons pickle  module or using\\nsklearnexternalsjoblib  which is more efficient at serializing \\nlarge NumPy arrays\\nfrom sklearnexternals  import joblib\\njoblibdumpmymodel  mymodelpkl \\n and later\\nmymodelloaded   joblibloadmymodelpkl \\nFineTune Your Model\\nLets assume that you now have a shortlist of promising models Y ou now need to\\nfinetune them Lets look at a few ways you can do that\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually until you\\nfind a great combination of hyperparameter values This would be very tedious work\\nand you may not have time to explore many combinations\\nInstead you should get ScikitLearns GridSearchCV  to search ', ' have time to explore many combinations\\nInstead you should get ScikitLearns GridSearchCV  to search for you All you need to\\ndo is tell it which hyperparameters you want it to experiment with and what values to\\ntry out and it will evaluate all the possible combinations of hyperparameter values\\nusing crossvalidation For example the following code searches for the best combi\\nnation of hyperparameter values for the RandomForestRegressor \\nfrom sklearnmodelselection  import GridSearchCV\\nparamgrid   \\n    nestimators  3 10 30 maxfeatures  2 4 6 8\\n    bootstrap  False nestimators  3 10 maxfeatures  2 3 4\\n  \\nforestreg   RandomForestRegressor \\ngridsearch   GridSearchCV forestreg  paramgrid  cv5\\n                           scoringnegmeansquarederror \\n                           returntrainscore True\\ngridsearch fithousingprepared  housinglabels \\nFineTune Your Model  79When you have no idea what value a hyperparameter should have\\na simple approach is to try out consecutive powers of 10 or a\\nsmaller nu', ' hyperparameter should have\\na simple approach is to try out consecutive powers of 10 or a\\nsmaller number if you want a more finegrained search as shown\\nin this example with the nestimators  hyperparameter\\nThis paramgrid  tells ScikitLearn to first evaluate all 3  4  12 combinations of\\nnestimators  and maxfeatures  hyperparameter values specified in the first dict\\ndont worry about what these hyperparameters mean for now they will be explained\\nin Chapter 7  then try all 2  3  6 combinations of hyperparameter values in the\\nsecond dict  but this time with the bootstrap  hyperparameter set to False  instead of\\nTrue  which is the default value for this hyperparameter\\nAll in all the grid search will explore 12  6  18 combinations of RandomForestRe\\ngressor  hyperparameter values and it will train each model five times since we are\\nusing fivefold cross validation In other words all in all there will be 18  5  90\\nrounds of training It may take quite a long time but when it is done you can get th', 'ill be 18  5  90\\nrounds of training It may take quite a long time but when it is done you can get the\\nbest combination of parameters like this\\n gridsearch bestparams\\nmaxfeatures 8 nestimators 30\\nSince 8 and 30 are the maximum values that were evaluated you\\nshould probably try searching again with higher values since the\\nscore may continue to improve\\nY ou can also get the best estimator directly\\n gridsearch bestestimator\\nRandomForestRegressorbootstrapTrue criterionmse maxdepthNone\\n           maxfeatures8 maxleafnodesNone minimpuritydecrease00\\n           minimpuritysplitNone minsamplesleaf1\\n           minsamplessplit2 minweightfractionleaf00\\n           nestimators30 njobsNone oobscoreFalse randomstateNone\\n           verbose0 warmstartFalse\\nIf GridSearchCV  is initialized with refitTrue  which is the\\ndefault then once it finds the best estimator using cross\\nvalidation it retrains it on the whole training set This is usually a\\ngood idea since feeding it more data will likely improve its pe', 'whole training set This is usually a\\ngood idea since feeding it more data will likely improve its perfor\\nmance\\nAnd of course the evaluation scores are also available\\n cvres  gridsearch cvresults\\n for meanscore  params in zipcvresmeantestscore  cvresparams \\n80  Chapter 2 EndtoEnd Machine Learning Project     printnpsqrtmeanscore  params\\n\\n6366905791727153 maxfeatures 2 nestimators 3\\n5562716171305252 maxfeatures 2 nestimators 10\\n5338457867637289 maxfeatures 2 nestimators 30\\n6096599185930139 maxfeatures 4 nestimators 3\\n5274098248528835 maxfeatures 4 nestimators 10\\n50377344409590376 maxfeatures 4 nestimators 30\\n5866384733372485 maxfeatures 6 nestimators 3\\n5200615355973719 maxfeatures 6 nestimators 10\\n50146465964159885 maxfeatures 6 nestimators 30\\n5786925504027614 maxfeatures 8 nestimators 3\\n5171109443660957 maxfeatures 8 nestimators 10\\n4968225345942335 maxfeatures 8 nestimators 30\\n62895088889905004 bootstrap False maxfeatures 2 nestimators 3\\n5465814484390074 bootstrap False maxfeatures 2 ne', '905004 bootstrap False maxfeatures 2 nestimators 3\\n5465814484390074 bootstrap False maxfeatures 2 nestimators 10\\n59470399594730654 bootstrap False maxfeatures 3 nestimators 3\\n5272501091081235 bootstrap False maxfeatures 3 nestimators 10\\n57490612956065226 bootstrap False maxfeatures 4 nestimators 3\\n5100951445842374 bootstrap False maxfeatures 4 nestimators 10\\nIn this example we obtain the best solution by setting the maxfeatures  hyperpara\\nmeter to 8 and the nestimators  hyperparameter to 30 The RMSE score for this\\ncombination is 49682 which is slightly better than the score you got earlier using the\\ndefault hyperparameter values which was 50182 Congratulations you have suc\\ncessfully finetuned your best model\\nDont forget that you can treat some of the data preparation steps as\\nhyperparameters For example the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\neg using the addbedroomsperroom  hyperparameter of your\\nCombinedAttributesAdder  tran', 'not sure about\\neg using the addbedroomsperroom  hyperparameter of your\\nCombinedAttributesAdder  transformer It may similarly be used\\nto automatically find the best way to handle outliers missing fea\\ntures feature selection and more\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations\\nlike in the previous example but when the hyperparameter search space  is large it is\\noften preferable to use RandomizedSearchCV  instead This class can be used in much\\nthe same way as the GridSearchCV  class but instead of trying out all possible combi\\nnations it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration This approach has two main bene\\nfits\\nFineTune Your Model  81If you let the randomized search run for say 1000 iterations this approach will\\nexplore 1000 different values for each hyperparameter instead of just a few val\\nues per hyperparameter with the grid search approach\\nY ou have', 'erparameter instead of just a few val\\nues per hyperparameter with the grid search approach\\nY ou have more control over the computing budget you want to allocate to hyper\\nparameter search simply by setting the number of iterations\\nEnsemble Methods\\nAnother way to finetune your system is to try to combine the models that perform\\nbest The group or ensemble will often perform better than the best individual\\nmodel just like Random Forests perform better than the individual Decision Trees\\nthey rely on especially if the individual models make very different types of errors\\nWe will cover this topic in more detail in Chapter 7 \\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models For\\nexample the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions\\n featureimportances   gridsearch bestestimator featureimportances\\n featureimportances\\narray733442355e02 629090705e02 411437985e', 'earch bestestimator featureimportances\\n featureimportances\\narray733442355e02 629090705e02 411437985e02 146726854e02\\n       141064835e02 148742809e02 142575993e02 366158981e01\\n       564191792e02 108792957e01 533510773e02 103114883e02\\n       164780994e01 602803867e05 196041560e03 285647464e03\\nLets display these importance scores next to their corresponding attribute names\\n extraattribs   roomsperhhold  popperhhold  bedroomsperroom \\n catencoder   fullpipeline namedtransformers cat\\n catonehotattribs   listcatencoder categories 0\\n attributes   numattribs   extraattribs   catonehotattribs\\n sortedzipfeatureimportances  attributes  reverseTrue\\n03661589806181342 medianincome\\n 01647809935615905 INLAND\\n 010879295677551573 popperhhold\\n 007334423551601242 longitude\\n 00629090704826203 latitude\\n 005641917918195401 roomsperhhold\\n 005335107734767581 bedroomsperroom\\n 0041143798478729635 housingmedianage\\n 0014874280890402767 population\\n 0014672685420543237 totalrooms\\n 0014257599323407807 households\\n 001', ' 0014874280890402767 population\\n 0014672685420543237 totalrooms\\n 0014257599323407807 households\\n 0014106483453584102 totalbedrooms\\n 0010311488326303787 1H OCEAN\\n 0002856474637320158 NEAR OCEAN\\n82  Chapter 2 EndtoEnd Machine Learning Project 000196041559947807 NEAR BAY\\n 6028038672736599e05 ISLAND\\nWith this information you may want to try dropping some of the less useful features\\neg apparently only one oceanproximity  category is really useful so you could try\\ndropping the others\\nY ou should also look at the specific errors that your system makes then try to under\\nstand why it makes them and what could fix the problem adding extra features or on\\nthe contrary getting rid of uninformative ones cleaning up outliers etc\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while you eventually have a system that performs\\nsufficiently well Now is the time to evaluate the final model on the test set There is\\nnothing special about this process just get the predictors and the lab', 'odel on the test set There is\\nnothing special about this process just get the predictors and the labels from your\\ntest set run your fullpipeline  to transform the data call transform  not\\nfittransform  you do not want to fit the test set and evaluate the final model\\non the test set\\nfinalmodel   gridsearch bestestimator\\nXtest  strattestset dropmedianhousevalue  axis1\\nytest  strattestset medianhousevalue copy\\nXtestprepared   fullpipeline transform Xtest\\nfinalpredictions   finalmodel predictXtestprepared \\nfinalmse   meansquarederror ytest finalpredictions \\nfinalrmse   npsqrtfinalmse      evaluates to 477302\\nIn some cases such a point estimate of the generalization error will not be quite\\nenough to convince you to launch what if it is just 01 better than the model cur\\nrently in production Y ou might want to have an idea of how precise this estimate is\\nFor this you can compute a 95 confidence  interval  for the generalization error using\\nscipystatstinterval \\n from scipy import stats\\n confid', '  interval  for the generalization error using\\nscipystatstinterval \\n from scipy import stats\\n confidence   095\\n squarederrors   finalpredictions   ytest  2\\n npsqrtstatstinterval confidence  lensquarederrors   1\\n                          locsquarederrors mean\\n                          scalestatssemsquarederrors \\n\\narray4568510470776 4969125001878\\nThe performance will usually be slightly worse than what you measured using cross\\nvalidation if you did a lot of hyperparameter tuning because your system ends up\\nfinetuned to perform well on the validation data and will likely not perform as well\\nFineTune Your Model  83on unknown datasets It is not the case in this example but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set the improvements would be unlikely to generalize to new data\\nNow comes the project prelaunch phase you need to present your solution high\\nlighting what you have learned what worked and what did not w', 'you need to present your solution high\\nlighting what you have learned what worked and what did not what assumptions\\nwere made and what your systems limitations are document everything and create\\nnice presentations with clear visualizations and easytoremember statements eg\\nthe median income is the number one predictor of housing prices In this Califor\\nnia housing example the final performance of the system is not better than the\\nexperts  but it may still be a good idea to launch it especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks\\nLaunch Monitor and Maintain Your System\\nPerfect you got approval to launch Y ou need to get your solution ready for produc\\ntion in particular by plugging the production input data sources into your system\\nand writing tests\\nY ou also need to write monitoring code to check your systems live performance at\\nregular intervals and trigger alerts when it drops This is important to catch not only\\nsudden bre', 'at\\nregular intervals and trigger alerts when it drops This is important to catch not only\\nsudden breakage but also performance degradation This is quite common because\\nmodels tend to rot as data evolves over time unless the models are regularly trained\\non fresh data\\nEvaluating your systems performance will require sampling the systems predictions\\nand evaluating them This will generally require a human analysis These analysts\\nmay be field experts or workers on a crowdsourcing platform such as Amazon\\nMechanical Turk or CrowdFlower Either way you need to plug the human evalua\\ntion pipeline into your system\\nY ou should also make sure you evaluate the systems input data quality Sometimes\\nperformance will degrade slightly because of a poor quality signal eg a malfunc\\ntioning sensor sending random values or another teams output becoming stale but\\nit may take a while before your systems performance degrades enough to trigger an\\nalert If you monitor your systems inputs you may catch this earlie', 'nce degrades enough to trigger an\\nalert If you monitor your systems inputs you may catch this earlier Monitoring the\\ninputs is particularly important for online learning systems\\nFinally you will generally want to train your models on a regular basis using fresh\\ndata Y ou should automate this process as much as possible If you dont you are very\\nlikely to refresh your model only every six months at best and your systems perfor\\nmance may fluctuate severely over time If your system is an online learning system\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state\\n84  Chapter 2 EndtoEnd Machine Learning ProjectTry It Out\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like and showed you some of the tools you can use to train a great system As\\nyou can see much of the work is in the data preparation step building monitoring\\ntools setting up human evaluation pipelines and automat', 'he data preparation step building monitoring\\ntools setting up human evaluation pipelines and automating regular model training\\nThe Machine Learning algorithms are also important of course but it is probably\\npreferable to be comfortable with the overall process and know three or four algo\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process\\nSo if you have not already done so now is a good time to pick up a laptop select a\\ndataset that you are interested in and try to go through the whole process from A to\\nZ A good place to start is on a competition website such as httpkagglecom  you\\nwill have a dataset to play with a clear goal and people to share the experience with\\nExercises\\nUsing this chapters housing dataset\\n1Try a Support Vector Machine regressor  sklearnsvmSVR  with various hyper\\nparameters such as kernellinear  with various values for the C hyperpara\\nmeter or kernelrbf  with various values for the C and gamma\\nhype', ' various values for the C hyperpara\\nmeter or kernelrbf  with various values for the C and gamma\\nhyperparameters Dont worry about what these hyperparameters mean for now\\nHow does the best SVR predictor perform\\n2Try replacing GridSearchCV  with RandomizedSearchCV \\n3Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes\\n4Try creating a single pipeline that does the full data preparation plus the final\\nprediction\\n5Automatically explore some preparation options using GridSearchCV \\nSolutions to these exercises are available in the online Jupyter notebooks at https\\ngithubcomageronhandsonml2 \\nTry It Out  851By default ScikitLearn caches downloaded datasets in a directory called HOMEscikitlearndata \\nCHAPTER 3\\nClassification\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following wi', 'ke advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 3 in the final\\nrelease of the book\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression predicting values and classification predicting classes In Chapter 2  we\\nexplored a regression task predicting housing values using various algorithms such\\nas Linear Regression Decision Trees and Random Forests which will be explained\\nin further detail in later chapters Now we will turn our attention to classification\\nsystems\\nMNIST\\nIn this chapter we will be using the MNIST dataset which is a set of 70000 small\\nimages of digits handwritten by high school students and employees of the US Cen\\nsus Bureau Each image is labeled with the digit it represents This set has been stud\\nied so much that it is often called the Hello World of Machine Learning whenever\\npeople come up with a new classification algorithm they are curious to see how it\\nwill perform on MN', 'people come up with a new classification algorithm they are curious to see how it\\nwill perform on MNIST Whenever someone learns Machine Learning sooner or\\nlater they tackle MNIST\\nScikitLearn provides many helper functions to download popular datasets MNIST is\\none of them The following code fetches the MNIST dataset1\\n87 from sklearndatasets  import fetchopenml\\n mnist  fetchopenml mnist784  version1\\n mnistkeys\\ndictkeysdata target featurenames DESCR details\\n           categories url\\nDatasets loaded by ScikitLearn generally have a similar dictionary structure includ\\ning\\nA DESCR  key describing the dataset\\nA data  key containing an array with one row per instance and one column per\\nfeature\\nA target  key containing an array with the labels\\nLets look at these arrays\\n X y  mnistdata mnisttarget \\n Xshape\\n70000 784\\n yshape\\n70000\\nThere are 70000 images and each image has 784 features This is because each image\\nis 2828 pixels and each feature simply represents one pixels intensity from 0\\nwhite to ', 'e each image\\nis 2828 pixels and each feature simply represents one pixels intensity from 0\\nwhite to 255 black Lets take a peek at one digit from the dataset All you need to\\ndo is grab an instances feature vector reshape it to a 2828 array and display it using\\nMatplotlibs imshow  function\\nimport matplotlib  as mpl\\nimport matplotlibpyplot  as plt\\nsomedigit   X0\\nsomedigitimage   somedigit reshape28 28\\npltimshowsomedigitimage  cmap  mplcmbinary interpolation nearest \\npltaxisoff\\npltshow\\nThis looks like a 5 and indeed thats what the label tells us\\n88  Chapter 3 Classification y0\\n5\\nNote that the label is a string We prefer numbers so lets cast y to integers\\n y  yastypenpuint8\\nFigure 31  shows a few more images from the MNIST dataset to give you a feel for\\nthe complexity of the classification task\\nFigure 31 A few digits from the MNIST dataset\\nBut wait Y ou should always create a test set and set it aside before inspecting the data\\nclosely The MNIST dataset is actually already split into a trai', ' it aside before inspecting the data\\nclosely The MNIST dataset is actually already split into a training set the first 60000\\nimages and a test set the last 10000 images\\nXtrain Xtest ytrain ytest  X60000 X60000 y60000 y60000\\nThe training set is already shuffled for us which is good as this guarantees that all\\ncrossvalidation folds will be similar you dont want one fold to be missing some dig\\nits Moreover some learning algorithms are sensitive to the order of the training\\nMNIST  892Shuffling may be a bad idea in some contextsfor example if you are working on time series data such as\\nstock market prices or weather conditions We will explore this in the next chapters\\ninstances and they perform poorly if they get many similar instances in a row Shuf\\nfling the dataset ensures that this wont happen2\\nTraining a Binary Classifier\\nLets simplify the problem for now and only try to identify one digitfor example\\nthe number 5 This 5detector will be an example of a binary classifier  capable of\\ndisti', 'tfor example\\nthe number 5 This 5detector will be an example of a binary classifier  capable of\\ndistinguishing between just two classes 5 and not5 Lets create the target vectors for\\nthis classification task\\nytrain5   ytrain  5   True for all 5s False for all other digits\\nytest5   ytest  5\\nOkay now lets pick a classifier and train it A good place to start is with a Stochastic\\nGradient Descent  SGD classifier using ScikitLearns SGDClassifier  class This clas\\nsifier has the advantage of being capable of handling very large datasets efficiently\\nThis is in part because SGD deals with training instances independently one at a time\\nwhich also makes SGD well suited for online learning  as we will see later Lets create\\nan SGDClassifier  and train it on the whole training set\\nfrom sklearnlinearmodel  import SGDClassifier\\nsgdclf  SGDClassifier randomstate 42\\nsgdclffitXtrain ytrain5 \\nThe SGDClassifier  relies on randomness during training hence\\nthe name stochastic If you want reproducible results y', 'r  relies on randomness during training hence\\nthe name stochastic If you want reproducible results you\\nshould set the randomstate  parameter\\nNow you can use it to detect images of the number 5\\n sgdclfpredictsomedigit \\narray True\\nThe classifier guesses that this image represents a 5  True  Looks like it guessed right\\nin this particular case Now lets evaluate this models performance\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor so we\\nwill spend a large part of this chapter on this topic There are many performance\\n90  Chapter 3 Classificationmeasures available so grab another coffee and get ready to learn many new concepts\\nand acronyms\\nMeasuring Accuracy Using CrossValidation\\nA good way to evaluate a model is to use crossvalidation just as you did in Chap\\nter 2 \\nImplementing CrossValidation\\nOccasionally you will need more control over the crossvalidation process than what\\nScikitLearn provides offtheshelf In these cases you can imp', 'er the crossvalidation process than what\\nScikitLearn provides offtheshelf In these cases you can implement cross\\nvalidation yourself it is actually fairly straightforward The following code does\\nroughly the same thing as ScikitLearns crossvalscore  function and prints the \\nsame result\\nfrom sklearnmodelselection  import StratifiedKFold\\nfrom sklearnbase  import clone\\nskfolds  StratifiedKFold nsplits 3 randomstate 42\\nfor trainindex  testindex  in skfoldssplitXtrain ytrain5 \\n    cloneclf   clonesgdclf\\n    Xtrainfolds   Xtraintrainindex \\n    ytrainfolds   ytrain5 trainindex \\n    Xtestfold   Xtraintestindex \\n    ytestfold   ytrain5 testindex \\n    cloneclf fitXtrainfolds  ytrainfolds \\n    ypred  cloneclf predictXtestfold \\n    ncorrect   sumypred  ytestfold \\n    printncorrect   lenypred   prints 09502 096565 and 096495\\nThe StratifiedKFold  class performs stratified sampling as explained in Chapter 2 \\nto produce folds that contain a representative ratio of each class At each iteration the\\ncode ', 'er 2 \\nto produce folds that contain a representative ratio of each class At each iteration the\\ncode creates a clone of the classifier trains that clone on the training folds and makes\\npredictions on the test fold Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions\\nLets use the crossvalscore  function to evaluate your SGDClassifier  model\\nusing Kfold crossvalidation with three folds Remember that Kfold cross\\nvalidation means splitting the training set into Kfolds in this case three then mak\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds see Chapter 2 \\nPerformance Measures  91 from sklearnmodelselection  import crossvalscore\\n crossvalscore sgdclf Xtrain ytrain5  cv3 scoringaccuracy \\narray096355 093795 095615\\nWow Above 93 accuracy  ratio of correct predictions on all crossvalidation folds \\nThis looks amazing doesnt it Well before you get too excited lets look at a very\\ndumb classifier that just classi', 'azing doesnt it Well before you get too excited lets look at a very\\ndumb classifier that just classifies every single image in the not5 class\\nfrom sklearnbase  import BaseEstimator\\nclass Never5Classifier BaseEstimator \\n    def fitself X yNone\\n        pass\\n    def predictself X\\n        return npzeroslenX 1 dtypebool\\nCan you guess this models accuracy Lets find out\\n never5clf   Never5Classifier \\n crossvalscore never5clf  Xtrain ytrain5  cv3 scoringaccuracy \\narray091125 090855 090915\\nThats right it has over 90 accuracy This is simply because only about 10 of the\\nimages are 5s so if you always guess that an image is not a 5 you will be right about\\n90 of the time Beats Nostradamus\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers especially when you are dealing with skewed datasets  ie when some\\nclasses are much more frequent than others\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu\\nsion', 'on Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu\\nsion matrix  The general idea is to count the number of times instances of class A are\\nclassified as class B For example to know the number of times the classifier confused\\nimages of 5s with 3s you would look in the 5th row and 3rd column of the confusion\\nmatrix\\nTo compute the confusion matrix you first need to have a set of predictions so they\\ncan be compared to the actual targets Y ou could make predictions on the test set but\\nlets keep it untouched for now remember that you want to use the test set only at the\\nvery end of your project once you have a classifier that you are ready to launch\\nInstead you can use the crossvalpredict  function\\nfrom sklearnmodelselection  import crossvalpredict\\nytrainpred   crossvalpredict sgdclf Xtrain ytrain5  cv3\\nJust like the crossvalscore  function crossvalpredict  performs Kfold\\ncrossvalidation but instead of returning the evaluation scores it returns the', 'redict  performs Kfold\\ncrossvalidation but instead of returning the evaluation scores it returns the predic\\n92  Chapter 3 Classificationtions made on each test fold This means that you get a clean prediction for each\\ninstance in the training set clean meaning that the prediction is made by a model\\nthat never saw the data during training\\nNow you are ready to get the confusion matrix using the confusionmatrix  func\\ntion Just pass it the target classes  ytrain5  and the predicted classes\\nytrainpred \\n from sklearnmetrics  import confusionmatrix\\n confusionmatrix ytrain5  ytrainpred \\narray53057  1522\\n        1325  4096\\nEach row in a confusion matrix represents an actual class  while each column repre\\nsents a predicted class  The first row of this matrix considers non5 images the nega\\ntive class  53057 of them were correctly classified as non5s they are called true\\nnegatives  while the remaining 1522 were wrongly classified as 5s  false positives \\nThe second row considers the images of 5s the', 'g 1522 were wrongly classified as 5s  false positives \\nThe second row considers the images of 5s the positive class  1325 were wrongly\\nclassified as non5s  false negatives  while the remaining 4096 were correctly classi\\nfied as 5s  true positives  A perfect classifier would have only true positives and true\\nnegatives so its confusion matrix would have nonzero values only on its main diago\\nnal top left to bottom right\\n ytrainperfectpredictions   ytrain5    pretend we reached perfection\\n confusionmatrix ytrain5  ytrainperfectpredictions \\narray54579     0\\n           0  5421\\nThe confusion matrix gives you a lot of information but sometimes you may prefer a\\nmore concise metric An interesting one to look at is the accuracy of the positive pre\\ndictions this is called the precision  of the classifier  Equation 31 \\nEquation 31 Precision\\nprecision TP\\nTPFP\\nTP is the number of true positives and FP is the number of false positives\\nA trivial way to have perfect precision is to make one single posit', 'is the number of false positives\\nA trivial way to have perfect precision is to make one single positive prediction and\\nensure it is correct precision  11  100 This would not be very useful since the\\nclassifier would ignore all but one positive instance So precision is typically used\\nalong with another metric named recall  also called sensitivity  or true positive rate\\nPerformance Measures  93TPR  this is the ratio of positive instances that are correctly detected by the classifier\\nEquation 32 \\nEquation 32 Recall\\nrecall TP\\nTPFN\\nFN is of course the number of false negatives\\nIf you are confused about the confusion matrix Figure 32  may help\\nFigure 32 An illustrated confusion matrix\\nPrecision and Recall\\nScikitLearn provides several functions to compute classifier metrics including preci\\nsion and recall\\n from sklearnmetrics  import precisionscore  recallscore\\n precisionscore ytrain5  ytrainpred    4096  4096  1522\\n07290850836596654\\n recallscore ytrain5  ytrainpred    4096  4096  1325\\n075558', 'ed    4096  4096  1522\\n07290850836596654\\n recallscore ytrain5  ytrainpred    4096  4096  1325\\n07555801512636044\\nNow your 5detector does not look as shiny as it did when you looked at its accuracy\\nWhen it claims an image represents a 5 it is correct only 729 of the time More\\nover it only detects 756 of the 5s\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore  in particular if you need a simple way to compare two classifiers The F1 score is \\nthe harmonic mean  of precision and recall  Equation 33  Whereas the regular mean\\n94  Chapter 3 Classificationtreats all values equally the harmonic mean gives much more weight to low values\\nAs a result the classifier will only get a high F1 score if both recall and precision are\\nhigh\\nEquation 33 F1\\nF12\\n1\\nprecision1\\nrecall 2 precision  recall\\nprecision  recallTP\\nTPFNFP\\n2\\nTo compute the F1 score simply call the f1score  function\\n from sklearnmetrics  import f1score\\n f1score ytrain5  ytrainpred \\n07420962043', 'the f1score  function\\n from sklearnmetrics  import f1score\\n f1score ytrain5  ytrainpred \\n07420962043663375\\nThe F1 score favors classifiers that have similar precision and recall This is not always\\nwhat you want in some contexts you mostly care about precision and in other con\\ntexts you really care about recall For example if you trained a classifier to detect vid\\neos that are safe for kids you would probably prefer a classifier that rejects many\\ngood videos low recall but keeps only safe ones high precision rather than a clas\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct in such cases you may even want to add a human pipeline to check the clas\\nsifiers video selection On the other hand suppose you train a classifier to detect\\nshoplifters on surveillance images it is probably fine if your classifier has only 30\\nprecision as long as it has 99 recall sure the security guards will get a few false\\nalerts but almost all shoplifters will get caug', 'recall sure the security guards will get a few false\\nalerts but almost all shoplifters will get caught\\nUnfortunately you cant have it both ways increasing precision reduces recall and\\nvice versa This is called the precisionrecall tradeoff \\nPrecisionRecall Tradeoff\\nTo understand this tradeoff lets look at how the SGDClassifier  makes its classifica\\ntion decisions For each instance it computes a score based on a decision function  \\nand if that score is greater than a threshold it assigns the instance to the positive\\nclass or else it assigns it to the negative class Figure 33  shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right Suppose the deci\\nsion threshold  is positioned at the central arrow between the two 5s you will find 4\\ntrue positives actual 5s on the right of that threshold and one false positive actually\\na 6 Therefore with that threshold the precision is 80 4 out of 5 But out of 6\\nactual 5s the classifier only detects 4 so the recal', 'old the precision is 80 4 out of 5 But out of 6\\nactual 5s the classifier only detects 4 so the recall is 67 4 out of 6 Now if you\\nraise the threshold move it to the arrow on the right the false positive the 6\\nbecomes a true negative thereby increasing precision up to 100 in this case but\\none true positive becomes a false negative decreasing recall down to 50 Conversely\\nlowering the threshold increases recall and reduces precision\\nPerformance Measures  95Figure 33 Decision threshold and precisionrecall tradeoff\\nScikitLearn does not let you set the threshold directly but it does give you access to\\nthe decision scores that it uses to make predictions Instead of calling the classifiers\\npredict  method you can call its decisionfunction  method which returns a\\nscore for each instance and then make predictions based on those scores using any\\nthreshold you want\\n yscores   sgdclfdecisionfunction somedigit \\n yscores\\narray241253175101\\n threshold   0\\n ysomedigitpred   yscores   threshold \\narray Tr', 'somedigit \\n yscores\\narray241253175101\\n threshold   0\\n ysomedigitpred   yscores   threshold \\narray True\\nThe SGDClassifier  uses a threshold equal to 0 so the previous code returns the same\\nresult as the predict  method ie True  Lets raise the threshold\\n threshold   8000\\n ysomedigitpred   yscores   threshold \\n ysomedigitpred\\narrayFalse\\nThis confirms that raising the threshold decreases recall The image actually repre\\nsents a 5 and the classifier detects it when the threshold is 0 but it misses it when the\\nthreshold is increased to 8000\\nNow how do you decide which threshold to use For this you will first need to get the\\nscores of all instances in the training set using the crossvalpredict  function\\nagain but this time specifying that you want it to return decision scores instead of\\npredictions\\nyscores   crossvalpredict sgdclf Xtrain ytrain5  cv3\\n                             methoddecisionfunction \\nNow with these scores you can compute precision and recall for all possible thresh\\nolds usin', 'nction \\nNow with these scores you can compute precision and recall for all possible thresh\\nolds using the precisionrecallcurve  function\\n96  Chapter 3 Classificationfrom sklearnmetrics  import precisionrecallcurve\\nprecisions  recalls thresholds   precisionrecallcurve ytrain5  yscores \\nFinally you can plot precision and recall as functions of the threshold value using\\nMatplotlib  Figure 34 \\ndef plotprecisionrecallvsthreshold precisions  recalls thresholds \\n    pltplotthresholds  precisions 1 b labelPrecision \\n    pltplotthresholds  recalls1 g labelRecall \\n      highlight the threshold add the legend axis label and grid\\nplotprecisionrecallvsthreshold precisions  recalls thresholds \\npltshow\\nFigure 34 Precision and recall versus the decision threshold\\nY ou may wonder why the precision curve is bumpier than the recall\\ncurve in Figure 34  The reason is that precision may sometimes go\\ndown when you raise the threshold although in general it will go\\nup To understand why look back at Figure 33 ', ' you raise the threshold although in general it will go\\nup To understand why look back at Figure 33  and notice what\\nhappens when you start from the central threshold and move it just\\none digit to the right precision goes from 45 80 down to 34\\n75 On the other hand recall can only go down when the thres\\nhold is increased which explains why its curve looks smooth\\nAnother way to select a good precisionrecall tradeoff is to plot precision directly\\nagainst recall as shown in Figure 35  the same threshold as earlier is highlighed\\nPerformance Measures  97Figure 35 Precision versus recall\\nY ou can see that precision really starts to fall sharply around 80 recall Y ou will\\nprobably want to select a precisionrecall tradeoff just before that dropfor example\\nat around 60 recall But of course the choice depends on your project\\nSo lets suppose you decide to aim for 90 precision Y ou look up the first plot and\\nfind that you need to use a threshold of about 8000 To be more precise you can\\nsearch for t', 'plot and\\nfind that you need to use a threshold of about 8000 To be more precise you can\\nsearch for the lowest threshold that gives you at least 90 precision  npargmax\\nwill give us the first index of the maximum value which in this case means the first\\nTrue  value\\nthreshold90precision   thresholds npargmaxprecisions   090  7816\\nTo make predictions on the training set for now instead of calling the classifiers\\npredict  method you can just run this code\\nytrainpred90   yscores   threshold90precision \\nLets check these predictions precision and recall\\n precisionscore ytrain5  ytrainpred90 \\n09000380083618396\\n recallscore ytrain5  ytrainpred90 \\n04368197749492714\\nGreat you have a 90 precision classifier  As you can see it is fairly easy to create a\\nclassifier with virtually any precision you want just set a high enough threshold and\\nyoure done Hmm not so fast A highprecision classifier is not very useful if its \\nrecall is too low\\n98  Chapter 3 ClassificationIf someone says lets reach 99 precisi', 'y useful if its \\nrecall is too low\\n98  Chapter 3 ClassificationIf someone says lets reach 99 precision  you should ask at\\nwhat recall\\nThe ROC Curve\\nThe receiver operating characteristic  ROC curve is another common tool used with\\nbinary classifiers It is very similar to the precisionrecall curve but instead of plot\\nting precision versus recall the ROC curve plots the true positive rate  another name\\nfor recall against the false positive rate  The FPR is the ratio of negative instances that\\nare incorrectly classified as positive It is equal to one minus the true negative rate  \\nwhich is the ratio of negative instances that are correctly classified as negative The\\nTNR is also called specificity  Hence the ROC curve plots sensitivity  recall versus\\n1  specificity \\nTo plot the ROC curve you first need to compute the TPR and FPR for various thres\\nhold values using the roccurve  function\\nfrom sklearnmetrics  import roccurve\\nfpr tpr thresholds   roccurve ytrain5  yscores \\nThen you can plot th', 'sklearnmetrics  import roccurve\\nfpr tpr thresholds   roccurve ytrain5  yscores \\nThen you can plot the FPR against the TPR using Matplotlib This code produces the\\nplot in Figure 36 \\ndef plotroccurve fpr tpr labelNone\\n    pltplotfpr tpr linewidth 2 labellabel\\n    pltplot0 1 0 1 k  dashed diagonal\\n      Add axis labels and grid\\nplotroccurve fpr tpr\\npltshow\\nPerformance Measures  99Figure 36 ROC curve\\nOnce again there is a tradeoff the higher the recall TPR the more false positives\\nFPR the classifier produces The dotted line represents the ROC curve of a purely\\nrandom classifier a good classifier stays as far away from that line as possible toward\\nthe topleft corner\\nOne way to compare classifiers is to measure the area under the curve  AUC A per\\nfect classifier will have a ROC AUC  equal to 1 whereas a purely random classifier will\\nhave a ROC AUC equal to 05 ScikitLearn provides a function to compute the ROC\\nAUC\\n from sklearnmetrics  import rocaucscore\\n rocaucscore ytrain5  yscores \\n0961177', ' compute the ROC\\nAUC\\n from sklearnmetrics  import rocaucscore\\n rocaucscore ytrain5  yscores \\n09611778893101814\\nSince the ROC curve is so similar to the precisionrecall or PR\\ncurve you may wonder how to decide which one to use As a rule\\nof thumb you should prefer the PR curve whenever the positive\\nclass is rare or when you care more about the false positives than\\nthe false negatives and the ROC curve otherwise For example\\nlooking at the previous ROC curve and the ROC AUC score you\\nmay think that the classifier is really good But this is mostly\\nbecause there are few positives 5s compared to the negatives\\nnon5s In contrast the PR curve makes it clear that the classifier\\nhas room for improvement the curve could be closer to the top\\nright corner\\n100  Chapter 3 ClassificationLets train a RandomForestClassifier  and compare its ROC curve and ROC AUC\\nscore to the SGDClassifier  First you need to get scores for each instance in the\\ntraining set But due to the way it works see Chapter 7  the Ran', ' get scores for each instance in the\\ntraining set But due to the way it works see Chapter 7  the RandomForestClassi\\nfier  class does not have a decisionfunction  method Instead it has a pre\\ndictproba  method ScikitLearn classifiers generally have one or the other The\\npredictproba  method returns an array containing a row per instance and a col\\numn per class each containing the probability that the given instance belongs to the\\ngiven class eg 70 chance that the image represents a 5\\nfrom sklearnensemble  import RandomForestClassifier\\nforestclf   RandomForestClassifier randomstate 42\\nyprobasforest   crossvalpredict forestclf  Xtrain ytrain5  cv3\\n                                    methodpredictproba \\nBut to plot a ROC curve you need scores not probabilities A simple solution is to\\nuse the positive classs probability as the score\\nyscoresforest   yprobasforest  1    score  proba of positive class\\nfprforest  tprforest  thresholdsforest   roccurve ytrain5 yscoresforest \\nNow you are ready to p', 'lass\\nfprforest  tprforest  thresholdsforest   roccurve ytrain5 yscoresforest \\nNow you are ready to plot the ROC curve It is useful to plot the first ROC curve as\\nwell to see how they compare  Figure 37 \\npltplotfpr tpr b labelSGD\\nplotroccurve fprforest  tprforest  Random Forest \\npltlegendloclower right \\npltshow\\nFigure 37 Comparing ROC curves\\nPerformance Measures  101As you can see in Figure 37  the RandomForestClassifier s ROC curve looks much\\nbetter than the SGDClassifier s it comes much closer to the topleft corner As a\\nresult its ROC AUC score is also significantly better\\n rocaucscore ytrain5  yscoresforest \\n09983436731328145\\nTry measuring the precision and recall scores you should find 990 precision and\\n866 recall Not too bad\\nHopefully you now know how to train binary classifiers choose the appropriate met\\nric for your task evaluate your classifiers using crossvalidation select the precision\\nrecall tradeoff that fits your needs and compare various models using ROC curves\\nand ROC AUC', 'ecision\\nrecall tradeoff that fits your needs and compare various models using ROC curves\\nand ROC AUC scores Now lets try to detect more than just the 5s\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes multiclass classifiers  also\\ncalled multinomial classifiers  can distinguish between more than two classes\\nSome algorithms such as Random Forest classifiers or naive Bayes classifiers are\\ncapable of handling multiple classes directly Others such as Support Vector Machine\\nclassifiers or Linear classifiers are strictly binary classifiers However there are vari\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers\\nFor example one way to create a system that can classify the digit images into 10\\nclasses from 0 to 9 is to train 10 binary classifiers one for each digit a 0detector a\\n1detector a 2detector and so on Then when you want to classify an image you get\\nthe decision score from each classifier for that ', 'on Then when you want to classify an image you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score This is called the oneversusall  OvA strategy \\nalso called oneversustherest \\nAnother strategy is to train a binary classifier for every pair of digits one to distin\\nguish 0s and 1s another to distinguish 0s and 2s another for 1s and 2s and so on\\nThis is called the oneversusone  OvO strategy If there are N classes you need to\\ntrain N  N  1  2 classifiers For the MNIST problem this means training 45\\nbinary classifiers When you want to classify an image you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels The main advan\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish\\nSome algorithms such as Support Vector Machine classifiers scale poorly with the\\nsize of the training set so for these algori', 'upport Vector Machine classifiers scale poorly with the\\nsize of the training set so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets For most binary classification algorithms however OvA is preferred\\n102  Chapter 3 ClassificationScikitLearn detects when you try to use a binary classification algorithm for a multi\\nclass classification task and it automatically runs OvA except for SVM classifiers for\\nwhich it uses OvO Lets try this with the SGDClassifier \\n sgdclffitXtrain ytrain   ytrain not ytrain5\\n sgdclfpredictsomedigit \\narray5 dtypeuint8\\nThat was easy This code trains the SGDClassifier  on the training set using the origi\\nnal target classes from 0 to 9  ytrain  instead of the 5versusall target classes\\nytrain5  Then it makes a prediction a correct one in this case Under the hood\\nScikitLearn actually trained 10 binary classifiers got their decision scores for the\\nimage and sele', '\\nScikitLearn actually trained 10 binary classifiers got their decision scores for the\\nimage and selected the class with the highest score\\nTo see that this is indeed the case you can call the decisionfunction  method\\nInstead of returning just one score per instance it now returns 10 scores one per\\nclass\\n somedigitscores   sgdclfdecisionfunction somedigit \\n somedigitscores\\narray1595522627845 3808096296175 1332666694897\\n           57352692379 176806846644    241253175101\\n        2552686498156 1229015704709  794605205023\\n        1063135888549\\nThe highest score is indeed the one corresponding to class 5\\n npargmaxsomedigitscores \\n5\\n sgdclfclasses\\narray0 1 2 3 4 5 6 7 8 9 dtypeuint8\\n sgdclfclasses 5\\n5\\nWhen a classifier is trained it stores the list of target classes in its\\nclasses  attribute ordered by value In this case the index of each\\nclass in the classes  array conveniently matches the class itself\\neg the class at index 5 happens to be class 5 but in general you\\nwont be so lucky\\nIf you w', 'ss itself\\neg the class at index 5 happens to be class 5 but in general you\\nwont be so lucky\\nIf you want to force ScikitLearn to use oneversusone or oneversusall you can use\\nthe OneVsOneClassifier  or OneVsRestClassifier  classes Simply create an instance\\nand pass a binary classifier to its constructor For example this code creates a multi\\nclass classifier using the OvO strategy based on a SGDClassifier \\n from sklearnmulticlass  import OneVsOneClassifier\\n ovoclf  OneVsOneClassifier SGDClassifier randomstate 42\\n ovoclffitXtrain ytrain\\n ovoclfpredictsomedigit \\nMulticlass Classification   103array5 dtypeuint8\\n lenovoclfestimators \\n45\\nTraining a RandomForestClassifier  is just as easy\\n forestclf fitXtrain ytrain\\n forestclf predictsomedigit \\narray5 dtypeuint8\\nThis time ScikitLearn did not have to run OvA or OvO because Random Forest\\nclassifiers  can directly classify instances into multiple classes Y ou can call\\npredictproba  to get the list of probabilities that the classifier assigned to e', 'asses Y ou can call\\npredictproba  to get the list of probabilities that the classifier assigned to each\\ninstance for each class\\n forestclf predictproba somedigit \\narray0   0   001 008 0   09  0   0   0   001\\nY ou can see that the classifier is fairly confident about its prediction the 09 at the 5th\\nindex in the array means that the model estimates a 90 probability that the image\\nrepresents a 5 It also thinks that the image could instead be a 2 a 3 or a 9 respec\\ntively with 1 8 and 1 probability\\nNow of course you want to evaluate these classifiers As usual you want to use cross\\nvalidation Lets evaluate the SGDClassifier s accuracy using the crossvalscore\\nfunction\\n crossvalscore sgdclf Xtrain ytrain cv3 scoringaccuracy \\narray08489802  087129356 086988048\\nIt gets over 84 on all test folds If you used a random classifier you would get 10\\naccuracy so this is not such a bad score but you can still do much better For exam\\nple simply scaling the inputs as discussed in Chapter 2  increases accu', 'till do much better For exam\\nple simply scaling the inputs as discussed in Chapter 2  increases accuracy above\\n89\\n from sklearnpreprocessing  import StandardScaler\\n scaler  StandardScaler \\n Xtrainscaled   scalerfittransform Xtrainastypenpfloat64\\n crossvalscore sgdclf Xtrainscaled  ytrain cv3 scoringaccuracy \\narray089707059 08960948  090693604\\nError Analysis\\nOf course if this were a real project you would follow the steps in your Machine\\nLearning project checklist see  exploring data preparation options trying out\\nmultiple models shortlisting the best ones and finetuning their hyperparameters\\nusing GridSearchCV  and automating as much as possible as you did in the previous\\nchapter Here we will assume that you have found a promising model and you want\\nto find ways to improve it One way to do this is to analyze the types of errors it\\nmakes\\n104  Chapter 3 ClassificationFirst you can look at the confusion matrix Y ou need to make predictions using the\\ncrossvalpredict  function then call the', 'the confusion matrix Y ou need to make predictions using the\\ncrossvalpredict  function then call the confusionmatrix  function just like\\nyou did earlier\\n ytrainpred   crossvalpredict sgdclf Xtrainscaled  ytrain cv3\\n confmx  confusionmatrix ytrain ytrainpred \\n confmx\\narray5578    0   22    7    8   45   35    5  222    1\\n          0 6410   35   26    4   44    4    8  198   13\\n         28   27 5232  100   74   27   68   37  354   11\\n         23   18  115 5254    2  209   26   38  373   73\\n         11   14   45   12 5219   11   33   26  299  172\\n         26   16   31  173   54 4484   76   14  482   65\\n         31   17   45    2   42   98 5556    3  123    1\\n         20   10   53   27   50   13    3 5696  173  220\\n         17   64   47   91    3  125   24   11 5421   48\\n         24   18   29   67  116   39    1  174  329 5152\\nThats a lot of numbers Its often more convenient to look at an image representation\\nof the confusion matrix using Matplotlibs matshow  function\\npltmatshowconfmx cmap', 'age representation\\nof the confusion matrix using Matplotlibs matshow  function\\npltmatshowconfmx cmappltcmgray\\npltshow\\nThis confusion matrix looks fairly good since most images are on the main diagonal\\nwhich means that they were classified correctly The 5s look slightly darker than the\\nother digits which could mean that there are fewer images of 5s in the dataset or that\\nthe classifier does not perform as well on 5s as on other digits In fact you can verify\\nthat both are the case\\nLets focus the plot on the errors First you need to divide each value in the confusion\\nmatrix by the number of images in the corresponding class so you can compare error\\nError Analysis  105rates instead of absolute number of errors which would make abundant classes look\\nunfairly bad\\nrowsums   confmxsumaxis1 keepdims True\\nnormconfmx   confmx  rowsums\\nNow lets fill the diagonal with zeros to keep only the errors and lets plot the result\\nnpfilldiagonal normconfmx  0\\npltmatshownormconfmx  cmappltcmgray\\npltshow\\nNow ', 'd lets plot the result\\nnpfilldiagonal normconfmx  0\\npltmatshownormconfmx  cmappltcmgray\\npltshow\\nNow you can clearly see the kinds of errors the classifier makes Remember that rows\\nrepresent actual classes while columns represent predicted classes The column for\\nclass 8 is quite bright which tells you that many images get misclassified as 8s How\\never the row for class 8 is not that bad telling you that actual 8s in general get prop\\nerly classified as 8s As you can see the confusion matrix is not necessarily\\nsymmetrical Y ou can also see that 3s and 5s often get confused in both directions\\nAnalyzing the confusion matrix can often give you insights on ways to improve your\\nclassifier Looking at this plot it seems that your efforts should be spent on reducing\\nthe false 8s For example you could try to gather more training data for digits that\\nlook like 8s but are not so the classifier can learn to distinguish them from real 8s\\nOr you could engineer new features that would help the classifier', 'n to distinguish them from real 8s\\nOr you could engineer new features that would help the classifierfor example writ\\ning an algorithm to count the number of closed loops eg 8 has two 6 has one 5 has\\nnone Or you could preprocess the images eg using ScikitImage Pillow or\\nOpenCV to make some patterns stand out more such as closed loops\\nAnalyzing individual errors can also be a good way to gain insights on what your\\nclassifier is doing and why it is failing but it is more difficult and timeconsuming\\n106  Chapter 3 Classification3But remember that our brain is a fantastic pattern recognition system and our visual system does a lot of\\ncomplex preprocessing before any information reaches our consciousness so the fact that it feels simple does\\nnot mean that it isFor example lets plot examples of 3s and 5s the plotdigits  function just uses\\nMatplotlibs imshow  function see this chapters Jupyter notebook for details\\ncla clb  3 5\\nXaa  Xtrainytrain  cla  ytrainpred   cla\\nXab  Xtrainytrain  cla  yt', 'otebook for details\\ncla clb  3 5\\nXaa  Xtrainytrain  cla  ytrainpred   cla\\nXab  Xtrainytrain  cla  ytrainpred   clb\\nXba  Xtrainytrain  clb  ytrainpred   cla\\nXbb  Xtrainytrain  clb  ytrainpred   clb\\npltfigurefigsize88\\npltsubplot221 plotdigits Xaa25 imagesperrow 5\\npltsubplot222 plotdigits Xab25 imagesperrow 5\\npltsubplot223 plotdigits Xba25 imagesperrow 5\\npltsubplot224 plotdigits Xbb25 imagesperrow 5\\npltshow\\nThe two 55 blocks on the left show digits classified as 3s and the two 55 blocks on\\nthe right show images classified as 5s Some of the digits that the classifier gets wrong\\nie in the bottomleft and topright blocks are so badly written that even a human\\nwould have trouble classifying them eg the 5 on the 1st row and 2nd column truly\\nlooks like a badly written 3 However most misclassified images seem like obvious\\nerrors to us and its hard to understand why the classifier made the mistakes it did3\\nThe reason is that we used a simple SGDClassifier  which is a linear model All it\\ndoes is as', 'it did3\\nThe reason is that we used a simple SGDClassifier  which is a linear model All it\\ndoes is assign a weight per class to each pixel and when it sees a new image it just\\nsums up the weighted pixel intensities to get a score for each class So since 3s and 5s\\ndiffer only by a few pixels this model will easily confuse them\\nError Analysis  107The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc If you draw a 3 with the junction slightly shifted to the left\\nthe classifier might classify it as a 5 and vice versa In other words this classifier is\\nquite sensitive to image shifting and rotation So one way to reduce the 35 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated This will probably help reduce other errors as well\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class In some cases you\\nmay want your classifier to output multiple clas', 's been assigned to just one class In some cases you\\nmay want your classifier to output multiple classes for each instance For example\\nconsider a facerecognition classifier what should it do if it recognizes several people\\non the same picture Of course it should attach one tag per person it recognizes Say\\nthe classifier has been trained to recognize three faces Alice Bob and Charlie then\\nwhen it is shown a picture of Alice and Charlie it should output 1 0 1 meaning\\n Alice yes Bob no Charlie yes Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification  system\\nWe wont go into face recognition just yet but lets look at a simpler example just for\\nillustration purposes\\nfrom sklearnneighbors  import KNeighborsClassifier\\nytrainlarge   ytrain  7\\nytrainodd   ytrain  2  1\\nymultilabel   npcytrainlarge  ytrainodd \\nknnclf  KNeighborsClassifier \\nknnclffitXtrain ymultilabel \\nThis code creates a ymultilabel  array containing two target labels for each digit\\ni', 'in ymultilabel \\nThis code creates a ymultilabel  array containing two target labels for each digit\\nimage the first indicates whether or not the digit is large 7 8 or 9 and the second\\nindicates whether or not it is odd The next lines create a KNeighborsClassifier  \\ninstance which supports multilabel classification but not all classifiers do and we\\ntrain it using the multiple targets array Now you can make a prediction and notice\\nthat it outputs two labels\\n knnclfpredictsomedigit \\narrayFalse  True\\nAnd it gets it right The digit 5 is indeed not large  False  and odd  True \\nThere are many ways to evaluate a multilabel classifier and selecting the right metric\\nreally depends on your project For example one approach is to measure the F1 score\\nfor each individual label or any other binary classifier metric discussed earlier then\\nsimply compute the average score This code computes the average F1 score across all\\nlabels\\n108  Chapter 3 Classification4ScikitLearn offers a few other averaging opti', ' score across all\\nlabels\\n108  Chapter 3 Classification4ScikitLearn offers a few other averaging options and multilabel classifier metrics see the documentation for\\nmore details\\n ytrainknnpred   crossvalpredict knnclf Xtrain ymultilabel  cv3\\n f1score ymultilabel  ytrainknnpred  averagemacro\\n0976410265560605\\nThis assumes that all labels are equally important which may not be the case In par\\nticular if you have many more pictures of Alice than of Bob or Charlie you may want\\nto give more weight to the classifiers score on pictures of Alice One simple option is\\nto give each label a weight equal to its support  ie the number of instances with that\\ntarget label To do this simply set averageweighted  in the preceding code4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput\\nmulticlass classification  or simply multioutput classification  It is simply a generaliza\\ntion of multilabel classification where each label can be multiclass ', 'ion  It is simply a generaliza\\ntion of multilabel classification where each label can be multiclass ie it can have\\nmore than two possible values\\nTo illustrate this lets build a system that removes noise from images It will take as\\ninput a noisy digit image and it will hopefully output a clean digit image repre\\nsented as an array of pixel intensities just like the MNIST images Notice that the\\nclassifiers output is multilabel one label per pixel and each label can have multiple\\nvalues pixel intensity ranges from 0 to 255 It is thus an example of a multioutput\\nclassification system\\nThe line between classification and regression is sometimes blurry\\nsuch as in this example Arguably predicting pixel intensity is more\\nakin to regression than to classification Moreover multioutput\\nsystems are not limited to classification tasks you could even have\\na system that outputs multiple labels per instance including both\\nclass labels and value labels\\nLets start by creating the training and test sets by', 'ce including both\\nclass labels and value labels\\nLets start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPys randint  function The target\\nimages will be the original images\\nnoise  nprandomrandint0 100 lenXtrain 784\\nXtrainmod   Xtrain  noise\\nnoise  nprandomrandint0 100 lenXtest 784\\nXtestmod   Xtest  noise\\nytrainmod   Xtrain\\nytestmod   Xtest\\nMultioutput Classification   1095Y ou can use the shift  function from the scipyndimageinterpolation  module For example\\nshiftimage 2 1 cval0  shifts the image 2 pixels down and 1 pixel to the rightLets take a peek at an image from the test set yes were snooping on the test data so\\nyou should be frowning right now\\nOn the left is the noisy input image and on the right is the clean target image Now\\nlets train the classifier and make it clean this image\\nknnclffitXtrainmod  ytrainmod \\ncleandigit   knnclfpredictXtestmod someindex \\nplotdigit cleandigit \\nLooks close enough to the targe', '\\ncleandigit   knnclfpredictXtestmod someindex \\nplotdigit cleandigit \\nLooks close enough to the target This concludes our tour of classification Hopefully\\nyou should now know how to select good metrics for classification tasks pick the\\nappropriate precisionrecall tradeoff compare classifiers and more generally build\\ngood classification systems for a variety of tasks\\nExercises\\n1Try to build a classifier for the MNIST dataset that achieves over 97 accuracy\\non the test set Hint the KNeighborsClassifier  works quite well for this task\\nyou just need to find good hyperparameter values try a grid search on the\\nweights  and nneighbors  hyperparameters\\n2Write a function that can shift an MNIST image in any direction left right up\\nor down by one pixel5 Then for each image in the training set create four shif\\n110  Chapter 3 Classificationted copies one per direction and add them to the training set Finally train your\\nbest model on this expanded training set and measure its accuracy on the test set', 'Finally train your\\nbest model on this expanded training set and measure its accuracy on the test set\\nY ou should observe that your model performs even better now This technique of\\nartificially growing the training set is called data augmentation  or training set\\nexpansion \\n3Tackle the Titanic  dataset A great place to start is on Kaggle \\n4Build a spam classifier a more challenging exercise\\nDownload examples of spam and ham from Apache SpamAssassins public\\ndatasets \\nUnzip the datasets and familiarize yourself with the data format\\nSplit the datasets into a training set and a test set\\nWrite a data preparation pipeline to convert each email into a feature vector\\nY our preparation pipeline should transform an email into a sparse vector\\nindicating the presence or absence of each possible word For example if all\\nemails only ever contain four words Hello  how  are  you  then the email\\nHello you Hello Hello you would be converted into a vector 1 0 0 1\\nmeaning Hello is present how is absent are ', 'Hello Hello you would be converted into a vector 1 0 0 1\\nmeaning Hello is present how is absent are is absent you is\\npresent or 3 0 0 2 if you prefer to count the number of occurrences of\\neach word\\nY ou may want to add hyperparameters to your preparation pipeline to control\\nwhether or not to strip off email headers convert each email to lowercase\\nremove punctuation replace all URLs with URL  replace all numbers with\\nNUMBER  or even perform stemming  ie trim off word endings there are\\nPython libraries available to do this\\nThen try out several classifiers and see if you can build a great spam classifier\\nwith both high recall and high precision\\nSolutions to these exercises are available in the online Jupyter notebooks at https\\ngithubcomageronhandsonml2 \\nExercises  111CHAPTER 4\\nTraining Models\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease', ' as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 4 in the final\\nrelease of the book\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes If you went through some of the exercises in the previous chapters\\nyou may have been surprised by how much you can get done without knowing any\\nthing about whats under the hood you optimized a regression system you improved\\na digit image classifier and you even built a spam classifier from scratchall this\\nwithout knowing how they actually work Indeed in many situations you dont really\\nneed to know the implementation details\\nHowever having a good understanding of how things work can help you quickly\\nhome in on the appropriate model the right training algorithm to use and a good set\\nof hyperparameters for your task Understanding whats under the hood will also help\\nyou debug issues and perform error analysis mo', 'ask Understanding whats under the hood will also help\\nyou debug issues and perform error analysis more efficiently Lastly most of the top\\nics discussed in this chapter will be essential in understanding building and training\\nneural networks discussed in Part II  of this book\\nIn this chapter we will start by looking at the Linear Regression model one of the\\nsimplest models there is We will discuss two very different ways to train it\\nUsing a direct closedform equation that directly computes the model parame\\nters that best fit the model to the training set ie the model parameters that\\nminimize the cost function over the training set\\n113Using an iterative optimization approach called Gradient Descent GD that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set eventually converging to the same set of parameters as the first\\nmethod We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II', 'riants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II  Batch GD Minibatch GD\\nand Stochastic GD\\nNext we will look at Polynomial Regression a more complex model that can fit non\\nlinear datasets Since this model has more parameters than Linear Regression it is\\nmore prone to overfitting the training data so we will look at how to detect whether\\nor not this is the case using learning curves and then we will look at several regulari\\nzation techniques that can reduce the risk of overfitting the training set\\nFinally we will look at two more models that are commonly used for classification\\ntasks Logistic Regression and Softmax Regression\\nThere will be quite a few math equations in this chapter using basic\\nnotions of linear algebra and calculus To understand these equa\\ntions you will need to know what vectors and matrices are how to\\ntranspose them multiply them and inverse them and what partial\\nderivatives are If you are unfamiliar with these concep', 'tiply them and inverse them and what partial\\nderivatives are If you are unfamiliar with these concepts please go\\nthrough the linear algebra and calculus introductory tutorials avail\\nable as Jupyter notebooks in the online supplemental material For\\nthose who are truly allergic to mathematics you should still go\\nthrough this chapter and simply skip the equations hopefully the\\ntext will be sufficient to help you understand most of the concepts\\nLinear Regression\\nIn Chapter 1  we looked at a simple regression model of life satisfaction lifesatisfac\\ntion  0  1  GDPpercapita \\nThis model is just a linear function of the input feature GDPpercapita  0 and 1 are\\nthe models parameters\\nMore generally a linear model makes a prediction by simply computing a weighted\\nsum of the input features plus a constant called the bias term  also called the intercept\\nterm  as shown in Equation 41 \\nEquation 41 Linear Regression model prediction\\ny01x12x2nxn\\n is the predicted value\\n114  Chapter 4 Training Modelsn is', 'r Regression model prediction\\ny01x12x2nxn\\n is the predicted value\\n114  Chapter 4 Training Modelsn is the number of features\\nxi is the ith feature value\\nj is the jth model parameter including the bias term 0 and the feature weights\\n1 2  n\\nThis can be written much more concisely using a vectorized form as shown in Equa\\ntion 42 \\nEquation 42 Linear Regression model prediction vectorized form\\nyhxx\\n is the models parameter vector  containing the bias term 0 and the feature\\nweights 1 to n\\nx is the instances feature vector  containing x0 to xn with x0 always equal to 1\\n  x is the dot product of the vectors  and x which is of course equal to\\n0x01x12x2nxn\\nh is the hypothesis function using the model parameters \\nIn Machine Learning vectors are often represented as column vec\\ntors which are 2D arrays with a single column If  and x are col\\numn vectors then the prediction is yTx where T is the\\ntranspose  of  a row vector instead of a column vector and Tx is\\nthe matrix multiplication of T and x It is', 'se  of  a row vector instead of a column vector and Tx is\\nthe matrix multiplication of T and x It is of course the same pre\\ndiction except it is now represented as a single cell matrix rather\\nthan a scalar value In this book we will use this notation to avoid\\nswitching between dot products and matrix multiplications\\nOkay thats the Linear Regression model so now how do we train it Well recall that\\ntraining a model means setting its parameters so that the model best fits the training\\nset For this purpose we first need a measure of how well or poorly the model fits\\nthe training data In Chapter 2  we saw that the most common performance measure\\nof a regression model is the Root Mean Square Error RMSE  Equation 21  There\\nfore to train a Linear Regression model you need to find the value of  that minimi\\nzes the RMSE In practice it is simpler to minimize the Mean Square Error MSE\\nLinear Regression  1151It is often the case that a learning algorithm will try to optimize a different function th', 'ion  1151It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model This is generally because that function is easier to compute because\\nit has useful differentiation properties that the performance measure lacks or because we want to constrain\\nthe model during training as we will see when we discuss regularization\\n2The demonstration that this returns the value of  that minimizes the cost function is outside the scope of this\\nbookthan the RMSE and it leads to the same result because the value that minimizes a\\nfunction also minimizes its square root1\\nThe MSE of a Linear Regression hypothesis h on a training set X is calculated using\\nEquation 43 \\nEquation 43 MSE cost function for a Linear Regression model\\nMSE Xh1\\nm\\ni 1m\\nTxiyi2\\nMost of these notations were presented in Chapter 2  see Notations  on page 43\\nThe only difference is that we write h instead of just h in order to make it clear that\\nthe mo', 'ge 43\\nThe only difference is that we write h instead of just h in order to make it clear that\\nthe model is parametrized by the vector  To simplify notations we will just write\\nMSE  instead of MSE X h\\nThe Normal Equation\\nTo find the value of  that minimizes the cost function there is a closedform solution\\nin other words a mathematical equation that gives the result directly This is called\\nthe Normal Equation  Equation 44 2\\nEquation 44 Normal Equation\\nXTX1\\xa0XT\\xa0y\\n is the value of  that minimizes the cost function\\ny is the vector of target values containing y1 to ym\\nLets generate some linearlooking data to test this equation on  Figure 41 \\nimport numpy as np\\nX  2  nprandomrand100 1\\ny  4  3  X  nprandomrandn100 1\\n116  Chapter 4 Training ModelsFigure 41 Randomly generated linear dataset\\nNow lets compute  using the Normal Equation We will use the inv  function from\\nNumPys Linear Algebra module  nplinalg  to compute the inverse of a matrix and\\nthe dot  method for matrix multiplication\\nXb  npcnp', 'nplinalg  to compute the inverse of a matrix and\\nthe dot  method for matrix multiplication\\nXb  npcnpones100 1 X   add x0  1 to each instance\\nthetabest   nplinalginvXbTdotXbdotXbTdoty\\nThe actual function that we used to generate the data is y  4  3 x1  Gaussian noise\\nLets see what the equation found\\n thetabest\\narray421509616\\n       277011339\\nWe would have hoped for 0  4 and 1  3 instead of 0  4215 and 1  2770 Close\\nenough but the noise made it impossible to recover the exact parameters of the origi\\nnal function\\nNow you can make predictions using \\n Xnew  nparray0 2\\n Xnewb  npcnpones2 1 Xnew  add x0  1 to each instance\\n ypredict   Xnewbdotthetabest \\n ypredict\\narray421509616\\n       975532293\\nLets plot this models predictions  Figure 42 \\npltplotXnew ypredict  r\\npltplotX y b\\nLinear Regression  1173Note that ScikitLearn separates the bias term  intercept  from the feature weights  coef pltaxis0 2 0 15\\npltshow\\nFigure 42 Linear Regression model predictions\\nPerforming linear regression using Sci', ' 2 0 15\\npltshow\\nFigure 42 Linear Regression model predictions\\nPerforming linear regression using ScikitLearn is quite simple3\\n from sklearnlinearmodel  import LinearRegression\\n linreg  LinearRegression \\n linregfitX y\\n linregintercept  linregcoef\\narray421509616 array277011339\\n linregpredictXnew\\narray421509616\\n       975532293\\nThe LinearRegression  class is based on the scipylinalglstsq  function the\\nname stands for least squares which you could call directly\\n thetabestsvd  residuals  rank s  nplinalglstsqXb y rcond1e6\\n thetabestsvd\\narray421509616\\n       277011339\\nThis function computes Xy where  is the pseudoinverse  of X specifically the\\nMoorePenrose inverse Y ou can use nplinalgpinv  to compute the pseudoin\\nverse directly\\n nplinalgpinvXbdoty\\narray421509616\\n       277011339\\n118  Chapter 4 Training ModelsThe pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  SVD that can decompose the training set\\nmatrix X into the matr', '\\ncalled Singular Value Decomposition  SVD that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U  VT see\\nnumpylinalgsvd  The pseudoinverse is computed as XVUT To compute\\nthe matrix  the algorithm takes  and sets to zero all values smaller than a tiny\\nthreshold value then it replaces all the nonzero values with their inverse and finally\\nit transposes the resulting matrix This approach is more efficient than computing the\\nNormal Equation plus it handles edge cases nicely indeed the Normal Equation may\\nnot work if the matrix XTX is not invertible ie singular such as if m  n or if some\\nfeatures are redundant but the pseudoinverse is always defined\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X which is an  n  1   n  1\\nmatrix where n is the number of features The computational complexity  of inverting\\nsuch a matrix is typically about On24 to On3 depending on the implementation\\nIn other words if you double the number of feat', 'ly about On24 to On3 depending on the implementation\\nIn other words if you double the number of features you multiply the computation\\ntime by roughly 224  53 to 23  8\\nThe SVD approach used by ScikitLearns LinearRegression  class is about On2 If\\nyou double the number of features you multiply the computation time by roughly 4\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large eg 100000 On the\\npositive side both are linear with regards to the number of instan\\nces in the training set they are Om so they handle large training\\nsets efficiently provided they can fit in memory\\nAlso once you have trained your Linear Regression model using the Normal Equa\\ntion or any other algorithm predictions are very fast the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features In other words making predictions on twice as many\\ninstances or twice as many features will just tak', 'In other words making predictions on twice as many\\ninstances or twice as many features will just take roughly twice as much time\\nNow we will look at very different ways to train a Linear Regression model better\\nsuited for cases where there are a large number of features or too many training\\ninstances to fit in memory\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function\\nGradient Descent  119Suppose you are lost in the mountains in a dense fog you can only feel the slope of\\nthe ground below your feet A good strategy to get to the bottom of the valley quickly\\nis to go downhill in the direction of the steepest slope This is exactly what Gradient\\nDescent does it measures the local gradient of the error function with regards to the \\nparameter vector  and it goes in the direction of descending gradi', 'ror function with regards to the \\nparameter vector  and it goes in the direction of descending gradient Once the gra\\ndient is zero you have reached a minimum\\nConcretely you start by filling  with random values this is called random initializa\\ntion and then you improve it gradually taking one baby step at a time each step\\nattempting to decrease the cost function eg the MSE until the algorithm converges\\nto a minimum see Figure 43 \\nFigure 43 Gradient Descent\\nAn important parameter in Gradient Descent is the size of the steps determined by \\nthe learning rate  hyperparameter If the learning rate is too small then the algorithm\\nwill have to go through many iterations to converge which will take a long time see\\nFigure 44 \\n120  Chapter 4 Training ModelsFigure 44 Learning rate too small\\nOn the other hand if the learning rate is too high you might jump across the valley\\nand end up on the other side possibly even higher up than you were before This\\nmight make the algorithm diverge with larger and', 'e possibly even higher up than you were before This\\nmight make the algorithm diverge with larger and larger values failing to find a good\\nsolution see Figure 45 \\nFigure 45 Learning rate too large\\nFinally not all cost functions look like nice regular bowls There may be holes ridges\\nplateaus and all sorts of irregular terrains making convergence to the minimum very\\ndifficult Figure 46  shows the two main challenges with Gradient Descent if the ran\\ndom initialization starts the algorithm on the left then it will converge to a local mini\\nmum  which is not as good as the global minimum  If it starts on the right then it will\\ntake a very long time to cross the plateau and if you stop too early you will never\\nreach the global minimum\\nGradient Descent  1214Technically speaking its derivative is Lipschitz continuous \\n5Since feature 1 is smaller it takes a larger change in 1 to affect the cost function which is why the bowl is\\nelongated along the 1 axis\\nFigure 46 Gradient Descent pitfalls\\nFortun', 'ction which is why the bowl is\\nelongated along the 1 axis\\nFigure 46 Gradient Descent pitfalls\\nFortunately the MSE cost function for a Linear Regression model happens to be a\\nconvex function  which means that if you pick any two points on the curve the line\\nsegment joining them never crosses the curve This implies that there are no local\\nminima just one global minimum It is also a continuous function with a slope that\\nnever changes abruptly4 These two facts have a great consequence Gradient Descent\\nis guaranteed to approach arbitrarily close the global minimum if you wait long\\nenough and if the learning rate is not too high\\nIn fact the cost function has the shape of a bowl but it can be an elongated bowl if\\nthe features have very different scales Figure 47  shows Gradient Descent on a train\\ning set where features 1 and 2 have the same scale on the left and on a training set\\nwhere feature 1 has much smaller values than feature 2 on the right5\\nFigure 47 Gradient Descent with and without f', '1 has much smaller values than feature 2 on the right5\\nFigure 47 Gradient Descent with and without feature scaling\\n122  Chapter 4 Training ModelsAs you can see on the left the Gradient Descent algorithm goes straight toward the\\nminimum thereby reaching it quickly whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum and it ends with a long\\nmarch down an almost flat valley It will eventually reach the minimum but it will\\ntake a long time\\nWhen using Gradient Descent you should ensure that all features\\nhave a similar scale eg using ScikitLearns StandardScaler\\nclass or else it will take much longer to converge\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function over the training\\nset It is a search in the models parameter space  the more parameters a model has\\nthe more dimensions this space has and the harder the search is searching for a nee\\ndle', ' a model has\\nthe more dimensions this space has and the harder the search is searching for a nee\\ndle in a 300dimensional haystack is much trickier than in three dimensions Fortu\\nnately since the cost function is convex in the case of Linear Regression the needle is\\nsimply at the bottom of the bowl\\nBatch Gradient Descent\\nTo implement Gradient Descent you need to compute the gradient of the cost func\\ntion with regards to each model parameter j In other words you need to calculate\\nhow much the cost function will change if you change j just a little bit This is called \\na partial derivative  It is like asking what is the slope of the mountain under my feet\\nif I face east and then asking the same question facing north and so on for all other\\ndimensions if you can imagine a universe with more than three dimensions Equa\\ntion 45  computes the partial derivative of the cost function with regards to parame\\nter j noted \\nj MSE \\nEquation 45 Partial derivatives of the cost function\\n\\njMSE 2\\nm\\ni 1m\\nTxi', 'o parame\\nter j noted \\nj MSE \\nEquation 45 Partial derivatives of the cost function\\n\\njMSE 2\\nm\\ni 1m\\nTxiyixji\\nInstead of computing these partial derivatives individually you can use Equation 46\\nto compute them all in one go The gradient vector noted MSE  contains all the\\npartial derivatives of the cost function one for each model parameter\\nGradient Descent  1236Eta   is the 7th letter of the Greek alphabet\\nEquation 46 Gradient vector of the cost function\\nMSE \\n0MSE \\n\\n1MSE \\n\\n\\nnMSE 2\\nmXTXy\\nNotice that this formula involves calculations over the full training\\nset X at each Gradient Descent step This is why the algorithm is\\ncalled Batch Gradient Descent  it uses the whole batch of training\\ndata at every step actually Full Gradient Descent  would probably\\nbe a better name As a result it is terribly slow on very large train\\ning sets but we will see much faster Gradient Descent algorithms\\nshortly However Gradient Descent scales well with the number of\\nfeatures training a Linear Regression model wh', 'wever Gradient Descent scales well with the number of\\nfeatures training a Linear Regression model when there are hun\\ndreds of thousands of features is much faster using Gradient\\nDescent than using the Normal Equation or SVD decomposition\\nOnce you have the gradient vector which points uphill just go in the opposite direc\\ntion to go downhill This means subtracting MSE  from  This is where the \\nlearning rate  comes into play6 multiply the gradient vector by  to determine the\\nsize of the downhill step  Equation 47 \\nEquation 47 Gradient Descent step\\nnext stepMSE \\nLets look at a quick implementation of this algorithm\\neta  01   learning rate\\nniterations   1000\\nm  100\\ntheta  nprandomrandn21   random initialization\\nfor iteration  in rangeniterations \\n    gradients   2m  XbTdotXbdottheta  y\\n    theta  theta  eta  gradients\\n124  Chapter 4 Training ModelsThat wasnt too hard Lets look at the resulting theta \\n theta\\narray421509616\\n       277011339\\nHey thats exactly what the Normal Equation found Gra', 'g theta \\n theta\\narray421509616\\n       277011339\\nHey thats exactly what the Normal Equation found Gradient Descent worked per\\nfectly But what if you had used a different learning rate eta Figure 48  shows the\\nfirst 10 steps of Gradient Descent using three different learning rates the dashed line\\nrepresents the starting point\\nFigure 48 Gradient Descent with various learning rates\\nOn the left the learning rate is too low the algorithm will eventually reach the solu\\ntion but it will take a long time In the middle the learning rate looks pretty good in\\njust a few iterations it has already converged to the solution On the right the learn\\ning rate is too high the algorithm diverges jumping all over the place and actually\\ngetting further and further away from the solution at every step\\nTo find a good learning rate you can use grid search see Chapter 2  However you\\nmay want to limit the number of iterations so that grid search can eliminate models\\nthat take too long to converge\\nY ou may wonder ', ' iterations so that grid search can eliminate models\\nthat take too long to converge\\nY ou may wonder how to set the number of iterations If it is too low you will still be\\nfar away from the optimal solution when the algorithm stops but if it is too high you\\nwill waste time while the model parameters do not change anymore A simple solu\\ntion is to set a very large number of iterations but to interrupt the algorithm when the\\ngradient vector becomes tinythat is when its norm becomes smaller than a tiny\\nnumber  called the tolerance because this happens when Gradient Descent has\\nalmost reached the minimum\\nGradient Descent  1257Outofcore algorithms are discussed in Chapter 1 Convergence Rate\\nWhen the cost function is convex and its slope does not change abruptly as is the\\ncase for the MSE cost function Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution but you may have to wait a while it\\ncan take O1  iterations to reach the optimum within a range', 'tion but you may have to wait a while it\\ncan take O1  iterations to reach the optimum within a range of  depending on the\\nshape of the cost function If you divide the tolerance by 10 to have a more precise\\nsolution then the algorithm may have to run about 10 times longer\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step which makes it very slow when\\nthe training set is large At the opposite extreme Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration It also makes it possible to\\ntrain on huge training sets since only one instance needs to be in memory at each\\niteration SGD can be implemented as an outofcore algorithm7\\nOn the other hand due to its stochastic ie random nat', ' can be implemented as an outofcore algorithm7\\nOn the other hand due to its stochastic ie random nature this algorithm is much\\nless regular than Batch Gradient Descent instead of gently decreasing until it reaches\\nthe minimum the cost function will bounce up and down decreasing only on aver\\nage Over time it will end up very close to the minimum but once it gets there it will\\ncontinue to bounce around never settling down see Figure 49  So once the algo\\nrithm stops the final parameter values are good but not optimal\\nFigure 49 Stochastic Gradient Descent\\n126  Chapter 4 Training ModelsWhen the cost function is very irregular as in Figure 46  this can actually help the\\nalgorithm jump out of local minima so Stochastic Gradient Descent has a better\\nchance of finding the global minimum than Batch Gradient Descent does\\nTherefore randomness is good to escape from local optima but bad because it means\\nthat the algorithm can never settle at the minimum One solution to this dilemma is\\nto gradually ', 'ans\\nthat the algorithm can never settle at the minimum One solution to this dilemma is\\nto gradually reduce the learning rate The steps start out large which helps make\\nquick progress and escape local minima then get smaller and smaller allowing the\\nalgorithm to settle at the global minimum This process is akin to simulated anneal\\ning an algorithm inspired from the process of annealing in metallurgy where molten\\nmetal is slowly cooled down The function that determines the learning rate at each\\niteration is called the learning schedule  If the learning rate is reduced too quickly you\\nmay get stuck in a local minimum or even end up frozen halfway to the minimum If\\nthe learning rate is reduced too slowly you may jump around the minimum for a\\nlong time and end up with a suboptimal solution if you halt training too early\\nThis code implements Stochastic Gradient Descent using a simple learning schedule\\nnepochs   50\\nt0 t1  5 50   learning schedule hyperparameters\\ndef learningschedule t\\n    ret', 'schedule\\nnepochs   50\\nt0 t1  5 50   learning schedule hyperparameters\\ndef learningschedule t\\n    return t0  t  t1\\ntheta  nprandomrandn21   random initialization\\nfor epoch in rangenepochs \\n    for i in rangem\\n        randomindex   nprandomrandintm\\n        xi  Xbrandomindex randomindex 1\\n        yi  yrandomindex randomindex 1\\n        gradients   2  xiTdotxidottheta  yi\\n        eta  learningschedule epoch  m  i\\n        theta  theta  eta  gradients\\nBy convention we iterate by rounds of m iterations each round is called an epoch  \\nWhile the Batch Gradient Descent code iterated 1000 times through the whole train\\ning set this code goes through the training set only 50 times and reaches a fairly good\\nsolution\\n theta\\narray421076011\\n       274856079\\nFigure 410  shows the first 20 steps of training notice how irregular the steps are\\nGradient Descent  127Figure 410 Stochastic Gradient Descent first 20 steps\\nNote that since instances are picked randomly some instances may be picked several\\ntimes pe', '20 steps\\nNote that since instances are picked randomly some instances may be picked several\\ntimes per epoch while others may not be picked at all If you want to be sure that the\\nalgorithm goes through every instance at each epoch another approach is to shuffle\\nthe training set making sure to shuffle the input features and the labels jointly then\\ngo through it instance by instance then shuffle it again and so on However this gen\\nerally converges more slowly\\nWhen using Stochastic Gradient Descent the training instances\\nmust be independent and identically distributed IID to ensure\\nthat the parameters get pulled towards the global optimum on\\naverage A simple way to ensure this is to shuffle the instances dur\\ning training eg pick each instance randomly or shuffle the train\\ning set at the beginning of each epoch If you do not do this for\\nexample if the instances are sorted by label then SGD will start by\\noptimizing for one label then the next and so on and it will not\\nsettle close to the glo', 'll start by\\noptimizing for one label then the next and so on and it will not\\nsettle close to the global minimum\\nTo perform Linear Regression using SGD with ScikitLearn you can use the SGDRe\\ngressor  class which defaults to optimizing the squared error cost function The fol\\nlowing code runs for maximum 1000 epochs  maxiter1000  or until the loss drops\\nby less than 1e3 during one epoch  tol1e3  starting with a learning rate of 01\\neta001  using the default learning schedule different from the preceding one\\nand it does not use any regularization  penaltyNone  more details on this shortly\\nfrom sklearnlinearmodel  import SGDRegressor\\nsgdreg  SGDRegressor maxiter 1000 tol1e3 penaltyNone eta001\\nsgdregfitX yravel\\n128  Chapter 4 Training ModelsOnce again you find a solution quite close to the one returned by the Normal Equa\\ntion\\n sgdregintercept  sgdregcoef\\narray424365286 array28250878\\nMinibatch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Minibatch Gradient\\nDes', 'radient Descent\\nThe last Gradient Descent algorithm we will look at is called Minibatch Gradient\\nDescent  It is quite simple to understand once you know Batch and Stochastic Gradi\\nent Descent at each step instead of computing the gradients based on the full train\\ning set as in Batch GD or based on just one instance as in Stochastic GD Mini\\nbatch GD computes the gradients on small random sets of instances called mini\\nbatches  The main advantage of Minibatch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations especially\\nwhen using GPUs\\nThe algorithms progress in parameter space is less erratic than with SGD especially\\nwith fairly large minibatches As a result Minibatch GD will end up walking\\naround a bit closer to the minimum than SGD But on the other hand it may be\\nharder for it to escape from local minima in the case of problems that suffer from\\nlocal minima unlike Linear Regression as we saw earlier Figure 411  shows the\\npaths ', 'hat suffer from\\nlocal minima unlike Linear Regression as we saw earlier Figure 411  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining They all end up near the minimum but Batch GDs path actually stops at the\\nminimum while both Stochastic GD and Minibatch GD continue to walk around\\nHowever dont forget that Batch GD takes a lot of time to take each step and Stochas\\ntic GD and Minibatch GD would also reach the minimum if you used a good learn\\ning schedule\\nFigure 411 Gradient Descent paths in parameter space\\nGradient Descent  1298While the Normal Equation can only perform Linear Regression the Gradient Descent algorithms can be\\nused to train many other models as we will see\\n9A quadratic equation is of the form y  ax2  bx  c\\nLets compare the algorithms weve discussed so far for Linear Regression8 recall that\\nm is the number of training instances and n is the number of features see Table 41 \\nTable 41 Comparison of algorithms for Linear Regressi', ' and n is the number of features see Table 41 \\nTable 41 Comparison of algorithms for Linear Regression\\nAlgorithm Large m Outofcore support Large n Hyperparams Scaling required ScikitLearn\\nNormal Equation Fast No Slow 0 No na\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast 2 Yes SGDRegressor\\nMinibatch GD Fast Yes Fast 2 Yes SGDRegressor\\nThere is almost no difference after training all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line Surprisingly\\nyou can actually use a linear model to fit nonlinear data A simple way to do this is to\\nadd powers of each feature as new features then train a linear model on this extended\\nset of features This technique is called Polynomial Regression \\nLets look at an example First lets generate some nonlinear data based on a simple\\nquadratic equation9 plus some no', 'n example First lets generate some nonlinear data based on a simple\\nquadratic equation9 plus some noise see Figure 412 \\nm  100\\nX  6  nprandomrandm 1  3\\ny  05  X2  X  2  nprandomrandnm 1\\n130  Chapter 4 Training ModelsFigure 412 Generated nonlinear and noisy dataset\\nClearly a straight line will never fit this data properly So lets use ScikitLearns Poly\\nnomialFeatures  class to transform our training data adding the square 2nddegree\\npolynomial of each feature in the training set as new features in this case there is\\njust one feature\\n from sklearnpreprocessing  import PolynomialFeatures\\n polyfeatures   PolynomialFeatures degree2 includebias False\\n Xpoly  polyfeatures fittransform X\\n X0\\narray075275929\\n Xpoly0\\narray075275929 056664654\\nXpoly  now contains the original feature of X plus the square of this feature Now you\\ncan fit a LinearRegression  model to this extended training data  Figure 413 \\n linreg  LinearRegression \\n linregfitXpoly y\\n linregintercept  linregcoef\\narray178134581 array093', '3 \\n linreg  LinearRegression \\n linregfitXpoly y\\n linregintercept  linregcoef\\narray178134581 array093366893 056456263\\nPolynomial Regression  131Figure 413 Polynomial Regression model predictions\\nNot bad the model estimates y 0  56 x12 0  93 x1 1  78  when in fact the original\\nfunction was y 0  5 x12 1  0 x1 2  0  Gaussian noise \\nNote that when there are multiple features Polynomial Regression is capable of find\\ning relationships between features which is something a plain Linear Regression\\nmodel cannot do This is made possible by the fact that PolynomialFeatures  also\\nadds all combinations of features up to the given degree For example if there were\\ntwo features a and b PolynomialFeatures  with degree3  would not only add the\\nfeatures a2 a3 b2 and b3 but also the combinations ab a2b and ab2\\nPolynomialFeaturesdegreed  transforms an array containing n\\nfeatures into an array containing nd\\ndn features where n is the\\nfactorial  of n equal to 1  2  3    n Beware of the combinato\\nrial explosio', ' features where n is the\\nfactorial  of n equal to 1  2  3    n Beware of the combinato\\nrial explosion of the number of features\\nLearning Curves\\nIf you perform highdegree Polynomial Regression you will likely fit the training\\ndata much better than with plain Linear Regression For example Figure 414  applies\\na 300degree polynomial model to the preceding training data and compares the\\nresult with a pure linear model and a quadratic model 2nddegree polynomial\\nNotice how the 300degree polynomial model wiggles around to get as close as possi\\nble to the training instances\\n132  Chapter 4 Training ModelsFigure 414 Highdegree Polynomial Regression\\nOf course this highdegree Polynomial Regression model is severely overfitting the\\ntraining data while the linear model is underfitting it The model that will generalize\\nbest in this case is the quadratic model It makes sense since the data was generated\\nusing a quadratic model but in general you wont know what function generated the\\ndata so how can you', 'using a quadratic model but in general you wont know what function generated the\\ndata so how can you decide how complex your model should be How can you tell\\nthat your model is overfitting or underfitting the data\\nIn Chapter 2  you used crossvalidation to get an estimate of a models generalization\\nperformance If a model performs well on the training data but generalizes poorly\\naccording to the crossvalidation metrics then your model is overfitting If it per\\nforms poorly on both then it is underfitting This is one way to tell when a model is\\ntoo simple or too complex\\nAnother way is to look at the learning curves  these are plots of the models perfor\\nmance on the training set and the validation set as a function of the training set size\\nor the training iteration To generate the plots simply train the model several times\\non different sized subsets of the training set The following code defines a function\\nthat plots the learning curves of a model given some training data\\nfrom sklearnmetric', 'nes a function\\nthat plots the learning curves of a model given some training data\\nfrom sklearnmetrics  import meansquarederror\\nfrom sklearnmodelselection  import traintestsplit\\ndef plotlearningcurves model X y\\n    Xtrain Xval ytrain yval  traintestsplit X y testsize 02\\n    trainerrors  valerrors    \\n    for m in range1 lenXtrain\\n        modelfitXtrainm ytrainm\\n        ytrainpredict   modelpredictXtrainm\\nLearning Curves  133        yvalpredict   modelpredictXval\\n        trainerrors appendmeansquarederror ytrainm ytrainpredict \\n        valerrors appendmeansquarederror yval yvalpredict \\n    pltplotnpsqrttrainerrors  r linewidth 2 labeltrain\\n    pltplotnpsqrtvalerrors  b linewidth 3 labelval\\nLets look at the learning curves of the plain Linear Regression model a straight line\\nFigure 415 \\nlinreg  LinearRegression \\nplotlearningcurves linreg X y\\nFigure 415 Learning curves\\nThis deserves a bit of explanation First lets look at the performance on the training\\ndata when there are just one or two ', ' explanation First lets look at the performance on the training\\ndata when there are just one or two instances in the training set the model can fit\\nthem perfectly which is why the curve starts at zero But as new instances are added\\nto the training set it becomes impossible for the model to fit the training data per\\nfectly both because the data is noisy and because it is not linear at all So the error on\\nthe training data goes up until it reaches a plateau at which point adding new instan\\nces to the training set doesnt make the average error much better or worse Now lets\\nlook at the performance of the model on the validation data When the model is\\ntrained on very few training instances it is incapable of generalizing properly which\\nis why the validation error is initially quite big Then as the model is shown more\\ntraining examples it learns and thus the validation error slowly goes down However\\nonce again a straight line cannot do a good job modeling the data so the error ends\\nup at a p', 'owever\\nonce again a straight line cannot do a good job modeling the data so the error ends\\nup at a plateau very close to the other curve\\nThese learning curves are typical of an underfitting model Both curves have reached\\na plateau they are close and fairly high\\n134  Chapter 4 Training ModelsIf your model is underfitting the training data adding more train\\ning examples will not help Y ou need to use a more complex model\\nor come up with better features\\nNow lets look at the learning curves of a 10thdegree polynomial model on the same\\ndata  Figure 416 \\nfrom sklearnpipeline  import Pipeline\\npolynomialregression   Pipeline \\n        polyfeatures  PolynomialFeatures degree10 includebias False\\n        linreg  LinearRegression \\n    \\nplotlearningcurves polynomialregression  X y\\nThese learning curves look a bit like the previous ones but there are two very impor\\ntant differences\\nThe error on the training data is much lower than with the Linear Regression\\nmodel\\nThere is a gap between the curves Thi', 'ining data is much lower than with the Linear Regression\\nmodel\\nThere is a gap between the curves This means that the model performs signifi\\ncantly better on the training data than on the validation data which is the hall\\nmark of an overfitting model However if you used a much larger training set\\nthe two curves would continue to get closer\\nFigure 416 Learning curves for the polynomial model\\nLearning Curves  13510This notion of bias is not to be confused with the bias term of linear models\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error\\nThe BiasVariance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodels generalization error can be expressed as the sum of three very different\\nerrors\\nBias\\nThis part of the generalization error is due to wrong assumptions such as assum\\ning that the data is linear when it is actually quadratic A highbias model is most\\nlikely to und', 'sum\\ning that the data is linear when it is actually quadratic A highbias model is most\\nlikely to underfit the training data10\\nVariance\\nThis part is due to the models excessive sensitivity to small variations in the\\ntraining data A model with many degrees of freedom such as a highdegree pol\\nynomial model is likely to have high variance and thus to overfit the training\\ndata\\nIrreducible error\\nThis part is due to the noisiness of the data itself The only way to reduce this\\npart of the error is to clean up the data eg fix the data sources such as broken\\nsensors or detect and remove outliers\\nIncreasing a models complexity will typically increase its variance and reduce its bias\\nConversely reducing a models complexity increases its bias and reduces its variance \\nThis is why it is called a tradeoff\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2 a good way to reduce overfitting is to regularize the\\nmodel ie to constrain it the fewer degrees of freedom it has the harder it will be\\nfor i', 'ularize the\\nmodel ie to constrain it the fewer degrees of freedom it has the harder it will be\\nfor it to overfit the data For example a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees\\nFor a linear model regularization is typically achieved by constraining the weights of\\nthe model We will now look at Ridge Regression Lasso Regression and Elastic Net\\nwhich implement three different ways to constrain the weights\\n136  Chapter 4 Training Models11It is common to use the notation J for cost functions that dont have a short name we will often use this\\nnotation throughout the rest of this book The context will make it clear which cost function is being dis\\ncussed\\n12Norms are discussed in Chapter 2 \\nRidge Regression\\nRidge Regression  also called Tikhonov regularization  is a regularized version of Lin\\near Regression a regularization term  equal to i 1ni2 is added to the cost function \\nThis forces the learning algorithm to not only fit the data but also k', 's added to the cost function \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible Note that the regularization term should only be added\\nto the cost function during training Once the model is trained you want to evaluate\\nthe models performance using the unregularized performance measure\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing Apart\\nfrom regularization another reason why they might be different is\\nthat a good training cost function should have optimization\\nfriendly derivatives while the performance measure used for test\\ning should be as close as possible to the final objective A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss discussed in a moment but evaluated using precision\\nrecall\\nThe hyperparameter  controls how much you want to regularize the model If   0\\nthen Ridge Regression is just Linear Regressi', 'trols how much you want to regularize the model If   0\\nthen Ridge Regression is just Linear Regression If  is very large then all weights end\\nup very close to zero and the result is a flat line going through the datas mean Equa\\ntion 48  presents the Ridge Regression cost function11\\nEquation 48 Ridge Regression cost function\\nJ MSE 1\\n2i 1ni2\\nNote that the bias term 0 is not regularized the sum starts at i  1 not 0 If we\\ndefine w as the vector of feature weights  1 to n then the regularization term is\\nsimply equal to   w 22 where  w 2 represents the 2 norm of the weight vector12\\nFor Gradient Descent just add w to the MSE gradient vector  Equation 46 \\nIt is important to scale the data eg using a StandardScaler  \\nbefore performing Ridge Regression as it is sensitive to the scale of\\nthe input features This is true of most regularized models\\nRegularized Linear Models  13713A square matrix full of 0s except for 1s on the main diagonal topleft to bottomrightFigure 417  shows several Ridge model', 'of 0s except for 1s on the main diagonal topleft to bottomrightFigure 417  shows several Ridge models trained on some linear data using different \\nvalue On the left plain Ridge models are used leading to linear predictions On the\\nright the data is first expanded using PolynomialFeaturesdegree10  then it is\\nscaled using a StandardScaler  and finally the Ridge models are applied to the result\\ning features this is Polynomial Regression with Ridge regularization Note how\\nincreasing  leads to flatter ie less extreme more reasonable predictions this\\nreduces the models variance but increases its bias\\nAs with Linear Regression we can perform Ridge Regression either by computing a \\nclosedform equation or by performing Gradient Descent The pros and cons are the\\nsame Equation 49  shows the closedform solution where A is the  n  1   n  1\\nidentity matrix13 except with a 0 in the topleft cell corresponding to the bias term\\nFigure 417 Ridge Regression\\nEquation 49 Ridge Regression closedform solution\\n', 'nding to the bias term\\nFigure 417 Ridge Regression\\nEquation 49 Ridge Regression closedform solution\\nXTXA1\\xa0XT\\xa0y\\nHere is how to perform Ridge Regression with ScikitLearn using a closedform solu\\ntion a variant of Equation 49  using a matrix factorization technique by AndrLouis\\nCholesky\\n from sklearnlinearmodel  import Ridge\\n ridgereg   Ridgealpha1 solvercholesky \\n ridgereg fitX y\\n138  Chapter 4 Training Models14Alternatively you can use the Ridge  class with the sag  solver Stochastic Average GD is a variant of SGD\\nFor more details see the presentation Minimizing Finite Sums with the Stochastic Average Gradient Algo\\nrithm  by Mark Schmidt et al from the University of British Columbia ridgereg predict15\\narray155071465\\nAnd using Stochastic Gradient Descent14\\n sgdreg  SGDRegressor penaltyl2\\n sgdregfitX yravel\\n sgdregpredict15\\narray147012588\\nThe penalty  hyperparameter sets the type of regularization term to use Specifying\\nl2  indicates that you want SGD to add a regularization term to the co', 'zation term to use Specifying\\nl2  indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the 2 norm of the weight vector this is simply Ridge\\nRegression\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  simply called Lasso\\nRegression  is another regularized version of Linear Regression just like Ridge\\nRegression it adds a regularization term to the cost function but it uses the 1 norm\\nof the weight vector instead of half the square of the 2 norm see Equation 410 \\nEquation 410 Lasso Regression cost function\\nJ MSE i 1ni\\nFigure 418  shows the same thing as Figure 417  but replaces Ridge models with\\nLasso models and uses smaller  values\\nRegularized Linear Models  139Figure 418 Lasso Regression\\nAn important characteristic of Lasso Regression is that it tends to completely elimi\\nnate the weights of the least important features ie set them to zero For example\\nthe dashed line in the right plot on Figure 418  with   ', 'nt features ie set them to zero For example\\nthe dashed line in the right plot on Figure 418  with   107 looks quadratic almost\\nlinear all the weights for the highdegree polynomial features are equal to zero In\\nother words Lasso Regression automatically performs feature selection and outputs a\\nsparse model  ie with few nonzero feature weights\\nY ou can get a sense of why this is the case by looking at Figure 419  on the topleft\\nplot the background contours ellipses represent an unregularized MSE cost func\\ntion    0 and the white circles show the Batch Gradient Descent path with that\\ncost function The foreground contours diamonds represent the 1 penalty and the\\ntriangles show the BGD path for this penalty only      Notice how the path first\\nreaches 1  0 then rolls down a gutter until it reaches 2  0 On the topright plot\\nthe contours represent the same cost function plus an 1 penalty with   05 The\\nglobal minimum is on the 2  0 axis BGD first reaches 2  0 then rolls down the\\ngutter until it', '05 The\\nglobal minimum is on the 2  0 axis BGD first reaches 2  0 then rolls down the\\ngutter until it reaches the global minimum The two bottom plots show the same\\nthing but uses an 2 penalty instead The regularized minimum is closer to   0 than\\nthe unregularized minimum but the weights do not get fully eliminated\\n140  Chapter 4 Training Models15Y ou can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra\\ndient vectors around that point\\nFigure 419 Lasso versus Ridge regularization\\nOn the Lasso cost function the BGD path tends to bounce across\\nthe gutter toward the end This is because the slope changes\\nabruptly at 2  0 Y ou need to gradually reduce the learning rate in\\norder to actually converge to the global minimum\\nThe Lasso cost function is not differentiable at i  0 for i  1 2  n but Gradient\\nDescent still works fine if you use a subgradient vector  g15 instead when any i  0\\nEquation 411  shows a subgradient vector equation you can use', 'ient vector  g15 instead when any i  0\\nEquation 411  shows a subgradient vector equation you can use for Gradient Descent\\nwith the Lasso cost function\\nEquation 411 Lasso Regression subgradient vector\\ngJMSE sign 1\\nsign 2\\n\\nsign n\\xa0\\xa0 where\\xa0 sign i1 if\\xa0 i 0\\n0 if\\xa0 i 0\\n1 if\\xa0 i 0\\nRegularized Linear Models  141Here is a small ScikitLearn example using the Lasso  class Note that you could\\ninstead use an SGDRegressorpenaltyl1 \\n from sklearnlinearmodel  import Lasso\\n lassoreg   Lassoalpha01\\n lassoreg fitX y\\n lassoreg predict15\\narray153788174\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression The\\nregularization term is a simple mix of both Ridge and Lassos regularization terms\\nand you can control the mix ratio r When r  0 Elastic Net is equivalent to Ridge\\nRegression and when r  1 it is equivalent to Lasso Regression see Equation 412 \\nEquation 412 Elastic Net cost function\\nJ MSE ri 1ni1 r\\n2i 1ni2\\nSo when should you use plain Linear Regression ie without any reg', 't function\\nJ MSE ri 1ni1 r\\n2i 1ni2\\nSo when should you use plain Linear Regression ie without any regularization\\nRidge Lasso or Elastic Net It is almost always preferable to have at least a little bit of\\nregularization so generally you should avoid plain Linear Regression Ridge is a good\\ndefault but if you suspect that only a few features are actually useful you should pre\\nfer Lasso or Elastic Net since they tend to reduce the useless features weights down to\\nzero as we have discussed In general Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated\\nHere is a short example using ScikitLearns ElasticNet  l1ratio  corresponds to\\nthe mix ratio r\\n from sklearnlinearmodel  import ElasticNet\\n elasticnet   ElasticNet alpha01 l1ratio 05\\n elasticnet fitX y\\n elasticnet predict15\\narray154333232\\nEarly Stopping\\nA very different way to regularize iterative lear', 'elasticnet predict15\\narray154333232\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum This is\\ncalled early stopping  Figure 420  shows a complex model in this case a highdegree\\nPolynomial Regression model being trained using Batch Gradient Descent As the\\nepochs go by the algorithm learns and its prediction error RMSE on the training set\\nnaturally goes down and so does its prediction error on the validation set However\\n142  Chapter 4 Training Modelsafter a while the validation error stops decreasing and actually starts to go back up\\nThis indicates that the model has started to overfit the training data With early stop\\nping you just stop training as soon as the validation error reaches the minimum It is\\nsuch a simple and efficient regularization technique that Geoffrey Hinton called it a\\nbeautiful free lunch \\nFigure 420 Early stopping regularization\\nWith Stochastic', 'ey Hinton called it a\\nbeautiful free lunch \\nFigure 420 Early stopping regularization\\nWith Stochastic and Minibatch Gradient Descent the curves are\\nnot so smooth and it may be hard to know whether you have\\nreached the minimum or not One solution is to stop only after the\\nvalidation error has been above the minimum for some time when\\nyou are confident that the model will not do any better then roll\\nback the model parameters to the point where the validation error\\nwas at a minimum\\nHere is a basic implementation of early stopping\\nfrom sklearnbase  import clone\\n prepare the data\\npolyscaler   Pipeline \\n        polyfeatures  PolynomialFeatures degree90 includebias False\\n        stdscaler  StandardScaler \\n    \\nXtrainpolyscaled   polyscaler fittransform Xtrain\\nXvalpolyscaled   polyscaler transform Xval\\nsgdreg  SGDRegressor maxiter 1 tolnpinfty warmstart True\\n                       penaltyNone learningrate constant  eta000005\\nRegularized Linear Models  143minimumvalerror   floatinf\\nbestepoch   N', 'rningrate constant  eta000005\\nRegularized Linear Models  143minimumvalerror   floatinf\\nbestepoch   None\\nbestmodel   None\\nfor epoch in range1000\\n    sgdregfitXtrainpolyscaled  ytrain   continues where it left off\\n    yvalpredict   sgdregpredictXvalpolyscaled \\n    valerror   meansquarederror yval yvalpredict \\n    if valerror   minimumvalerror \\n        minimumvalerror   valerror\\n        bestepoch   epoch\\n        bestmodel   clonesgdreg\\nNote that with warmstartTrue  when the fit  method is called it just continues\\ntraining where it left off instead of restarting from scratch\\nLogistic Regression\\nAs we discussed in Chapter 1  some regression algorithms can be used for classifica\\ntion as well and vice versa Logistic Regression  also called Logit Regression  is com\\nmonly used to estimate the probability that an instance belongs to a particular class\\neg what is the probability that this email is spam If the estimated probability is\\ngreater than 50 then the model predicts that the instance belon', 'spam If the estimated probability is\\ngreater than 50 then the model predicts that the instance belongs to that class\\ncalled the positive class labeled 1 or else it predicts that it does not ie it\\nbelongs to the negative class labeled 0 This makes it a binary classifier\\nEstimating Probabilities\\nSo how does it work Just like a Linear Regression model a Logistic Regression\\nmodel computes a weighted sum of the input features plus a bias term but instead\\nof outputting the result directly like the Linear Regression model does it outputs the\\nlogistic  of this result see Equation 413 \\nEquation 413 Logistic Regression model estimated probability vectorized form\\nphxxT\\nThe logisticnoted is a sigmoid function  ie Sshaped that outputs a number\\nbetween 0 and 1 It is defined as shown in Equation 414  and Figure 421 \\nEquation 414 Logistic function\\nt1\\n1  exp t\\n144  Chapter 4 Training ModelsFigure 421 Logistic function\\nOnce the Logistic Regression model has estimated the probability p  hx that an\\ninstan', 'istic function\\nOnce the Logistic Regression model has estimated the probability p  hx that an\\ninstance x belongs to the positive class it can make its prediction  easily see Equa\\ntion 415 \\nEquation 415 Logistic Regression model prediction\\ny0 if p 0  5\\n1 if p 0  5\\nNotice that t  05 when t  0 and t  05 when t  0 so a Logistic Regression\\nmodel predicts 1 if xT  is positive and 0 if it is negative\\nThe score t is often called the logit  this name comes from the fact\\nthat the logit function defined as logit p  log p  1  p is the\\ninverse of the logistic function Indeed if you compute the logit of\\nthe estimated probability p you will find that the result is t The\\nlogit is also called the logodds  since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class\\nTraining and Cost Function\\nGood now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions But how is it trained The objective ', ' Regression model estimates probabilities and\\nmakes predictions But how is it trained The objective of training is to set the param\\neter vector  so that the model estimates high probabilities for positive instances  y \\n1 and low probabilities for negative instances  y  0 This idea is captured by the\\ncost function shown in Equation 416  for a single training instance x\\nEquation 416 Cost function of a single training instance\\nclog p if\\xa0y 1\\nlog 1 pif\\xa0y 0\\nLogistic Regression  145This cost function makes sense because  log t grows very large when t approaches\\n0 so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance On the other hand  log t is close to 0 when t is close to 1 so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance which is precisely what we want\\nThe cost', 'r a negative\\ninstance or close to 1 for a positive instance which is precisely what we want\\nThe cost function over the whole training set is simply the average cost over all train\\ning instances It can be written in a single expression as you can verify easily called \\nthe log loss  shown in Equation 417 \\nEquation 417 Logistic Regression cost function log loss\\nJ 1\\nmi 1myilogpi1 yilog1 pi\\nThe bad news is that there is no known closedform equation to compute the value of\\n that minimizes this cost function there is no equivalent of the Normal Equation\\nBut the good news is that this cost function is convex so Gradient Descent or any\\nother optimization algorithm is guaranteed to find the global minimum if the learn\\ning rate is not too large and you wait long enough The partial derivatives of the cost\\nfunction with regards to the jth model parameter j is given by Equation 418 \\nEquation 418 Logistic cost function partial derivatives\\n\\njJ1\\nm\\ni 1m\\nTxiyixji\\nThis equation looks very much like Equati', 'tic cost function partial derivatives\\n\\njJ1\\nm\\ni 1m\\nTxiyixji\\nThis equation looks very much like Equation 45  for each instance it computes the\\nprediction error and multiplies it by the jth feature value and then it computes the\\naverage over all training instances Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm Thats\\nit you now know how to train a Logistic Regression model For Stochastic GD you\\nwould of course just take one instance at a time and for Minibatch GD you would\\nuse a minibatch at a time\\nDecision Boundaries\\nLets use the iris dataset to illustrate Logistic Regression This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies IrisSetosa IrisVersicolor and IrisVirginica see Figure 422 \\n146  Chapter 4 Training Models16Photos reproduced from the corresponding Wikipedia pages IrisVirginica photo by Frank Mayfield  Crea\\ntive Commons BYSA 20  I', 'he corresponding Wikipedia pages IrisVirginica photo by Frank Mayfield  Crea\\ntive Commons BYSA 20  IrisVersicolor photo by D Gordon E Robertson  Creative Commons BYSA 30 \\nand IrisSetosa photo is public domain\\n17NumPys reshape  function allows one dimension to be 1 which means unspecified the value is inferred\\nfrom the length of the array and the remaining dimensions\\nFigure 422 Flowers of three iris plant species16\\nLets try to build a classifier to detect the IrisVirginica type based only on the petal\\nwidth feature First lets load the data\\n from sklearn import datasets\\n iris  datasets loadiris \\n listiriskeys\\ndata target targetnames DESCR featurenames filename\\n X  irisdata 3   petal width\\n y  iristarget   2astypenpint   1 if IrisVirginica else 0\\nNow lets train a Logistic Regression model\\nfrom sklearnlinearmodel  import LogisticRegression\\nlogreg  LogisticRegression \\nlogregfitX y\\nLets look at the models estimated probabilities for flowers with petal widths varying\\nfrom 0 to 3 cm  Figure 42', 't the models estimated probabilities for flowers with petal widths varying\\nfrom 0 to 3 cm  Figure 423 17\\nXnew  nplinspace 0 3 1000reshape1 1\\nyproba  logregpredictproba Xnew\\npltplotXnew yproba 1 g labelIrisVirginica \\nLogistic Regression  14718It is the the set of points x such that 0  1x1  2x2  0 which defines a straight linepltplotXnew yproba 0 b labelNot IrisVirginica \\n  more Matplotlib code to make the image look pretty\\nFigure 423 Estimated probabilities and decision boundary\\nThe petal width of IrisVirginica flowers represented by triangles ranges from 14\\ncm to 25 cm while the other iris flowers represented by squares generally have a\\nsmaller petal width ranging from 01 cm to 18 cm Notice that there is a bit of over\\nlap Above about 2 cm the classifier is highly confident that the flower is an Iris\\nVirginica it outputs a high probability to that class while below 1 cm it is highly\\nconfident that it is not an IrisVirginica high probability for the Not IrisVirginica\\nclass In between the', 'dent that it is not an IrisVirginica high probability for the Not IrisVirginica\\nclass In between these extremes the classifier is unsure However if you ask it to\\npredict the class using the predict  method rather than the predictproba\\nmethod it will return whichever class is the most likely Therefore there is a decision\\nboundary  at around 16 cm where both probabilities are equal to 50 if the petal\\nwidth is higher than 16 cm the classifier will predict that the flower is an Iris\\nVirginica or else it will predict that it is not even if it is not very confident\\n logregpredict17 15\\narray1 0\\nFigure 424  shows the same dataset but this time displaying two features petal width\\nand length Once trained the Logistic Regression classifier can estimate the probabil\\nity that a new flower is an IrisVirginica based on these two features The dashed line\\nrepresents the points where the model estimates a 50 probability this is the models\\ndecision boundary Note that it is a linear boundary18 Each parall', ' 50 probability this is the models\\ndecision boundary Note that it is a linear boundary18 Each parallel line represents the\\npoints where the model outputs a specific probability from 15 bottom left to 90\\ntop right All the flowers beyond the topright line have an over 90 chance of\\nbeing IrisVirginica according to the model\\n148  Chapter 4 Training ModelsFigure 424 Linear decision boundary\\nJust like the other linear models Logistic Regression models can be regularized using \\n1 or 2 penalties ScitkitLearn actually adds an 2 penalty by default\\nThe hyperparameter controlling the regularization strength of a\\nScikitLearn LogisticRegression  model is not alpha  as in other\\nlinear models but its inverse C The higher the value of C the less\\nthe model is regularized\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly\\nwithout having to train and combine multiple binary classifiers as discussed in\\nChapter 3  This is called Softmax  Regression  or M', 'bine multiple binary classifiers as discussed in\\nChapter 3  This is called Softmax  Regression  or Multinomial Logistic Regression \\nThe idea is quite simple when given an instance x the Softmax Regression model\\nfirst computes a score skx for each class k then estimates the probability of each\\nclass by applying the softmax  function  also called the normalized exponential  to the\\nscores The equation to compute skx should look familiar as it is just like the equa\\ntion for Linear Regression prediction see Equation 419 \\nEquation 419 Softmax  score for class k\\nskxxTk\\nNote that each class has its own dedicated parameter vector k All these vectors are\\ntypically stored as rows in a parameter matrix  \\nOnce you have computed the score of every class for the instance x you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function  Equation 420  it computes the exponential of every score\\nLogistic Regression  149then normalizes them divi', 'on 420  it computes the exponential of every score\\nLogistic Regression  149then normalizes them dividing by the sum of all the exponentials The scores are\\ngenerally called logits or logodds although they are actually unnormalized log\\nodds\\nEquation 420 Softmax  function\\npksxkexp skx\\nj 1Kexp sjx\\nK is the number of classes\\nsx is a vector containing the scores of each class for the instance x\\nsxk is the estimated probability that the instance x belongs to class k given\\nthe scores of each class for that instance\\nJust like the Logistic Regression classifier the Softmax Regression classifier predicts\\nthe class with the highest estimated probability which is simply the class with the\\nhighest score as shown in Equation 421 \\nEquation 421 Softmax  Regression classifier  prediction\\ny argmax\\nksxk argmax\\nkskx argmax\\nkkTx\\nThe argmax  operator returns the value of a variable that maximizes a function In\\nthis equation it returns the value of k that maximizes the estimated probability\\nsxk\\nThe Softmax Re', '\\nthis equation it returns the value of k that maximizes the estimated probability\\nsxk\\nThe Softmax Regression classifier predicts only one class at a time\\nie it is multiclass not multioutput so it should be used only with\\nmutually exclusive classes such as different types of plants Y ou\\ncannot use it to recognize multiple people in one picture\\nNow that you know how the model estimates probabilities and makes predictions\\nlets take a look at training The objective is to have a model that estimates a high\\nprobability for the target class and consequently a low probability for the other\\nclasses Minimizing the cost function shown in Equation 422  called the cross\\nentropy  should lead to this objective because it penalizes the model when it estimates\\na low probability for a target class Cross entropy is frequently used to measure how\\n150  Chapter 4 Training Modelswell a set of estimated class probabilities match the target classes we will use it again\\nseveral times in the following chapters\\nE', 'robabilities match the target classes we will use it again\\nseveral times in the following chapters\\nEquation 422 Cross entropy cost function\\nJ 1\\nmi 1mk 1Kykilogpki\\nyki is the target probability that the ith instance belongs to class k In general it is\\neither equal to 1 or 0 depending on whether the instance belongs to the class or\\nnot\\nNotice that when there are just two classes  K  2 this cost function is equivalent to\\nthe Logistic Regressions cost function log loss see Equation 417 \\nCross Entropy\\nCross entropy originated from information theory Suppose you want to efficiently\\ntransmit information about the weather every day If there are eight options sunny\\nrainy etc you could encode each option using 3 bits since 23  8 However if you\\nthink it will be sunny almost every day it would be much more efficient to code\\nsunny on just one bit 0 and the other seven options on 4 bits starting with a 1\\nCross entropy measures the average number of bits you actually send per option If\\nyour assumptio', 'a 1\\nCross entropy measures the average number of bits you actually send per option If\\nyour assumption about the weather is perfect cross entropy will just be equal to the\\nentropy of the weather itself ie its intrinsic unpredictability But if your assump\\ntions are wrong eg if it rains often cross entropy will be greater by an amount \\ncalled the KullbackLeibler divergence \\nThe cross entropy between two probability distributions p and q is defined as\\nHpq  xpxlogqx at least when the distributions are discrete For more\\ndetails check out this video \\nThe gradient vector of this cost function with regards to k is given by Equation\\n423 \\nEquation 423 Cross entropy gradient vector for class k\\n\\nkJ1\\nm\\ni 1m\\npkiykixi\\nNow you can compute the gradient vector for every class then use Gradient Descent\\nor any other optimization algorithm to find the parameter matrix  that minimizes\\nthe cost function\\nLogistic Regression  151Lets use Softmax Regression to classify the iris flowers into all three classes Sci', 'c Regression  151Lets use Softmax Regression to classify the iris flowers into all three classes Scikit\\nLearns LogisticRegression  uses oneversusall by default when you train it on more\\nthan two classes but you can set the multiclass  hyperparameter to multinomial\\nto switch it to Softmax Regression instead Y ou must also specify a solver that sup\\nports Softmax Regression such as the lbfgs  solver see ScikitLearns documenta\\ntion for more details It also applies 2 regularization by default which you can\\ncontrol using the hyperparameter C\\nX  irisdata 2 3   petal length petal width\\ny  iristarget \\nsoftmaxreg   LogisticRegression multiclass multinomial solverlbfgs C10\\nsoftmaxreg fitX y\\nSo the next time you find an iris with 5 cm long and 2 cm wide petals you can ask\\nyour model to tell you what type of iris it is and it will answer IrisVirginica class 2\\nwith 942 probability or IrisVersicolor with 58 probability\\n softmaxreg predict5 2\\narray2\\n softmaxreg predictproba 5 2\\narray638014896e07 57492', 'th 58 probability\\n softmaxreg predict5 2\\narray2\\n softmaxreg predictproba 5 2\\narray638014896e07 574929995e02 942506362e01\\nFigure 425  shows the resulting decision boundaries represented by the background\\ncolors Notice that the decision boundaries between any two classes are linear The\\nfigure also shows the probabilities for the IrisVersicolor class represented by the\\ncurved lines eg the line labeled with 0450 represents the 45 probability bound\\nary Notice that the model can predict a class that has an estimated probability below\\n50 For example at the point where all decision boundaries meet all classes have an\\nequal estimated probability of 33\\nFigure 425 Softmax  Regression decision boundaries\\n152  Chapter 4 Training ModelsExercises\\n1What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features\\n2Suppose the features in your training set have very different scales What algo\\nrithms might suffer from this and how What can you do about it\\n3Can Gr', 'ry different scales What algo\\nrithms might suffer from this and how What can you do about it\\n3Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model\\n4Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough\\n5Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch If you notice that the validation error consistently goes up what is\\nlikely going on How can you fix this\\n6Is it a good idea to stop Minibatch Gradient Descent immediately when the vali\\ndation error goes up\\n7Which Gradient Descent algorithm among those we discussed will reach the\\nvicinity of the optimal solution the fastest Which will actually converge How\\ncan you make the others converge as well\\n8Suppose you are using Polynomial Regression Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror What is happening What are three ways to solve this\\n9Suppose', 'training error and the validation\\nerror What is happening What are three ways to solve this\\n9Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high Would you say that the\\nmodel suffers from high bias or high variance Should you increase the regulari\\nzation hyperparameter  or reduce it\\n10Why would you want to use\\nRidge Regression instead of plain Linear Regression ie without any regulari\\nzation\\nLasso instead of Ridge Regression\\nElastic Net instead of Lasso\\n11Suppose you want to classify pictures as outdoorindoor and daytimenighttime\\nShould you implement two Logistic Regression classifiers or one Softmax Regres\\nsion classifier\\n12Implement Batch Gradient Descent with early stopping for Softmax Regression \\nwithout using ScikitLearn\\nSolutions to these exercises are available in \\nExercises  153CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and une', 'ctor Machines\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 5 in the final\\nrelease of the book\\nA Support Vector Machine  SVM is a very powerful and versatile Machine Learning\\nmodel capable of performing linear or nonlinear classification regression and even\\noutlier detection It is one of the most popular models in Machine Learning and any\\none interested in Machine Learning should have it in their toolbox SVMs are partic\\nularly well suited for classification of complex but small or mediumsized datasets\\nThis chapter will explain the core concepts of SVMs how to use them and how they\\nwork\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures Figure 51\\nshows part of the iris dataset that was introduced at the end of Chapter 4  The two\\nclasses can clear', 'hows part of the iris dataset that was introduced at the end of Chapter 4  The two\\nclasses can clearly be separated easily with a straight line they are linearly separable \\nThe left plot shows the decision boundaries of three possible linear classifiers The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly The other two models work perfectly on\\nthis training set but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances In contrast the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi\\nfier this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible Y ou can think of an SVM classifier as fitting the\\n155widest possible street represented by the parallel dashed lines between the classes\\nThis is called large margin classification \\nFigure 51 Large marg', 'el dashed lines between the classes\\nThis is called large margin classification \\nFigure 51 Large margin classification\\nNotice that adding more training instances off the street will not affect the decision\\nboundary at all it is fully determined or supported by the instances located on the\\nedge of the street These instances are called the support vectors  they are circled in\\nFigure 51 \\nSVMs are sensitive to the feature scales as you can see in\\nFigure 52  on the left plot the vertical scale is much larger than the\\nhorizontal scale so the widest possible street is close to horizontal\\nAfter feature scaling eg using ScikitLearns StandardScaler  \\nthe decision boundary looks much better on the right plot\\nFigure 52 Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side this is\\ncalled hard margin classification  There are two main issues with hard margin classifi\\ncation First it only works if the data is linearly ', 're are two main issues with hard margin classifi\\ncation First it only works if the data is linearly separable and second it is quite sensi\\ntive to outliers Figure 53  shows the iris dataset with just one additional outlier on\\nthe left it is impossible to find a hard margin and on the right the decision boundary\\nends up very different from the one we saw in Figure 51  without the outlier and it\\nwill probably not generalize as well\\n156  Chapter 5 Support Vector MachinesFigure 53 Hard margin sensitivity to outliers\\nTo avoid these issues it is preferable to use a more flexible model The objective is to\\nfind a good balance between keeping the street as large as possible and limiting the\\nmargin violations  ie instances that end up in the middle of the street or even on the\\nwrong side This is called soft margin classification \\nIn ScikitLearns SVM classes you can control this balance using the C hyperparame\\nter a smaller C value leads to a wider street but more margin violations Figure 54\\nshow', ' hyperparame\\nter a smaller C value leads to a wider street but more margin violations Figure 54\\nshows the decision boundaries and margins of two soft margin SVM classifiers on a\\nnonlinearly separable dataset On the left using a low C value the margin is quite\\nlarge but many instances end up on the street On the right using a high C value the\\nclassifier makes fewer margin violations but ends up with a smaller margin However\\nit seems likely that the first classifier will generalize better in fact even on this train\\ning set it makes fewer prediction errors since most of the margin violations are\\nactually on the correct side of the decision boundary\\nFigure 54 Large margin left  versus fewer margin violations right\\nIf your SVM model is overfitting you can try regularizing it by\\nreducing C\\nThe following ScikitLearn code loads the iris dataset scales the features and then\\ntrains a linear SVM model using the LinearSVC  class with C  1 and the hinge loss\\nfunction described shortly to detect Iri', 'del using the LinearSVC  class with C  1 and the hinge loss\\nfunction described shortly to detect IrisVirginica flowers The resulting model is\\nrepresented on the left of Figure 54 \\nLinear SVM Classification   157import numpy as np\\nfrom sklearn import datasets\\nfrom sklearnpipeline  import Pipeline\\nfrom sklearnpreprocessing  import StandardScaler\\nfrom sklearnsvm  import LinearSVC\\niris  datasets loadiris \\nX  irisdata 2 3   petal length petal width\\ny  iristarget   2astypenpfloat64   IrisVirginica\\nsvmclf  Pipeline \\n        scaler  StandardScaler \\n        linearsvc  LinearSVC C1 losshinge\\n    \\nsvmclffitX y\\nThen as usual you can use the model to make predictions\\n svmclfpredict55 17\\narray1\\nUnlike Logistic Regression classifiers SVM classifiers do not out\\nput probabilities for each class\\nAlternatively you could use the SVC class using SVCkernellinear C1  but it\\nis much slower especially with large training sets so it is not recommended Another\\noption is to use the SGDClassifier  class with SGDCl', ' training sets so it is not recommended Another\\noption is to use the SGDClassifier  class with SGDClassifierlosshinge\\nalpha1mC  This applies regular Stochastic Gradient Descent see Chapter 4  to\\ntrain a linear SVM classifier It does not converge as fast as the LinearSVC  class but it\\ncan be useful to handle huge datasets that do not fit in memory outofcore train\\ning or to handle online classification tasks\\nThe LinearSVC  class regularizes the bias term so you should center\\nthe training set first by subtracting its mean This is automatic if\\nyou scale the data using the StandardScaler  Moreover make sure\\nyou set the loss  hyperparameter to hinge  as it is not the default\\nvalue Finally for better performance you should set the dual\\nhyperparameter to False  unless there are more features than\\ntraining instances we will discuss duality later in the chapter\\n158  Chapter 5 Support Vector MachinesNonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly w', 'esNonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases many datasets are not even close to being linearly separable One approach to\\nhandling nonlinear datasets is to add more features such as polynomial features as\\nyou did in Chapter 4  in some cases this can result in a linearly separable dataset\\nConsider the left plot in Figure 55  it represents a simple dataset with just one feature\\nx1 This dataset is not linearly separable as you can see But if you add a second fea\\nture x2  x12 the resulting 2D dataset is perfectly linearly separable\\nFigure 55 Adding features to make a dataset linearly separable\\nTo implement this idea using ScikitLearn you can create a Pipeline  containing a\\nPolynomialFeatures  transformer discussed in Polynomial Regression on page\\n130 followed by a StandardScaler  and a LinearSVC  Lets test this on the moons\\ndataset this is a toy dataset for binary classification in which the data points are sha\\nped as', 'oons\\ndataset this is a toy dataset for binary classification in which the data points are sha\\nped as two interleaving half circles see Figure 56  Y ou can generate this dataset\\nusing the makemoons  function\\nfrom sklearndatasets  import makemoons\\nfrom sklearnpipeline  import Pipeline\\nfrom sklearnpreprocessing  import PolynomialFeatures\\npolynomialsvmclf   Pipeline \\n        polyfeatures  PolynomialFeatures degree3\\n        scaler  StandardScaler \\n        svmclf  LinearSVC C10 losshinge\\n    \\npolynomialsvmclf fitX y\\nNonlinear SVM Classification   159Figure 56 Linear SVM classifier  using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms not just SVMs but at a low polynomial degree it\\ncannot deal with very complex datasets and with a high polynomial degree it creates\\na huge number of features making the model too slow\\nFortunately when using SVMs you can apply an almost miraculous mathematica', 'making the model too slow\\nFortunately when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick  it is explained in a moment It makes it possible to\\nget the same result as if you added many polynomial features even with very high\\ndegree polynomials without actually having to add them So there is no combinato\\nrial explosion of the number of features since you dont actually add any features This\\ntrick is implemented by the SVC class Lets test it on the moons dataset\\nfrom sklearnsvm  import SVC\\npolykernelsvmclf   Pipeline \\n        scaler  StandardScaler \\n        svmclf  SVCkernelpoly degree3 coef01 C5\\n    \\npolykernelsvmclf fitX y\\nThis code trains an SVM classifier using a 3rddegree polynomial kernel It is repre\\nsented on the left of Figure 57  On the right is another SVM classifier using a 10th\\ndegree polynomial kernel Obviously if your model is overfitting you might want to\\n160  Chapter 5 Support Vector Machinesreduce the polynomial degree Converse', 'itting you might want to\\n160  Chapter 5 Support Vector Machinesreduce the polynomial degree Conversely if it is underfitting you can try increasing\\nit The hyperparameter coef0  controls how much the model is influenced by high\\ndegree polynomials versus lowdegree polynomials\\nFigure 57 SVM classifiers  with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search see Chapter 2  It is often faster to first do a very\\ncoarse grid search then a finer grid search around the best values\\nfound Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame\\nter space\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark  For example lets take the onedimensional dataset discussed earlier and\\nadd two landmarks to it at x1  2 and x1  1 see the left plot ', 'ensional dataset discussed earlier and\\nadd two landmarks to it at x1  2 and x1  1 see the left plot in Figure 58  Next\\nlets define the similarity function to be the Gaussian Radial Basis Function  RBF \\nwith   03 see Equation 51 \\nEquation 51 Gaussian RBF\\nx  exp x 2\\nIt is a bellshaped function varying from 0 very far away from the landmark to 1 at\\nthe landmark Now we are ready to compute the new features For example lets look\\nat the instance x1  1 it is located at a distance of 1 from the first landmark and 2\\nfrom the second landmark Therefore its new features are x2  exp 03  12  074\\nand x3  exp 03  22  030 The plot on the right of Figure 58  shows the trans\\nformed dataset dropping the original features As you can see it is now linearly\\nseparable\\nNonlinear SVM Classification   161Figure 58 Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset This creat', 'pproach is to create a\\nlandmark at the location of each and every instance in the dataset This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features assuming you\\ndrop the original features If your training set is very large you end up with an\\nequally large number of features\\nGaussian RBF Kernel\\nJust like the polynomial features method the similarity features method can be useful\\nwith any Machine Learning algorithm but it may be computationally expensive to\\ncompute all the additional features especially on large training sets However once\\nagain the kernel trick does its SVM magic it makes it possible to obtain a similar\\nresult as if you had added many similarity features without actually having to add\\nthem Lets try the Gaussian RBF kernel using the SVC class\\nrbfkernelsvmclf   Pipeline \\n     ', 'g to add\\nthem Lets try the Gaussian RBF kernel using the SVC class\\nrbfkernelsvmclf   Pipeline \\n        scaler  StandardScaler \\n        svmclf  SVCkernelrbf gamma5 C0001\\n    \\nrbfkernelsvmclf fitX y\\nThis model is represented on the bottom left of Figure 59  The other plots show\\nmodels trained with different values of hyperparameters gamma   and C Increasing\\ngamma  makes the bellshape curve narrower see the left plot of Figure 58  and as a\\nresult each instances range of influence is smaller the decision boundary ends up\\nbeing more irregular wiggling around individual instances Conversely a small gamma  \\nvalue makes the bellshaped curve wider so instances have a larger range of influ\\nence and the decision boundary ends up smoother So  acts like a regularization\\nhyperparameter if your model is overfitting you should reduce it and if it is under\\nfitting you should increase it similar to the C hyperparameter\\n162  Chapter 5 Support Vector Machines1 A Dual Coordinate Descent Method for Largesca', 'hyperparameter\\n162  Chapter 5 Support Vector Machines1 A Dual Coordinate Descent Method for Largescale Linear SVM  Lin et al 2008\\nFigure 59 SVM classifiers  using an RBF kernel\\nOther kernels exist but are used much more rarely For example some kernels are\\nspecialized for specific data structures String kernels  are sometimes used when classi\\nfying text documents or DNA sequences eg using the string subsequence kernel  or\\nkernels based on the Levenshtein distance \\nWith so many kernels to choose from how can you decide which\\none to use As a rule of thumb you should always try the linear\\nkernel first remember that LinearSVC  is much faster than SVCker\\nnellinear  especially if the training set is very large or if it\\nhas plenty of features If the training set is not too large you should\\ntry the Gaussian RBF kernel as well it works well in most cases\\nThen if you have spare time and computing power you can also\\nexperiment with a few other kernels using crossvalidation and grid\\nsearch especial', 'ower you can also\\nexperiment with a few other kernels using crossvalidation and grid\\nsearch especially if there are kernels specialized for your training\\nsets data structure\\nComputational Complexity\\nThe LinearSVC  class is based on the liblinear  library which implements an optimized\\nalgorithm  for linear SVMs1 It does not support the kernel trick but it scales almost\\nNonlinear SVM Classification   1632Sequential Minimal Optimization SMO  J Platt 1998linearly with the number of training instances and the number of features its training\\ntime complexity is roughly Om  n\\nThe algorithm takes longer if you require a very high precision This is controlled by\\nthe tolerance hyperparameter  called tol in ScikitLearn In most classification\\ntasks the default tolerance is fine\\nThe SVC class is based on the libsvm  library which implements an algorithm  that sup\\nports the kernel trick2 The training time complexity is usually between Om2  n\\nand Om3  n Unfortunately this means that it gets dreadfully', 'ime complexity is usually between Om2  n\\nand Om3  n Unfortunately this means that it gets dreadfully slow when the num\\nber of training instances gets large eg hundreds of thousands of instances This\\nalgorithm is perfect for complex but small or medium training sets However it scales\\nwell with the number of features especially with sparse features  ie when each\\ninstance has few nonzero features In this case the algorithm scales roughly with the\\naverage number of nonzero features per instance Table 51  compares ScikitLearns\\nSVM classification classes\\nTable 51 Comparison of ScikitLearn classes for SVM classification\\nClass Time complexity Outofcore support Scaling required Kernel trick\\nLinearSVC Om  n No Yes No\\nSGDClassifier Om  n Yes Yes No\\nSVC Om  n to O m  nNo Yes Yes\\nSVM Regression\\nAs we mentioned earlier the SVM algorithm is quite versatile not only does it sup\\nport linear and nonlinear classification but it also supports linear and nonlinear\\nregression The trick is to reverse the obj', ' classification but it also supports linear and nonlinear\\nregression The trick is to reverse the objective instead of trying to fit the largest pos\\nsible street between two classes while limiting margin violations SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\nie instances off the street The width of the street is controlled by a hyperparame\\nter  Figure 510  shows two linear SVM Regression models trained on some random\\nlinear data one with a large margin    15 and the other with a small margin   \\n05\\n164  Chapter 5 Support Vector MachinesFigure 510 SVM Regression\\nAdding more training instances within the margin does not affect the models predic\\ntions thus the model is said to be insensitive \\nY ou can use ScikitLearns LinearSVR  class to perform linear SVM Regression The\\nfollowing code produces the model represented on the left of Figure 510  the train\\ning data should be scaled and centered first\\nfrom sklearnsvm  import LinearSVR\\n', 'igure 510  the train\\ning data should be scaled and centered first\\nfrom sklearnsvm  import LinearSVR\\nsvmreg  LinearSVR epsilon15\\nsvmregfitX y\\nTo tackle nonlinear regression tasks you can use a kernelized SVM model For exam\\nple Figure 511  shows SVM Regression on a random quadratic training set using a\\n2nddegree polynomial kernel There is little regularization on the left plot ie a large\\nC value and much more regularization on the right plot ie a small C value\\nFigure 511 SVM regression using a 2nddegree polynomial kernel\\nSVM Regression  165The following code produces the model represented on the left of Figure 511  using\\nScikitLearns SVR class which supports the kernel trick The SVR class is the regres\\nsion equivalent of the SVC class and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class The LinearSVR  class scales linearly with the size of the train\\ning set just like the LinearSVC  class while the SVR class gets much too slow when\\nthe training set grows large jus', 'ke the LinearSVC  class while the SVR class gets much too slow when\\nthe training set grows large just like the SVC class\\nfrom sklearnsvm  import SVR\\nsvmpolyreg   SVRkernelpoly degree2 C100 epsilon01\\nsvmpolyreg fitX y\\nSVMs can also be used for outlier detection see ScikitLearns doc\\numentation for more details\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork starting with linear SVM classifiers Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn\\ning and come back later when you want to get a deeper understanding of SVMs\\nFirst a word about notations in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector  including the bias term 0 and the input feature\\nweights 1 to n and adding a bias input x0  1 to all instances In this chapter we will\\nuse a different convention which is more convenient and more common when you\\nare dealing', 'ter we will\\nuse a different convention which is more convenient and more common when you\\nare dealing with SVMs the bias term will be called b and the feature weights vector\\nwill be called w No bias feature will be added to the input feature vectors\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com\\nputing the decision function wT x  b  w1 x1    wn xn  b if the result is positive\\nthe predicted class  is the positive class 1 or else it is the negative class 0 see\\nEquation 52 \\nEquation 52 Linear SVM classifier  prediction\\ny0 if wTxb 0\\n1 if wTxb 0\\n166  Chapter 5 Support Vector Machines3More generally when there are n features the decision function is an ndimensional hyperplane  and the deci\\nsion boundary is an  n  1dimensional hyperplaneFigure 512  shows the decision function that corresponds to the model on the left of\\nFigure 54  it is a twodimensional plane since this dataset has two features petal\\nwidth and petal length', 're 54  it is a twodimensional plane since this dataset has two features petal\\nwidth and petal length The decision boundary is the set of points where the decision\\nfunction is equal to 0 it is the intersection of two planes which is a straight line rep\\nresented by the thick solid line3\\nFigure 512 Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or 1\\nthey are parallel and at equal distance to the decision boundary forming a margin\\naround it Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations hard margin\\nor limiting them soft margin\\nTraining Objective\\nConsider the slope of the decision function it is equal to the norm of the weight vec\\ntor  w  If we divide this slope by 2 the points where the decision function is equal\\nto 1 are going to be twice as far away from the decision boundary In other words\\ndividing the slope by 2 will mul', 'g to be twice as far away from the decision boundary In other words\\ndividing the slope by 2 will multiply the margin by 2 Perhaps this is easier to visual\\nize in 2D in Figure 513  The smaller the weight vector w the larger the margin\\nUnder the Hood  1674Zeta   is the 6th letter of the Greek alphabet\\nFigure 513 A smaller weight vector results in a larger margin\\nSo we want to minimize  w  to get a large margin However if we also want to avoid\\nany margin violation hard margin then we need the decision function to be greater\\nthan 1 for all positive training instances and lower than 1 for negative training\\ninstances If we define ti  1 for negative instances if yi  0 and ti  1 for positive\\ninstances if yi  1 then we can express this constraint as tiwT xi  b  1 for all\\ninstances\\nWe can therefore express the hard margin linear SVM classifier objective as the con\\nstrained optimization  problem in Equation 53 \\nEquation 53 Hard margin linear SVM classifier  objective\\nminimizewb1\\n2wTw\\nsubject to t', ' Equation 53 \\nEquation 53 Hard margin linear SVM classifier  objective\\nminimizewb1\\n2wTw\\nsubject to tiwTxib 1 for i 1 2m\\nWe are minimizing 1\\n2wT w which is equal to 1\\n2 w 2 rather than\\nminimizing  w  Indeed 1\\n2 w 2 has a nice and simple derivative\\nit is just w while  w  is not differentiable at w  0 Optimization\\nalgorithms work much better on differentiable functions\\nTo get the soft margin objective we need to introduce a slack variable  i  0 for each\\ninstance4 i measures how much the ith instance is allowed to violate the margin We\\nnow have two conflicting objectives making the slack variables as small as possible to\\nreduce the margin violations and making 1\\n2wT w as small as possible to increase the\\nmargin This is where the C hyperparameter comes in it allows us to define the trade\\n168  Chapter 5 Support Vector Machines5To learn more about Quadratic Programming you can start by reading Stephen Boyd and Lieven Vanden\\nberghe Convex Optimization  Cambridge UK Cambridge University Press 2', 'Stephen Boyd and Lieven Vanden\\nberghe Convex Optimization  Cambridge UK Cambridge University Press 2004 or watch Richard Browns\\nseries of video lectures off between these two objectives This gives us the constrained optimization problem\\nin Equation 54 \\nEquation 54 Soft margin linear SVM classifier  objective\\nminimizewb1\\n2wTwC\\ni 1m\\ni\\nsubject to tiwTxib 1  iand i 0 for i 1 2m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints Such problems are known as Quadratic Program\\nming  QP problems Many offtheshelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book5 The general\\nproblem formulation is given by Equation 55 \\nEquation 55 Quadratic Programming problem\\nMinimize\\np1\\n2pTHp  fTp\\nsubject to Apb\\nwherepis an npdimensional vector  np number of parameters\\nHis an npnpmatrix\\nfis an npdimensional vector\\nAis an ncnpmatrix  nc number of constraints\\nbis an nc', '\\nHis an npnpmatrix\\nfis an npdimensional vector\\nAis an ncnpmatrix  nc number of constraints\\nbis an ncdimensional vector\\nNote that the expression A p  b actually defines nc constraints pT ai  bi for i  1\\n2  nc where ai is the vector containing the elements of the ith row of A and bi is\\nthe ith element of b\\nY ou can easily verify that if you set the QP parameters in the following way you get\\nthe hard margin linear SVM classifier objective\\nnp  n  1 where n is the number of features the 1 is for the bias term\\nUnder the Hood  1696The objective function is convex and the inequality constraints are continuously differentiable and convex\\nfunctionsnc  m where m is the number of training instances\\nH is the np  np identity matrix except with a zero in the topleft cell to ignore\\nthe bias term\\nf  0 an npdimensional vector full of 0s\\nb  1 an ncdimensional vector full of 1s\\nai  ti x i where x i is equal to xi with an extra bias feature x 0  1\\nSo one way to train a hard margin linear SVM classifier is ', ' to xi with an extra bias feature x 0  1\\nSo one way to train a hard margin linear SVM classifier is just to use an offtheshelf\\nQP solver by passing it the preceding parameters The resulting vector p will contain\\nthe bias term b  p0 and the feature weights wi  pi for i  1 2  n Similarly you\\ncan use a QP solver to solve the soft margin problem see the exercises at the end of\\nthe chapter\\nHowever to use the kernel trick we are going to look at a different constrained opti\\nmization problem\\nThe Dual Problem\\nGiven a constrained optimization problem known as the primal problem  it is possi\\nble to express a different but closely related problem called its dual problem  The sol\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem but under some conditions it can even have the same solutions as the pri\\nmal problem Luckily the SVM problem happens to meet these conditions6 so you\\ncan choose to solve the primal problem or the dual problem both will have the s', 'e conditions6 so you\\ncan choose to solve the primal problem or the dual problem both will have the same\\nsolution Equation 56  shows the dual form of the linear SVM objective if you are\\ninterested in knowing how to derive the dual problem from the primal problem\\nsee \\nEquation 56 Dual form of the linear SVM objective\\nminimize1\\n2\\ni 1m\\n\\nj 1m\\nijtitjxiTxj\\ni 1m\\ni\\nsubject to i 0 for i 1 2m\\n170  Chapter 5 Support Vector Machines7As explained in Chapter 4  the dot product of two vectors a and b is normally noted a  b However in\\nMachine Learning vectors are frequently represented as column vectors ie singlecolumn matrices so the\\ndot product is achieved by computing aTb To remain consistent with the rest of the book we will use this\\nnotation here ignoring the fact that this technically results in a singlecell matrix rather than a scalar valueOnce you find the vector  that minimizes this equation using a QP solver you can\\ncompute w and b that minimize the primal problem by using Equation 57 \\nEquati', 'ng a QP solver you can\\ncompute w and b that minimize the primal problem by using Equation 57 \\nEquation 57 From the dual solution to the primal solution\\nw\\ni 1m\\nitixi\\nb1\\nns\\ni 1\\ni 0m\\ntiwTxi\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features More importantly it makes the ker\\nnel trick possible while the primal does not So what is this kernel trick anyway\\nKernelized SVM\\nSuppose you want to apply a 2nddegree polynomial transformation to a two\\ndimensional training set such as the moons training set then train a linear SVM\\nclassifier on the transformed training set Equation 58  shows the 2nddegree polyno\\nmial mapping function  that you want to apply\\nEquation 58 Seconddegree polynomial mapping\\nxx1\\nx2x12\\n2x1x2\\nx22\\nNotice that the transformed vector is threedimensional instead of twodimensional\\nNow lets look at what happens to a couple of twodimensional vectors a and b if we\\napply this 2nddegree polynomial mapping and t', 'ns to a couple of twodimensional vectors a and b if we\\napply this 2nddegree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors See Equation 59 \\nUnder the Hood  171Equation 59 Kernel trick for a 2nddegree polynomial mapping\\naTb a12\\n2a1a2\\na22Tb12\\n2b1b2\\nb22a12b12 2a1b1a2b2a22b22\\na1b1a2b22a1\\na2Tb1\\nb22\\naTb2\\nHow about that The dot product of the transformed vectors is equal to the square of\\nthe dot product of the original vectors aT b   aT b2\\nNow here is the key insight if you apply the transformation  to all training instan\\nces then the dual problem see Equation 56  will contain the dot product xiT\\nxj But if  is the 2nddegree polynomial transformation defined in Equation 58 \\nthen you can replace this dot product of transformed vectors simply by xiTxj2\\n So\\nyou dont actually need to transform the training instances at all just replace the dot\\nproduct by its square in Equation 56  The result will be strictly the same as if you\\nwent through the trouble of actually t', 'n Equation 56  The result will be strictly the same as if you\\nwent through the trouble of actually transforming the training set then fitting a linear\\nSVM algorithm but this trick makes the whole process much more computationally\\nefficient This is the essence of the kernel trick\\nThe function Ka b   aT b2 is called a 2nddegree polynomial kernel  In Machine\\nLearning a kernel  is a function capable of computing the dot product aT b\\nbased only on the original vectors a and b without having to compute or even to\\nknow about the transformation  Equation 510  lists some of the most commonly\\nused kernels\\nEquation 510 Common kernels\\nLinear KabaTb\\nPolynomial KabaTbrd\\nGaussian RBF Kab exp ab2\\nSigmoid Kab tanh aTbr\\n172  Chapter 5 Support Vector MachinesMercers Theorem\\nAccording to Mercers theorem  if a function Ka b respects a few mathematical con\\nditions called Mercers conditions  K must be continuous symmetric in its arguments\\nso Ka b  Kb a etc then there exists a function  that maps a and b into', 'us symmetric in its arguments\\nso Ka b  Kb a etc then there exists a function  that maps a and b into\\nanother space possibly with much higher dimensions such that Ka b  aT b\\nSo you can use K as a kernel since you know  exists even if you dont know what \\nis In the case of the Gaussian RBF kernel it can be shown that  actually maps each\\ntraining instance to an infinitedimensional space so its a good thing you dont need\\nto actually perform the mapping\\nNote that some frequently used kernels such as the Sigmoid kernel dont respect all\\nof Mercers conditions yet they generally work well in practice\\nThere is still one loose end we must tie Equation 57  shows how to go from the dual\\nsolution to the primal solution in the case of a linear SVM classifier but if you apply\\nthe kernel trick you end up with equations that include xi In fact w must have\\nthe same number of dimensions as xi which may be huge or even infinite so you\\ncant compute it But how can you make predictions without knowing w Well t', 'ge or even infinite so you\\ncant compute it But how can you make predictions without knowing w Well the\\ngood news is that you can plug in the formula for w from Equation 57  into the deci\\nsion function for a new instance xn and you get an equation with only dot products\\nbetween input vectors This makes it possible to use the kernel trick once again\\nEquation 511 \\nEquation 511 Making predictions with a kernelized SVM\\nhwbxnwTxnb\\ni 1m\\nitixiT\\nxnb\\n\\ni 1m\\nitixiTxnb\\n\\ni 1\\ni 0m\\nitiKxixnb\\nNote that since i  0 only for support vectors making predictions involves comput\\ning the dot product of the new input vector xn with only the support vectors not all\\nthe training instances Of course you also need to compute the bias term b using the\\nsame trick  Equation 512 \\nUnder the Hood  173Equation 512 Computing the bias term using the kernel trick\\nb1\\nns\\ni 1\\ni 0m\\ntiwTxi1\\nns\\ni 1\\ni 0m\\nti\\nj 1m\\njtjxjT\\nxi\\n1\\nns\\ni 1\\ni 0m\\nti\\nj 1\\nj 0m\\njtjKxixj\\nIf you are starting to get a headache its perfectly normal its an unfortunat', '0m\\nti\\nj 1\\nj 0m\\njtjKxixj\\nIf you are starting to get a headache its perfectly normal its an unfortunate side\\neffect of the kernel trick\\nOnline SVMs\\nBefore concluding this chapter lets take a quick look at online SVM classifiers recall\\nthat online learning means learning incrementally typically as new instances arrive\\nFor linear SVM classifiers one method is to use Gradient Descent eg using\\nSGDClassifier  to minimize the cost function in Equation 513  which is derived\\nfrom the primal problem Unfortunately it converges much more slowly than the\\nmethods based on QP \\nEquation 513 Linear SVM classifier  cost function\\nJwb1\\n2wTw  C\\ni 1m\\nmax 0 1  tiwTxib\\nThe first sum in the cost function will push the model to have a small weight vector\\nw leading to a larger margin The second sum computes the total of all margin viola\\ntions An instances margin violation is equal to 0 if it is located off the street and on\\nthe correct side or else it is proportional to the distance to the correct side of the\\nstr', 'et and on\\nthe correct side or else it is proportional to the distance to the correct side of the\\nstreet Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max 0 1  t is called the hinge loss  function represented below It is\\nequal to 0 when t  1 Its derivative slope is equal to 1 if t  1 and 0 if t  1 It is not\\ndifferentiable at t  1 but just like for Lasso Regression see Lasso Regression  on\\npage 139 you can still use Gradient Descent using any subderivative  at t  1 ie any\\nvalue between 1 and 0\\n174  Chapter 5 Support Vector Machines8Incremental and Decremental Support Vector Machine Learning  G Cauwenberghs T Poggio 2001\\n9Fast Kernel Classifiers with Online and Active Learning A Bordes S Ertekin J Weston L Bottou 2005\\nIt is also possible to implement online kernelized SVMsfor example using Incre\\nmental and Decremental SVM Learning8 or Fast Kernel Classifiers with Online and\\nActive Learning 9 However these ar', 'remental SVM Learning8 or Fast Kernel Classifiers with Online and\\nActive Learning 9 However these are implemented in Matlab and C For large\\nscale nonlinear problems you may want to consider using neural networks instead \\nsee Part II \\nExercises\\n1What is the fundamental idea behind Support Vector Machines\\n2What is a support vector\\n3Why is it important to scale the inputs when using SVMs\\n4Can an SVM classifier output a confidence score when it classifies an instance\\nWhat about a probability\\n5Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features\\n6Say you trained an SVM classifier with an RBF kernel It seems to underfit the\\ntraining set should you increase or decrease  gamma  What about C\\n7How should you set the QP parameters  H f A and b to solve the soft margin\\nlinear SVM classifier problem using an offtheshelf QP solver\\n8Train a LinearSVC  on a linearly separable dataset Then train an SVC and a', 'an offtheshelf QP solver\\n8Train a LinearSVC  on a linearly separable dataset Then train an SVC and a\\nSGDClassifier  on the same dataset See if you can get them to produce roughly\\nthe same model\\n9Train an SVM classifier on the MNIST dataset Since SVM classifiers are binary\\nclassifiers you will need to use oneversusall to classify all 10 digits Y ou may\\nExercises  175want to tune the hyperparameters using small validation sets to speed up the pro\\ncess What accuracy can you reach\\n10Train an SVM regressor on the California housing dataset\\nSolutions to these exercises are available in \\n176  Chapter 5 Support Vector MachinesCHAPTER 6\\nDecision Trees\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 6 in the final\\nrelease of the book\\nLike SVMs Decision Trees  are versatile Machine Learning algorithms ', 'n the final\\nrelease of the book\\nLike SVMs Decision Trees  are versatile Machine Learning algorithms that can per\\nform both classification and regression tasks and even multioutput tasks They are\\nvery powerful algorithms capable of fitting complex datasets For example in Chap\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset\\nfitting it perfectly actually overfitting it\\nDecision Trees are also the fundamental components of Random Forests see Chap\\nter 7  which are among the most powerful Machine Learning algorithms available\\ntoday\\nIn this chapter we will start by discussing how to train visualize and make predic\\ntions with Decision Trees Then we will go through the CART training algorithm\\nused by ScikitLearn and we will discuss how to regularize trees and use them for\\nregression tasks Finally we will discuss some of the limitations of Decision Trees\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees lets just build one and take a look a', 'g and Visualizing a Decision Tree\\nTo understand Decision Trees lets just build one and take a look at how it makes pre\\ndictions The following code trains a DecisionTreeClassifier  on the iris dataset\\nsee Chapter 4 \\nfrom sklearndatasets  import loadiris\\nfrom sklearntree  import DecisionTreeClassifier\\n1771Graphviz is an open source graph visualization software package available at httpwwwgraphvizorg iris  loadiris \\nX  irisdata 2  petal length and width\\ny  iristarget\\ntreeclf   DecisionTreeClassifier maxdepth 2\\ntreeclf fitX y\\nY ou can visualize the trained Decision Tree by first using the exportgraphviz  \\nmethod to output a graph definition file called iristreedot \\nfrom sklearntree  import exportgraphviz\\nexportgraphviz \\n        treeclf \\n        outfile imagepath iristreedot \\n        featurenames irisfeaturenames 2\\n        classnames iristargetnames \\n        roundedTrue\\n        filledTrue\\n    \\nThen you can convert this dot file to a variety of formats such as PDF or PNG using\\nthe dot comman', ' \\nThen you can convert this dot file to a variety of formats such as PDF or PNG using\\nthe dot commandline tool from the graphviz  package1 This command line converts\\nthe dot file to a png image file\\n dot Tpng iristreedot o iristreepng\\nY our first decision tree looks like Figure 61 \\n178  Chapter 6 Decision TreesFigure 61 Iris Decision Tree\\nMaking Predictions\\nLets see how the tree represented in Figure 61  makes predictions Suppose you find\\nan iris flower and you want to classify it Y ou start at the root node  depth 0 at the\\ntop this node asks whether the flowers petal length is smaller than 245 cm If it is\\nthen you move down to the roots left child node depth 1 left In this case it is a leaf\\nnode  ie it does not have any children nodes so it does not ask any questions you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an IrisSetosa  classsetosa \\nNow suppose you find another flower but this time the petal length is greater than\\n24', '  classsetosa \\nNow suppose you find another flower but this time the petal length is greater than\\n245 cm Y ou must move down to the roots right child node depth 1 right which is\\nnot a leaf node so it asks another question is the petal width smaller than 175 cm If\\nit is then your flower is most likely an IrisVersicolor depth 2 left If not it is likely\\nan IrisVirginica depth 2 right Its really that simple\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation In particular they dont require feature\\nscaling or centering at all\\nMaking Predictions  179A nodes samples  attribute counts how many training instances it applies to For\\nexample 100 training instances have a petal length greater than 245 cm depth 1\\nright among which 54 have a petal width smaller than 175 cm depth 2 left A\\nnodes value  attribute tells you how many training instances of each class this node\\napplies to for example the bottomright node applies to 0 IrisSetosa 1 Iris\\nVersicolor and', ' this node\\napplies to for example the bottomright node applies to 0 IrisSetosa 1 Iris\\nVersicolor and 45 IrisVirginica Finally a nodes gini  attribute measures its impur\\nity a node is pure  gini0  if all training instances it applies to belong to the same\\nclass For example since the depth1 left node applies only to IrisSetosa training\\ninstances it is pure and its gini  score is 0 Equation 61  shows how the training algo\\nrithm computes the gini score Gi of the ith node For example the depth2 left node\\nhas a gini  score equal to 1  0542  49542  5542  0168 Another impurity\\nmeasure  is discussed shortly\\nEquation 61 Gini impurity\\nGi 1  \\nk 1n\\npik2\\npik is the ratio of class k instances among the training instances in the ith node\\nScikitLearn uses the CART algorithm which produces only binary\\ntrees  nonleaf nodes always have two children ie questions only\\nhave yesno answers However other algorithms such as ID3 can\\nproduce Decision Trees with nodes that have more than two chil\\ndren\\nFigure 62  sh', 'ms such as ID3 can\\nproduce Decision Trees with nodes that have more than two chil\\ndren\\nFigure 62  shows this Decision Trees decision boundaries The thick vertical line rep\\nresents the decision boundary of the root node depth 0 petal length  245 cm\\nSince the left area is pure only IrisSetosa it cannot be split any further However\\nthe right area is impure so the depth1 right node splits it at petal width  175 cm\\nrepresented by the dashed line Since maxdepth  was set to 2 the Decision Tree\\nstops right there However if you set maxdepth  to 3 then the two depth2 nodes\\nwould each add another decision boundary represented by the dotted lines\\n180  Chapter 6 Decision TreesFigure 62 Decision Tree decision boundaries\\nModel Interpretation White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter\\npret Such models are often called white box models  In contrast as we will see Ran\\ndom Forests or neural networks are generally considered black bo', 'els  In contrast as we will see Ran\\ndom Forests or neural networks are generally considered black box models  They\\nmake great predictions and you can easily check the calculations that they performed\\nto make these predictions nevertheless it is usually hard to explain in simple terms\\nwhy the predictions were made For example if a neural network says that a particu\\nlar person appears on a picture it is hard to know what actually contributed to this\\nprediction did the model recognize that persons eyes Her mouth Her nose Her\\nshoes Or even the couch that she was sitting on Conversely Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be eg\\nfor flower classification\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic\\nular class k first it traverses the tree to find the leaf node for this instance and then it\\nreturns the ratio of training instances of class k in this node For ', 'e for this instance and then it\\nreturns the ratio of training instances of class k in this node For example suppose\\nyou have found a flower whose petals are 5 cm long and 15 cm wide The corre\\nsponding leaf node is the depth2 left node so the Decision Tree should output the\\nfollowing probabilities 0 for IrisSetosa 054 907 for IrisVersicolor 4954\\nand 93 for IrisVirginica 554 And of course if you ask it to predict the class it\\nshould output IrisVersicolor class 1 since it has the highest probability Lets check\\nthis\\n treeclf predictproba 5 15\\narray0         090740741 009259259\\nEstimating Class Probabilities  181 treeclf predict5 15\\narray1\\nPerfect Notice that the estimated probabilities would be identical anywhere else in\\nthe bottomright rectangle of Figure 62 for example if the petals were 6 cm long\\nand 15 cm wide even though it seems obvious that it would most likely be an Iris\\nVirginica in this case\\nThe CART Training Algorithm\\nScikitLearn uses the Classification  And Regression Tree  CAR', ' this case\\nThe CART Training Algorithm\\nScikitLearn uses the Classification  And Regression Tree  CART algorithm to train\\nDecision Trees also called growing trees The idea is really quite simple the algo\\nrithm first splits the training set in two subsets using a single feature k and a thres\\nhold tk eg petal length   245 cm How does it choose k and tk It searches for the\\npair  k tk that produces the purest subsets weighted by their size The cost function\\nthat the algorithm tries to minimize is given by Equation 62 \\nEquation 62 CART cost function for classification\\nJktkmleft\\nmGleftmright\\nmGright\\nwhereGleftrightmeasures the impurity of the leftright subset\\nmleftrightis the number of instances in the leftright subset\\nOnce it has successfully split the training set in two it splits the subsets using the\\nsame logic then the subsubsets and so on recursively It stops recursing once it rea\\nches the maximum depth defined by the maxdepth  hyperparameter or if it cannot\\nfind a split that will reduc', 'e maximum depth defined by the maxdepth  hyperparameter or if it cannot\\nfind a split that will reduce impurity A few other hyperparameters described in a\\nmoment control additional stopping conditions  minsamplessplit  minsam\\nplesleaf  minweightfractionleaf  and maxleafnodes \\n182  Chapter 6 Decision Trees2P is the set of problems that can be solved in polynomial time NP is the set of problems whose solutions can\\nbe verified in polynomial time An NPHard problem is a problem to which any NP problem can be reduced\\nin polynomial time An NPComplete problem is both NP and NPHard A major open mathematical ques\\ntion is whether or not P  NP  If P  NP which seems likely then no polynomial algorithm will ever be\\nfound for any NPComplete problem except perhaps on a quantum computer\\n3log2 is the binary logarithm It is equal to log2m  logm  log2\\n4A reduction of entropy is often called an information gain \\nAs you can see the CART algorithm is a greedy algorithm  it greed\\nily searches for an optimum sp', 'in \\nAs you can see the CART algorithm is a greedy algorithm  it greed\\nily searches for an optimum split at the top level then repeats the\\nprocess at each level It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down A greedy\\nalgorithm often produces a reasonably good solution but it is not\\nguaranteed to be the optimal solution\\nUnfortunately finding the optimal tree is known to be an NP\\nComplete  problem2 it requires Oexp m time making the prob\\nlem intractable even for fairly small training sets This is why we\\nmust settle for a reasonably good solution\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf\\nDecision Trees are generally approximately balanced so traversing the Decision Tree\\nrequires going through roughly Olog2m nodes3 Since each node only requires\\nchecking the value of one feature the overall prediction complexity is just Olog2m\\nindependent of the number of features So pred', 'ature the overall prediction complexity is just Olog2m\\nindependent of the number of features So predictions are very fast even when deal\\ning with large training sets\\nHowever the training algorithm compares all features or less if maxfeatures  is set\\non all samples at each node This results in a training complexity of On  m logm\\nFor small training sets less than a few thousand instances ScikitLearn can speed up\\ntraining by presorting the data set presortTrue  but this slows down training con\\nsiderably for larger training sets\\nGini Impurity or Entropy\\nBy default the Gini impurity measure is used but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to entropy  The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder\\nentropy approaches zero when molecules are still and well ordered It later spread to a\\nwide variety of domains including Shannons information theory  where it measures\\nthe average information content of', ' domains including Shannons information theory  where it measures\\nthe average information content of a message4 entropy is zero when all messages are\\nidentical In Machine Learning it is frequently used as an impurity measure a sets\\nComputational Complexity  1835See Sebastian Raschkas interesting analysis for more details entropy is zero when it contains instances of only one class Equation 63  shows the\\ndefinition of the entropy of the ith node For example the depth2 left node in\\nFigure 61  has an entropy equal to 49\\n54log249\\n545\\n54log25\\n54  0445\\nEquation 63 Entropy\\nHi  \\nk 1\\npik 0n\\npiklog2pik\\nSo should you use Gini impurity or entropy The truth is most of the time it does not\\nmake a big difference they lead to similar trees Gini impurity is slightly faster to\\ncompute so it is a good default However when they differ Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree while entropy tends to\\nproduce slightly more balanced trees5\\nRegularization Hyperparamet', 'of the tree while entropy tends to\\nproduce slightly more balanced trees5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data as opposed to lin\\near models which obviously assume that the data is linear for example If left\\nunconstrained the tree structure will adapt itself to the training data fitting it very\\nclosely and most likely overfitting it Such a model is often called a nonparametric\\nmodel  not because it does not have any parameters it often has a lot but because the\\nnumber of parameters is not determined prior to training so the model structure is\\nfree to stick closely to the data In contrast a parametric model  such as a linear model\\nhas a predetermined number of parameters so its degree of freedom is limited\\nreducing the risk of overfitting but increasing the risk of underfitting\\nTo avoid overfitting the training data you need to restrict the Decision Trees freedom\\nduring training As you know by now this is called regularization The', 'rict the Decision Trees freedom\\nduring training As you know by now this is called regularization The regularization\\nhyperparameters depend on the algorithm used but generally you can at least restrict\\nthe maximum depth of the Decision Tree In ScikitLearn this is controlled by the\\nmaxdepth  hyperparameter the default value is None  which means unlimited\\nReducing maxdepth  will regularize the model and thus reduce the risk of overfitting\\nThe DecisionTreeClassifier  class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree minsamplessplit  the minimum number of sam\\nples a node must have before it can be split minsamplesleaf  the minimum num\\nber of samples a leaf node must have minweightfractionleaf  same as\\nminsamplesleaf  but expressed as a fraction of the total number of weighted\\n184  Chapter 6 Decision Treesinstances maxleafnodes  maximum number of leaf nodes and maxfeatures\\nmaximum number of features that are evaluated for splitting at each node Increas\\ni', 's and maxfeatures\\nmaximum number of features that are evaluated for splitting at each node Increas\\ning min  hyperparameters or reducing max  hyperparameters will regularize the\\nmodel\\nOther algorithms work by first training the Decision Tree without\\nrestrictions then pruning  deleting unnecessary nodes A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant  Stan\\ndard statistical tests such as the 2 test are used to estimate the\\nprobability that the improvement is purely the result of chance\\nwhich is called the null hypothesis  If this probability called the p\\nvalue  is higher than a given threshold typically 5 controlled by\\na hyperparameter then the node is considered unnecessary and its\\nchildren are deleted The pruning continues until all unnecessary\\nnodes have been pruned\\nFigure 63  shows two Decision Trees trained on the moons dataset introduced in\\nChapter 5  On the left the Decision Tree is trained wi', 'es trained on the moons dataset introduced in\\nChapter 5  On the left the Decision Tree is trained with the default hyperparameters\\nie no restrictions and on the right the Decision Tree is trained with minsam\\nplesleaf4  It is quite obvious that the model on the left is overfitting and the\\nmodel on the right will probably generalize better\\nFigure 63 Regularization using minsamplesleaf\\nRegression\\nDecision Trees are also capable of performing regression tasks Lets build a regres\\nsion tree using ScikitLearns DecisionTreeRegressor  class training it on a noisy\\nquadratic dataset with maxdepth2 \\nfrom sklearntree  import DecisionTreeRegressor\\nRegression  185treereg   DecisionTreeRegressor maxdepth 2\\ntreereg fitX y\\nThe resulting tree is represented on Figure 64 \\nFigure 64 A Decision Tree for regression\\nThis tree looks very similar to the classification tree you built earlier The main differ\\nence is that instead of predicting a class in each node it predicts a value For example\\nsuppose you want t', 's that instead of predicting a class in each node it predicts a value For example\\nsuppose you want to make a prediction for a new instance with x1  06 Y ou traverse\\nthe tree starting at the root and you eventually reach the leaf node that predicts\\nvalue01106  This prediction is simply the average target value of the 110 training\\ninstances associated to this leaf node This prediction results in a Mean Squared Error\\nMSE equal to 00151 over these 110 instances\\nThis models predictions are represented on the left of Figure 65  If you set\\nmaxdepth3  you get the predictions represented on the right Notice how the pre\\ndicted value for each region is always the average target value of the instances in that\\nregion The algorithm splits each region in a way that makes most training instances\\nas close as possible to that predicted value\\n186  Chapter 6 Decision TreesFigure 65 Predictions of two Decision Tree regression models\\nThe CART algorithm works mostly the same way as earlier except that instea', 'on Tree regression models\\nThe CART algorithm works mostly the same way as earlier except that instead of try\\ning to split the training set in a way that minimizes impurity it now tries to split the\\ntraining set in a way that minimizes the MSE Equation 64  shows the cost function\\nthat the algorithm tries to minimize\\nEquation 64 CART cost function for regression\\nJktkmleft\\nmMSEleftmright\\nmMSErightwhereMSEnode\\ninodeynodeyi2\\nynode1\\nmnode\\ninodeyi\\nJust like for classification tasks Decision Trees are prone to overfitting when dealing\\nwith regression tasks Without any regularization ie using the default hyperpara\\nmeters you get the predictions on the left of Figure 66  It is obviously overfitting\\nthe training set very badly Just setting minsamplesleaf10  results in a much more\\nreasonable model represented on the right of Figure 66 \\nFigure 66 Regularizing a Decision Tree regressor\\nRegression  1876It randomly selects the set of features to evaluate at each nodeInstability\\nHopefully by now you ar', '6It randomly selects the set of features to evaluate at each nodeInstability\\nHopefully by now you are convinced that Decision Trees have a lot going for them\\nthey are simple to understand and interpret easy to use versatile and powerful\\nHowever they do have a few limitations First as you may have noticed Decision\\nTrees love orthogonal decision boundaries all splits are perpendicular to an axis\\nwhich makes them sensitive to training set rotation For example Figure 67  shows a\\nsimple linearly separable dataset on the left a Decision Tree can split it easily while\\non the right after the dataset is rotated by 45 the decision boundary looks unneces\\nsarily convoluted Although both Decision Trees fit the training set perfectly it is very\\nlikely that the model on the right will not generalize well One way to limit this prob\\nlem is to use PCA see Chapter 8  which often results in a better orientation of the\\ntraining data\\nFigure 67 Sensitivity to training set rotation\\nMore generally the main iss', 'tion of the\\ntraining data\\nFigure 67 Sensitivity to training set rotation\\nMore generally the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data For example if you just remove the widest Iris\\nVersicolor from the iris training set the one with petals 48 cm long and 18 cm wide\\nand train a new Decision Tree you may get the model represented in Figure 68  As\\nyou can see it looks very different from the previous Decision Tree  Figure 62 \\nActually since the training algorithm used by ScikitLearn is stochastic6 you may\\nget very different models even on the same training data unless you set the\\nrandomstate  hyperparameter\\n188  Chapter 6 Decision TreesFigure 68 Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees as\\nwe will see in the next chapter\\nExercises\\n1What is the approximate depth of a Decision Tree trained without restrictions\\non a training set with 1 million instances\\n2I', ' depth of a Decision Tree trained without restrictions\\non a training set with 1 million instances\\n2Is a nodes Gini impurity generally lower or greater than its parents Is it gener\\nally lowergreater or always  lowergreater\\n3If a Decision Tree is overfitting the training set is it a good idea to try decreasing\\nmaxdepth \\n4If a Decision Tree is underfitting the training set is it a good idea to try scaling\\nthe input features\\n5If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances\\n6If your training set contains 100000 instances will setting presortTrue  speed\\nup training\\n7Train and finetune a Decision Tree for the moons dataset\\naGenerate a moons dataset using makemoonsnsamples10000 noise04 \\nbSplit it into a training set and a test set using traintestsplit \\nExercises  189cUse grid search with crossvalidation with the help of the GridSearc', 'g traintestsplit \\nExercises  189cUse grid search with crossvalidation with the help of the GridSearchCV\\nclass to find good hyperparameter values for a DecisionTreeClassifier  \\nHint try various values for maxleafnodes \\ndTrain it on the full training set using these hyperparameters and measure\\nyour models performance on the test set Y ou should get roughly 85 to 87\\naccuracy\\n8Grow a forest\\naContinuing the previous exercise generate 1000 subsets of the training set\\neach containing 100 instances selected randomly Hint you can use Scikit\\nLearns ShuffleSplit  class for this\\nbTrain one Decision Tree on each subset using the best hyperparameter values\\nfound above Evaluate these 1000 Decision Trees on the test set Since they\\nwere trained on smaller sets these Decision Trees will likely perform worse\\nthan the first Decision Tree achieving only about 80 accuracy\\ncNow comes the magic For each test set instance generate the predictions of\\nthe 1000 Decision Trees and keep only the most frequent predi', 't instance generate the predictions of\\nthe 1000 Decision Trees and keep only the most frequent prediction you can\\nuse SciPys mode  function for this This gives you majorityvote predictions\\nover the test set\\ndEvaluate these predictions on the test set you should obtain a slightly higher\\naccuracy than your first model about 05 to 15 higher Congratulations\\nyou have trained a Random Forest classifier\\nSolutions to these exercises are available in \\n190  Chapter 6 Decision TreesCHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 7 in the final\\nrelease of the book\\nSuppose you ask a complex question to thousands of random people then aggregate\\ntheir answers In many cases you will find that this aggregated answer is better than\\nan experts answer This is calle', ' many cases you will find that this aggregated answer is better than\\nan experts answer This is called the wisdom of the crowd  Similarly if you aggregate\\nthe predictions of a group of predictors such as classifiers or regressors you will\\noften get better predictions than with the best individual predictor A group of pre\\ndictors is called an ensemble  thus this technique is called Ensemble Learning  and an\\nEnsemble Learning algorithm is called an Ensemble method \\nFor example you can train a group of Decision Tree classifiers each on a different\\nrandom subset of the training set To make predictions you just obtain the predic\\ntions of all individual trees then predict the class that gets the most votes see the last\\nexercise in Chapter 6  Such an ensemble of Decision Trees is called a Random Forest  \\nand despite its simplicity this is one of the most powerful Machine Learning algo\\nrithms available today\\nMoreover as we discussed in Chapter 2  you will often use Ensemble methods near\\nthe end', 'ilable today\\nMoreover as we discussed in Chapter 2  you will often use Ensemble methods near\\nthe end of a project once you have already built a few good predictors to combine\\nthem into an even better predictor In fact the winning solutions in Machine Learn\\ning competitions often involve several Ensemble methods most famously in the Net\\nflix Prize competition \\nIn this chapter we will discuss the most popular Ensemble methods including bag\\nging boosting  stacking  and a few others We will also explore Random Forests\\n191Voting Classifiers\\nSuppose you have trained a few classifiers each one achieving about 80 accuracy\\nY ou may have a Logistic Regression classifier an SVM classifier a Random Forest\\nclassifier a KNearest Neighbors classifier and perhaps a few more see Figure 71 \\nFigure 71 Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes This majorityvote classi\\n', 'dictions of\\neach classifier and predict the class that gets the most votes This majorityvote classi\\nfier is called a hard voting  classifier see Figure 72 \\nFigure 72 Hard voting classifier  predictions\\n192  Chapter 7 Ensemble Learning and Random ForestsSomewhat surprisingly this voting classifier often achieves a higher accuracy than the\\nbest classifier in the ensemble In fact even if each classifier is a weak learner  mean\\ning it does only slightly better than random guessing the ensemble can still be a\\nstrong learner  achieving high accuracy provided there are a sufficient number of\\nweak learners and they are sufficiently diverse\\nHow is this possible The following analogy can help shed some light on this mystery\\nSuppose you have a slightly biased coin that has a 51 chance of coming up heads\\nand 49 chance of coming up tails If you toss it 1000 times you will generally get\\nmore or less 510 heads and 490 tails and hence a majority of heads If you do the\\nmath you will find that the proba', '10 heads and 490 tails and hence a majority of heads If you do the\\nmath you will find that the probability of obtaining a majority of heads after 1000\\ntosses is close to 75 The more you toss the coin the higher the probability eg\\nwith 10000 tosses the probability climbs over 97 This is due to the law of large\\nnumbers  as you keep tossing the coin the ratio of heads gets closer and closer to the\\nprobability of heads 51 Figure 73  shows 10 series of biased coin tosses Y ou can\\nsee that as the number of tosses increases the ratio of heads approaches 51 Eventu\\nally all 10 series end up so close to 51 that they are consistently above 50\\nFigure 73 The law of large numbers\\nSimilarly suppose you build an ensemble containing 1000 classifiers that are individ\\nually correct only 51 of the time barely better than random guessing If you pre\\ndict the majority voted class you can hope for up to 75 accuracy However this is\\nonly true if all classifiers are perfectly independent making uncorrelated erro', 'racy However this is\\nonly true if all classifiers are perfectly independent making uncorrelated errors\\nwhich is clearly not the case since they are trained on the same data They are likely to\\nmake the same types of errors so there will be many majority votes for the wrong\\nclass reducing the ensembles accuracy\\nVoting Classifiers   193Ensemble methods work best when the predictors are as independ\\nent from one another as possible One way to get diverse classifiers\\nis to train them using very different algorithms This increases the\\nchance that they will make very different types of errors improving\\nthe ensembles accuracy\\nThe following code creates and trains a voting classifier in ScikitLearn composed of\\nthree diverse classifiers the training set is the moons dataset introduced in Chap\\nter 5 \\nfrom sklearnensemble  import RandomForestClassifier\\nfrom sklearnensemble  import VotingClassifier\\nfrom sklearnlinearmodel  import LogisticRegression\\nfrom sklearnsvm  import SVC\\nlogclf  LogisticRegress', 'om sklearnlinearmodel  import LogisticRegression\\nfrom sklearnsvm  import SVC\\nlogclf  LogisticRegression \\nrndclf  RandomForestClassifier \\nsvmclf  SVC\\nvotingclf   VotingClassifier \\n    estimators lr logclf rf rndclf svc svmclf\\n    votinghard\\nvotingclf fitXtrain ytrain\\nLets look at each classifiers accuracy on the test set\\n from sklearnmetrics  import accuracyscore\\n for clf in logclf rndclf svmclf votingclf \\n     clffitXtrain ytrain\\n     ypred  clfpredictXtest\\n     printclfclass name  accuracyscore ytest ypred\\n\\nLogisticRegression 0864\\nRandomForestClassifier 0896\\nSVC 0888\\nVotingClassifier 0904\\nThere you have it The voting classifier slightly outperforms all the individual classifi\\ners\\nIf all classifiers are able to estimate class probabilities ie they have a pre\\ndictproba  method then you can tell ScikitLearn to predict the class with the\\nhighest class probability averaged over all the individual classifiers This is called soft\\nvoting  It often achieves higher performance than hard voting ', 'idual classifiers This is called soft\\nvoting  It often achieves higher performance than hard voting because it gives more\\nweight to highly confident votes All you need to do is replace votinghard  with\\nvotingsoft  and ensure that all classifiers can estimate class probabilities This is\\nnot the case of the SVC class by default so you need to set its probability  hyperpara\\nmeter to True  this will make the SVC class use crossvalidation to estimate class prob\\nabilities slowing down training and it will add a predictproba  method If you\\n194  Chapter 7 Ensemble Learning and Random Forests1Bagging Predictors  L Breiman 1996\\n2In statistics resampling with replacement is called bootstrapping \\n3Pasting small votes for classification in large databases and online  L Breiman 1999modify the preceding code to use soft voting you will find that the voting classifier\\nachieves over 912 accuracy\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms', 'and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms\\nas just discussed Another approach is to use the same training algorithm for every\\npredictor but to train them on different random subsets of the training set When\\nsampling is performed with  replacement this method is called bagging1 short for\\nbootstrap aggregating2 When sampling is performed without  replacement it is called\\npasting 3\\nIn other words both bagging and pasting allow training instances to be sampled sev\\neral times across multiple predictors but only bagging allows training instances to be\\nsampled several times for the same predictor This sampling and training process is\\nrepresented in Figure 74 \\nFigure 74 Pastingbagging training set sampling and training\\nOnce all predictors are trained the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors The aggregation\\nfunction is typically the statistical mode  ie the most frequen', 'ns of all predictors The aggregation\\nfunction is typically the statistical mode  ie the most frequent prediction just like a\\nhard voting classifier for classification or the average for regression Each individual\\nBagging and Pasting  1954Bias and variance were introduced in Chapter 4 \\n5maxsamples  can alternatively be set to a float between 00 and 10 in which case the max number of instances\\nto sample is equal to the size of the training set times maxsamples \\npredictor has a higher bias than if it were trained on the original training set but\\naggregation reduces both bias and variance4 Generally the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set\\nAs you can see in Figure 74  predictors can all be trained in parallel via different\\nCPU cores or even different servers Similarly predictions can be made in parallel\\nThis is one of the reasons why bagging and pasting are such popular methods they\\nscale very w', 'rallel\\nThis is one of the reasons why bagging and pasting are such popular methods they\\nscale very well\\nBagging and Pasting in ScikitLearn\\nScikitLearn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier  class or BaggingRegressor  for regression The following code trains an\\nensemble of 500 Decision Tree classifiers5 each trained on 100 training instances ran\\ndomly sampled from the training set with replacement this is an example of bagging\\nbut if you want to use pasting instead just set bootstrapFalse  The njobs  param\\neter tells ScikitLearn the number of CPU cores to use for training and predictions\\n1 tells ScikitLearn to use all available cores\\nfrom sklearnensemble  import BaggingClassifier\\nfrom sklearntree  import DecisionTreeClassifier\\nbagclf  BaggingClassifier \\n    DecisionTreeClassifier  nestimators 500\\n    maxsamples 100 bootstrap True njobs1\\nbagclffitXtrain ytrain\\nypred  bagclfpredictXtest\\nThe BaggingClassifier  automatically performs soft voting\\ninstea', 'in ytrain\\nypred  bagclfpredictXtest\\nThe BaggingClassifier  automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba\\nbilities ie if it has a predictproba  method which is the case\\nwith Decision Trees classifiers\\nFigure 75  compares the decision boundary of a single Decision Tree with the deci\\nsion boundary of a bagging ensemble of 500 trees from the preceding code both\\ntrained on the moons dataset As you can see the ensembles predictions will likely\\ngeneralize much better than the single Decision Trees predictions the ensemble has a\\ncomparable bias but a smaller variance it makes roughly the same number of errors\\non the training set but the decision boundary is less irregular\\n196  Chapter 7 Ensemble Learning and Random Forests6As m grows this ratio approaches 1  exp1  63212\\nFigure 75 A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on ', 'rees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on so bagging ends up with a slightly higher bias than pasting but this also\\nmeans that predictors end up being less correlated so the ensembles variance is\\nreduced Overall bagging often results in better models which explains why it is gen\\nerally preferred However if you have spare time and CPU power you can use cross\\nvalidation to evaluate both bagging and pasting and select the one that works best\\nOutofBag Evaluation\\nWith bagging some instances may be sampled several times for any given predictor\\nwhile others may not be sampled at all By default a BaggingClassifier  samples m\\ntraining instances with replacement  bootstrapTrue  where m is the size of the\\ntraining set This means that only about 63 of the training instances are sampled on\\naverage for each predictor6 The remaining 37 of the training instances that are not\\nsampled are called outofbag  oob instances Note that they are not the ', 'aining instances that are not\\nsampled are called outofbag  oob instances Note that they are not the same 37\\nfor all predictors\\nSince a predictor never sees the oob instances during training it can be evaluated on\\nthese instances without the need for a separate validation set Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor\\nIn ScikitLearn you can set oobscoreTrue  when creating a BaggingClassifier  to\\nrequest an automatic oob evaluation after training The following code demonstrates\\nthis The resulting evaluation score is available through the oobscore  variable\\n bagclf  BaggingClassifier \\n     DecisionTreeClassifier  nestimators 500\\n     bootstrap True njobs1 oobscore True\\n\\n bagclffitXtrain ytrain\\nBagging and Pasting  1977Ensembles on Random Patches  G Louppe and P  Geurts 2012\\n8The random subspace method for constructing decision forests  Tin Kam Ho 1998 bagclfoobscore\\n090133333333333332\\nAccording to this oob evaluation this BaggingClassifier', 'am Ho 1998 bagclfoobscore\\n090133333333333332\\nAccording to this oob evaluation this BaggingClassifier  is likely to achieve about\\n901 accuracy on the test set Lets verify this\\n from sklearnmetrics  import accuracyscore\\n ypred  bagclfpredictXtest\\n accuracyscore ytest ypred\\n091200000000000003\\nWe get 912 accuracy on the test setclose enough\\nThe oob decision function for each training instance is also available through the\\noobdecisionfunction  variable In this case since the base estimator has a pre\\ndictproba  method the decision function returns the class probabilities for each\\ntraining instance For example the oob evaluation estimates that the first training\\ninstance has a 6825 probability of belonging to the positive class and 3175 of\\nbelonging to the negative class\\n bagclfoobdecisionfunction\\narray031746032 068253968\\n       034117647 065882353\\n       1         0        \\n       \\n       1         0        \\n       003108808 096891192\\n       057291667 042708333\\nRandom Patches and Random Subs', '      0        \\n       003108808 096891192\\n       057291667 042708333\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier  class supports sampling the features as well This is con\\ntrolled by two hyperparameters maxfeatures  and bootstrapfeatures  They work\\nthe same way as maxsamples  and bootstrap  but for feature sampling instead of\\ninstance sampling Thus each predictor will be trained on a random subset of the\\ninput features\\nThis is particularly useful when you are dealing with highdimensional inputs such\\nas images Sampling both training instances and features is called the Random\\nPatches  method 7 Keeping all training instances ie bootstrapFalse  and maxsam\\nples10  but sampling features ie bootstrapfeaturesTrue  andor maxfea\\ntures  smaller than 10 is called the Random Subspaces  method 8\\n198  Chapter 7 Ensemble Learning and Random Forests9Random Decision Forests  T Ho 1995\\n10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees\\n11', ' BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees\\n11There are a few notable exceptions splitter  is absent forced to random  presort  is absent forced to\\nFalse  maxsamples  is absent forced to 10 and baseestimator  is absent forced to DecisionTreeClassi\\nfier  with the provided hyperparametersSampling features results in even more predictor diversity trading a bit more bias for\\na lower variance\\nRandom Forests\\nAs we have discussed a Random Forest9 is an ensemble of Decision Trees generally\\ntrained via the bagging method or sometimes pasting typically with maxsamples\\nset to the size of the training set Instead of building a BaggingClassifier  and pass\\ning it a DecisionTreeClassifier  you can instead use the RandomForestClassifier\\nclass which is more convenient and optimized for Decision Trees10 similarly there is\\na RandomForestRegressor  class for regression tasks The following code trains a\\nRandom Forest classifier with 500 trees each limit', ' for regression tasks The following code trains a\\nRandom Forest classifier with 500 trees each limited to maximum 16 nodes using\\nall available CPU cores\\nfrom sklearnensemble  import RandomForestClassifier\\nrndclf  RandomForestClassifier nestimators 500 maxleafnodes 16 njobs1\\nrndclffitXtrain ytrain\\nypredrf   rndclfpredictXtest\\nWith a few exceptions a RandomForestClassifier  has all the hyperparameters of a\\nDecisionTreeClassifier  to control how trees are grown plus all the hyperpara\\nmeters of a BaggingClassifier  to control the ensemble itself11\\nThe Random Forest algorithm introduces extra randomness when growing trees\\ninstead of searching for the very best feature when splitting a node see Chapter 6  it\\nsearches for the best feature among a random subset of features This results in a\\ngreater tree diversity which once again trades a higher bias for a lower variance\\ngenerally yielding an overall better model The following BaggingClassifier  is\\nroughly equivalent to the previous RandomFore', 'erall better model The following BaggingClassifier  is\\nroughly equivalent to the previous RandomForestClassifier \\nbagclf  BaggingClassifier \\n    DecisionTreeClassifier splitter random  maxleafnodes 16\\n    nestimators 500 maxsamples 10 bootstrap True njobs1\\nRandom Forests  19912Extremely randomized trees  P  Geurts D Ernst L Wehenkel 2005\\nExtraTrees\\nWhen you are growing a tree in a Random Forest at each node only a random subset\\nof the features is considered for splitting as discussed earlier It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds like regular Decision Trees do\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 or ExtraTrees  for short Once again this trades more bias for a\\nlower variance It also makes ExtraTrees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every no', 'n than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most timeconsuming tasks of growing a tree\\nY ou can create an ExtraTrees classifier using ScikitLearns ExtraTreesClassifier\\nclass Its API is identical to the RandomForestClassifier  class Similarly the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier  Gen\\nerally the only way to know is to try both and compare them using\\ncrossvalidation and tuning the hyperparameters using grid\\nsearch\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature ScikitLearn measures a features importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\nacross all trees in the forest More precisely it is a weighted average ', 'e reduce impurity on average\\nacross all trees in the forest More precisely it is a weighted average where each\\nnodes weight is equal to the number of training samples that are associated with it\\nsee Chapter 6 \\nScikitLearn computes this score automatically for each feature after training then it\\nscales the results so that the sum of all importances is equal to 1 Y ou can access the\\nresult using the featureimportances  variable For example the following code\\ntrains a RandomForestClassifier  on the iris dataset introduced in Chapter 4  and\\noutputs each features importance It seems that the most important features are the\\npetal length 44 and width 42 while sepal length and width are rather unim\\nportant in comparison 11 and 2 respectively\\n200  Chapter 7 Ensemble Learning and Random Forests from sklearndatasets  import loadiris\\n iris  loadiris \\n rndclf  RandomForestClassifier nestimators 500 njobs1\\n rndclffitirisdata iristarget \\n for name score in zipirisfeaturenames  rndclffeatureimportance', 'jobs1\\n rndclffitirisdata iristarget \\n for name score in zipirisfeaturenames  rndclffeatureimportances \\n     printname score\\n\\nsepal length cm 0112492250999\\nsepal width cm 00231192882825\\npetal length cm 0441030464364\\npetal width cm 0423357996355\\nSimilarly if you train a Random Forest classifier on the MNIST dataset introduced\\nin Chapter 3  and plot each pixels importance you get the image represented in\\nFigure 76 \\nFigure 76 MNIST pixel importance according to a Random Forest classifier\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter in particular if you need to perform feature selection\\nBoosting\\nBoosting  originally called hypothesis boosting  refers to any Ensemble method that\\ncan combine several weak learners into a strong learner The general idea of most\\nboosting methods is to train predictors sequentially each trying to correct its prede\\ncessor There are many boosting methods available but by far the most popular are\\nBoosting  20113 A Decis', 'or There are many boosting methods available but by far the most popular are\\nBoosting  20113 A DecisionTheoretic Generalization of OnLine Learning and an Application to Boosting  Y oav Freund\\nRobert E Schapire 1997\\n14This is just for illustrative purposes SVMs are generally not good base predictors for AdaBoost because they\\nare slow and tend to be unstable with AdaBoostAdaBoost13 short for Adaptive Boosting  and Gradient Boosting  Lets start with Ada\\nBoost\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted This results in new predic\\ntors focusing more and more on the hard cases This is the technique used by Ada\\nBoost\\nFor example to build an AdaBoost classifier a first base classifier such as a Decision\\nTree is trained and used to make predictions on the training set The relative weight\\nof misclassified training instances is then increased A second classifier is trained\\nusing the updat', 'of misclassified training instances is then increased A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set weights\\nare updated and so on see Figure 77 \\nFigure 77 AdaBoost sequential training with instance weight updates\\nFigure 78  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset in this example each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14 The first classifier gets many instances wrong so their weights\\n202  Chapter 7 Ensemble Learning and Random Forestsget boosted The second classifier therefore does a better job on these instances and\\nso on The plot on the right represents the same sequence of predictors except that\\nthe learning rate is halved ie the misclassified instance weights are boosted half as\\nmuch at every iteration As you can see this sequential learning technique has some\\nsimilarities with Gradient Descent except that instead of tweaking a single predictors\\npa', 'e has some\\nsimilarities with Gradient Descent except that instead of tweaking a single predictors\\nparameters to minimize a cost function AdaBoost adds predictors to the ensemble\\ngradually making it better\\nFigure 78 Decision boundaries of consecutive predictors\\nOnce all predictors are trained the ensemble makes predictions very much like bag\\nging or pasting except that predictors have different weights depending on their\\noverall accuracy on the weighted training set\\nThere is one important drawback to this sequential learning techni\\nque it cannot be parallelized or only partially since each predic\\ntor can only be trained after the previous predictor has been\\ntrained and evaluated As a result it does not scale as well as bag\\nging or pasting\\nLets take a closer look at the AdaBoost algorithm Each instance weight wi is initially\\nset to 1\\nm A first predictor is trained and its weighted error rate r1 is computed on the\\ntraining set see Equation 71 \\nEquation 71 Weighted error rate of the jth pr', 'te r1 is computed on the\\ntraining set see Equation 71 \\nEquation 71 Weighted error rate of the jth predictor\\nrj\\ni 1\\nyjiyim\\nwi\\n\\ni 1m\\nwiwhere yjiis the jthpredictors prediction for the ithinstance\\nBoosting  20315The original AdaBoost algorithm does not use a learning rate hyperparameterThe predictors weight j is then computed using Equation 72  where  is the learn\\ning rate hyperparameter defaults to 115 The more accurate the predictor is the\\nhigher its weight will be If it is just guessing randomly then its weight will be close to\\nzero However if it is most often wrong ie less accurate than random guessing\\nthen its weight will be negative\\nEquation 72 Predictor weight\\njlog1 rj\\nrj\\nNext the instance weights are updated using Equation 73  the misclassified instances\\nare boosted\\nEquation 73 Weight update rule\\nfori 1 2m\\nwiwiifyjiyi\\nwiexp jifyjiyi\\nThen all the instance weights are normalized ie divided by i 1mwi\\nFinally a new predictor is trained using the updated weights and the whole process i', 'vided by i 1mwi\\nFinally a new predictor is trained using the updated weights and the whole process is\\nrepeated the new predictors weight is computed the instance weights are updated\\nthen another predictor is trained and so on The algorithm stops when the desired\\nnumber of predictors is reached or when a perfect predictor is found\\nTo make predictions AdaBoost simply computes the predictions of all the predictors\\nand weighs them using the predictor weights j The predicted class is the one that\\nreceives the majority of weighted votes see Equation 74 \\nEquation 74 AdaBoost predictions\\nyx argmax\\nk\\nj 1\\nyjxkN\\njwhere Nis the number of predictors\\n204  Chapter 7 Ensemble Learning and Random Forests16For more details see MultiClass AdaBoost  J Zhu et al 2006\\n17First introduced in  Arcing the Edge  L Breiman 1997 and further developed in the paper Greedy Func\\ntion Approximation A Gradient Boosting Machine  Jerome H Friedman 1999\\nScikitLearn actually uses a multiclass version of AdaBoost called SAMM', 'chine  Jerome H Friedman 1999\\nScikitLearn actually uses a multiclass version of AdaBoost called SAMME16 which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function \\nWhen there are just two classes SAMME is equivalent to AdaBoost Moreover if the\\npredictors can estimate class probabilities ie if they have a predictproba\\nmethod ScikitLearn can use a variant of SAMME called SAMMER  the R stands\\nfor Real which relies on class probabilities rather than predictions and generally\\nperforms better\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps  using\\nScikitLearns AdaBoostClassifier  class as you might expect there is also an Ada\\nBoostRegressor  class A Decision Stump is a Decision Tree with maxdepth1 in\\nother words a tree composed of a single decision node plus two leaf nodes This is\\nthe default base estimator for the AdaBoostClassifier  class\\nfrom sklearnensemble  import AdaBoostClassifier\\nadaclf  AdaBoostClassifier \\n    DecisionTreeC', ' class\\nfrom sklearnensemble  import AdaBoostClassifier\\nadaclf  AdaBoostClassifier \\n    DecisionTreeClassifier maxdepth 1 nestimators 200\\n    algorithm SAMMER  learningrate 05\\nadaclffitXtrain ytrain\\nIf your AdaBoost ensemble is overfitting the training set you can\\ntry reducing the number of estimators or more strongly regulariz\\ning the base estimator\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting 17 Just like AdaBoost\\nGradient Boosting works by sequentially adding predictors to an ensemble each one\\ncorrecting its predecessor However instead of tweaking the instance weights at every\\niteration like AdaBoost does this method tries to fit the new predictor to the residual\\nerrors  made by the previous predictor\\nLets go through a simple regression example using Decision Trees as the base predic\\ntors of course Gradient Boosting also works great with regression tasks This is\\ncalled  Gradient Tree Boosting  or Gradient Boosted Regression Trees  GBRT  First lets\\nfi', 'ks This is\\ncalled  Gradient Tree Boosting  or Gradient Boosted Regression Trees  GBRT  First lets\\nfit a DecisionTreeRegressor  to the training set for example a noisy quadratic train\\ning set\\nBoosting  205from sklearntree  import DecisionTreeRegressor\\ntreereg1   DecisionTreeRegressor maxdepth 2\\ntreereg1 fitX y\\nNow train a second DecisionTreeRegressor  on the residual errors made by the first\\npredictor\\ny2  y  treereg1 predictX\\ntreereg2   DecisionTreeRegressor maxdepth 2\\ntreereg2 fitX y2\\nThen we train a third regressor on the residual errors made by the second predictor\\ny3  y2  treereg2 predictX\\ntreereg3   DecisionTreeRegressor maxdepth 2\\ntreereg3 fitX y3\\nNow we have an ensemble containing three trees It can make predictions on a new\\ninstance simply by adding up the predictions of all the trees\\nypred  sumtreepredictXnew for tree in treereg1  treereg2  treereg3 \\nFigure 79  represents the predictions of these three trees in the left column and the\\nensembles predictions in the right column I', 'dictions of these three trees in the left column and the\\nensembles predictions in the right column In the first row the ensemble has just one\\ntree so its predictions are exactly the same as the first trees predictions In the second\\nrow a new tree is trained on the residual errors of the first tree On the right you can\\nsee that the ensembles predictions are equal to the sum of the predictions of the first\\ntwo trees Similarly in the third row another tree is trained on the residual errors of\\nthe second tree Y ou can see that the ensembles predictions gradually get better as\\ntrees are added to the ensemble\\nA simpler way to train GBRT ensembles is to use ScikitLearns GradientBoostingRe\\ngressor  class Much like the RandomForestRegressor  class it has hyperparameters to\\ncontrol the growth of Decision Trees eg maxdepth  minsamplesleaf  and so on\\nas well as hyperparameters to control the ensemble training such as the number of\\ntrees nestimators  The following code creates the same ensemble as ', 'e training such as the number of\\ntrees nestimators  The following code creates the same ensemble as the previous\\none\\nfrom sklearnensemble  import GradientBoostingRegressor\\ngbrt  GradientBoostingRegressor maxdepth 2 nestimators 3 learningrate 10\\ngbrtfitX y\\n206  Chapter 7 Ensemble Learning and Random ForestsFigure 79 Gradient Boosting\\nThe learningrate  hyperparameter scales the contribution of each tree If you set it\\nto a low value such as 01 you will need more trees in the ensemble to fit the train\\ning set but the predictions will usually generalize better This is a regularization tech\\nnique called shrinkage  Figure 710  shows two GBRT ensembles trained with a low\\nlearning rate the one on the left does not have enough trees to fit the training set\\nwhile the one on the right has too many trees and overfits the training set\\nBoosting  207Figure 710 GBRT ensembles with not enough predictors left  and too many right\\nIn order to find the optimal number of trees you can use early stopping see ', 'eft  and too many right\\nIn order to find the optimal number of trees you can use early stopping see Chap\\nter 4  A simple way to implement this is to use the stagedpredict  method it\\nreturns an iterator over the predictions made by the ensemble at each stage of train\\ning with one tree two trees etc The following code trains a GBRT ensemble with\\n120 trees then measures the validation error at each stage of training to find the opti\\nmal number of trees and finally trains another GBRT ensemble using the optimal\\nnumber of trees\\nimport numpy as np\\nfrom sklearnmodelselection  import traintestsplit\\nfrom sklearnmetrics  import meansquarederror\\nXtrain Xval ytrain yval  traintestsplit X y\\ngbrt  GradientBoostingRegressor maxdepth 2 nestimators 120\\ngbrtfitXtrain ytrain\\nerrors  meansquarederror yval ypred\\n          for ypred in gbrtstagedpredict Xval\\nbstnestimators   npargminerrors\\ngbrtbest   GradientBoostingRegressor maxdepth 2nestimators bstnestimators \\ngbrtbest fitXtrain ytrain\\nThe validation err', 'BoostingRegressor maxdepth 2nestimators bstnestimators \\ngbrtbest fitXtrain ytrain\\nThe validation errors are represented on the left of Figure 711  and the best models\\npredictions are represented on the right\\n208  Chapter 7 Ensemble Learning and Random ForestsFigure 711 Tuning the number of trees using early stopping\\nIt is also possible to implement early stopping by actually stopping training early\\ninstead of training a large number of trees first and then looking back to find the\\noptimal number Y ou can do so by setting warmstartTrue  which makes Scikit\\nLearn keep existing trees when the fit  method is called allowing incremental\\ntraining The following code stops training when the validation error does not\\nimprove for five iterations in a row\\ngbrt  GradientBoostingRegressor maxdepth 2 warmstart True\\nminvalerror   floatinf\\nerrorgoingup   0\\nfor nestimators  in range1 120\\n    gbrtnestimators   nestimators\\n    gbrtfitXtrain ytrain\\n    ypred  gbrtpredictXval\\n    valerror   meansquarederror', 'rs   nestimators\\n    gbrtfitXtrain ytrain\\n    ypred  gbrtpredictXval\\n    valerror   meansquarederror yval ypred\\n    if valerror   minvalerror \\n        minvalerror   valerror\\n        errorgoingup   0\\n    else\\n        errorgoingup   1\\n        if errorgoingup   5\\n            break   early stopping\\nThe GradientBoostingRegressor  class also supports a subsample  hyperparameter\\nwhich specifies the fraction of training instances to be used for training each tree For\\nexample if subsample025  then each tree is trained on 25 of the training instan\\nces selected randomly As you can probably guess by now this trades a higher bias\\nfor a lower variance It also speeds up training considerably This technique is called\\nStochastic Gradient Boosting \\nBoosting  20918Stacked Generalization  D Wolpert 1992\\nIt is possible to use Gradient Boosting with other cost functions\\nThis is controlled by the loss  hyperparameter see ScikitLearns\\ndocumentation for more details\\nIt is worth noting that an optimized impleme', 'rameter see ScikitLearns\\ndocumentation for more details\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost  which stands for Extreme Gradient Boosting\\nThis package was initially developed by Tianqi Chen as part of the Distributed Deep\\nMachine Learning Community  DMLC  and it aims at being extremely fast scalable\\nand portable In fact XGBoost is often an important component of the winning\\nentries in ML competitions XGBoosts API is quite similar to ScikitLearns\\nimport xgboost\\nxgbreg  xgboostXGBRegressor \\nxgbregfitXtrain ytrain\\nypred  xgbregpredictXval\\nXGBoost also offers several nice features such as automatically taking care of early\\nstopping\\nxgbregfitXtrain ytrain\\n            evalset Xval yval earlystoppingrounds 2\\nypred  xgbregpredictXval\\nY ou should definitely check it out\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking  short for\\nstacked generalization 18 It is based on a sim', 'discuss in this chapter is called stacking  short for\\nstacked generalization 18 It is based on a simple idea instead of using trivial functions\\nsuch as hard voting to aggregate the predictions of all predictors in an ensemble\\nwhy dont we train a model to perform this aggregation Figure 712  shows such an\\nensemble performing a regression task on a new instance Each of the bottom three\\npredictors predicts a different value 31 27 and 29 and then the final predictor \\ncalled a blender  or a meta learner  takes these predictions as inputs and makes the\\nfinal prediction 30\\n210  Chapter 7 Ensemble Learning and Random Forests19Alternatively it is possible to use outoffold predictions In some contexts this is called stacking  while using a\\nholdout set is called blending  However for many people these terms are synonymous\\nFigure 712 Aggregating predictions using a blending predictor\\nTo train the blender a common approach is to use a holdout set19 Lets see how it\\nworks First the training set is sp', 'ender a common approach is to use a holdout set19 Lets see how it\\nworks First the training set is split in two subsets The first subset is used to train the\\npredictors in the first layer see Figure 713 \\nFigure 713 Training the first layer\\nNext the first layer predictors are used to make predictions on the second heldout\\nset see Figure 714  This ensures that the predictions are clean  since the predictors\\nnever saw these instances during training Now for each instance in the holdout set\\nStacking  211there are three predicted values We can create a new training set using these predic\\nted values as input features which makes this new training set threedimensional\\nand keeping the target values The blender is trained on this new training set so it\\nlearns to predict the target value given the first layers predictions\\nFigure 714 Training the blender\\nIt is actually possible to train several different blenders this way eg one using Lin\\near Regression another using Random Forest Regression and s', 'erent blenders this way eg one using Lin\\near Regression another using Random Forest Regression and so on we get a whole\\nlayer of blenders The trick is to split the training set into three subsets the first one is\\nused to train the first layer the second one is used to create the training set used to\\ntrain the second layer using predictions made by the predictors of the first layer\\nand the third one is used to create the training set to train the third layer using pre\\ndictions made by the predictors of the second layer Once this is done we can make\\na prediction for a new instance by going through each layer sequentially as shown in\\nFigure 715 \\n212  Chapter 7 Ensemble Learning and Random ForestsFigure 715 Predictions in a multilayer stacking ensemble\\nUnfortunately ScikitLearn does not support stacking directly but it is not too hard\\nto roll out your own implementation see the following exercises Alternatively you\\ncan use an open source implementation such as brew  available at httpsgithu', 'rcises Alternatively you\\ncan use an open source implementation such as brew  available at httpsgithubcom\\nviisarbrew \\nExercises\\n1If you have trained five different models on the exact same training data and\\nthey all achieve 95 precision is there any chance that you can combine these\\nmodels to get better results If so how If not why\\n2What is the difference between hard and soft voting classifiers\\n3Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers What about pasting ensembles boosting ensembles random\\nforests or stacking ensembles\\n4What is the benefit of outofbag evaluation\\n5What makes ExtraTrees more random than regular Random Forests How can\\nthis extra randomness help Are ExtraTrees slower or faster than regular Ran\\ndom Forests\\n6If your AdaBoost ensemble underfits the training data what hyperparameters\\nshould you tweak and how\\nExercises  2137If your Gradient Boosting ensemble overfits the training set should you increase\\nor decrease the', '2137If your Gradient Boosting ensemble overfits the training set should you increase\\nor decrease the learning rate\\n8Load the MNIST data introduced in Chapter 3  and split it into a training set a\\nvalidation set and a test set eg use 50000 instances for training 10000 for val\\nidation and 10000 for testing Then train various classifiers such as a Random\\nForest classifier an ExtraTrees classifier and an SVM Next try to combine\\nthem into an ensemble that outperforms them all on the validation set using a\\nsoft or hard voting classifier Once you have found one try it on the test set How\\nmuch better does it perform compared to the individual classifiers\\n9Run the individual classifiers from the previous exercise to make predictions on\\nthe validation set and create a new training set with the resulting predictions\\neach training instance is a vector containing the set of predictions from all your\\nclassifiers for an image and the target is the images class Train a classifier on\\nthis new training ', 'classifiers for an image and the target is the images class Train a classifier on\\nthis new training set Congratulations you have just trained a blender and\\ntogether with the classifiers they form a stacking ensemble Now lets evaluate the\\nensemble on the test set For each image in the test set make predictions with all\\nyour classifiers then feed the predictions to the blender to get the ensembles pre\\ndictions How does it compare to the voting classifier you trained earlier\\nSolutions to these exercises are available in \\n214  Chapter 7 Ensemble Learning and Random ForestsCHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 8 in the final\\nrelease of the book\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance Not ', 'ine Learning problems involve thousands or even millions of features for\\neach training instance Not only does this make training extremely slow it can also\\nmake it much harder to find a good solution as we will see This problem is often\\nreferred to as the curse of dimensionality \\nFortunately in realworld problems it is often possible to reduce the number of fea\\ntures considerably turning an intractable problem into a tractable one For example\\nconsider the MNIST images introduced in Chapter 3  the pixels on the image bor\\nders are almost always white so you could completely drop these pixels from the\\ntraining set without losing much information Figure 76  confirms that these pixels\\nare utterly unimportant for the classification task Moreover two neighboring pixels\\nare often highly correlated if you merge them into a single pixel eg by taking the\\nmean of the two pixel intensities you will not lose much information\\n2151Well four dimensions if you count time and a few more if you are a stri', 'ot lose much information\\n2151Well four dimensions if you count time and a few more if you are a string theorist\\nReducing dimensionality does lose some information just like\\ncompressing an image to JPEG can degrade its quality so even\\nthough it will speed up training it may also make your system per\\nform slightly worse It also makes your pipelines a bit more com\\nplex and thus harder to maintain So you should first try to train\\nyour system with the original data before considering using dimen\\nsionality reduction if training is too slow In some cases however\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per\\nformance but in general it wont it will just speed up training\\nApart from speeding up training dimensionality reduction is also extremely useful\\nfor data visualization or DataViz  Reducing the number of dimensions down to two\\nor three makes it possible to plot a condensed view of a highdimensional training\\ns', 'ions down to two\\nor three makes it possible to plot a condensed view of a highdimensional training\\nset on a graph and often gain some important insights by visually detecting patterns\\nsuch as clusters Moreover DataViz is essential to communicate your conclusions to\\npeople who are not data scientists in particular decision makers who will use your\\nresults\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in highdimensional space Then we will present the two main approaches to\\ndimensionality reduction projection and Manifold Learning and we will go\\nthrough three of the most popular dimensionality reduction techniques PCA Kernel\\nPCA and LLE\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a highdimensional space Even a basic 4D hypercube is incredibly hard to\\npicture in our mind see Figure 81  let alone a 200dimensional ellipsoid bent in a\\n1000dimensional space\\n216  Chapte', 'mind see Figure 81  let alone a 200dimensional ellipsoid bent in a\\n1000dimensional space\\n216  Chapter 8 Dimensionality Reduction2Watch a rotating tesseract projected into 3D space at httpshomlinfo30  Image by Wikipedia user Nerd\\nBoy1392  Creative Commons BYSA 30  Reproduced from httpsenwikipediaorgwikiTesseract \\n3Fun fact anyone you know is probably an extremist in at least one dimension eg how much sugar they put\\nin their coffee if you consider enough dimensions\\nFigure 81 Point segment square cube and tesseract 0D to 4D hypercubes2\\nIt turns out that many things behave very differently in highdimensional space For\\nexample if you pick a random point in a unit square a 1  1 square it will have only\\nabout a 04 chance of being located less than 0001 from a border in other words it\\nis very unlikely that a random point will be extreme along any dimension But in a\\n10000dimensional unit hypercube a 1  1    1 cube with ten thousand 1s this\\nprobability is greater than 99999999 Most points in a h', 'e a 1  1    1 cube with ten thousand 1s this\\nprobability is greater than 99999999 Most points in a highdimensional hypercube\\nare very close to the border3\\nHere is a more troublesome difference if you pick two points randomly in a unit\\nsquare the distance between these two points will be on average roughly 052 If you\\npick two random points in a unit 3D cube the average distance will be roughly 066\\nBut what about two points picked randomly in a 1000000dimensional hypercube\\nWell the average distance believe it or not will be about 40825 roughly\\n1 000 0006  This is quite counterintuitive how can two points be so far apart\\nwhen they both lie within the same unit hypercube This fact implies that high\\ndimensional datasets are at risk of being very sparse most training instances are\\nlikely to be far away from each other Of course this also means that a new instance\\nwill likely be far away from any training instance making predictions much less relia\\nble than in lower dimensions since they will', 'ny training instance making predictions much less relia\\nble than in lower dimensions since they will be based on much larger extrapolations\\nIn short the more dimensions the training set has the greater the risk of overfitting\\nit\\nIn theory one solution to the curse of dimensionality could be to increase the size of\\nthe training set to reach a sufficient density of training instances Unfortunately in\\npractice the number of training instances required to reach a given density grows\\nexponentially with the number of dimensions With just 100 features much less than\\nThe Curse of Dimensionality  217in the MNIST problem you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 01 of each other on\\naverage assuming they were spread out uniformly across all dimensions\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms lets take a look at\\nthe two main approaches to reducing dim', 'ific dimensionality reduction algorithms lets take a look at\\nthe two main approaches to reducing dimensionality projection and Manifold\\nLearning\\nProjection\\nIn most realworld problems training instances are not spread out uniformly across\\nall dimensions Many features are almost constant while others are highly correlated\\nas discussed earlier for MNIST As a result all training instances actually lie within\\nor close to a much lowerdimensional subspace  of the highdimensional space This\\nsounds very abstract so lets look at an example In Figure 82  you can see a 3D data\\nset represented by the circles\\nFigure 82 A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane this is a lowerdimensional 2D\\nsubspace of the highdimensional 3D space Now if we project every training\\ninstance perpendicularly onto this subspace as represented by the short lines con\\nnecting the instances to the plane we get the new 2D dataset shown in Figure 83 \\nTada We have just redu', 'ting the instances to the plane we get the new 2D dataset shown in Figure 83 \\nTada We have just reduced the datasets dimensionality from 3D to 2D Note that\\nthe axes correspond to new features z1 and z2 the coordinates of the projections on\\nthe plane\\n218  Chapter 8 Dimensionality ReductionFigure 83 The new 2D dataset after  projection\\nHowever projection is not always the best approach to dimensionality reduction In\\nmany cases the subspace may twist and turn such as in the famous Swiss roll  toy data\\nset represented in Figure 84 \\nFigure 84 Swiss roll dataset\\nMain Approaches for Dimensionality Reduction  219Simply projecting onto a plane eg by dropping x3 would squash different layers of\\nthe Swiss roll together as shown on the left of Figure 85  However what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 85 \\nFigure 85 Squashing by projecting onto a plane left  versus unrolling the Swiss roll\\nright\\nManifold Learning\\nThe Swiss roll is an example ', ' a plane left  versus unrolling the Swiss roll\\nright\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold  Put simply a 2D manifold is a 2D\\nshape that can be bent and twisted in a higherdimensional space More generally a\\nddimensional manifold is a part of an ndimensional space where d  n that locally\\nresembles a ddimensional hyperplane In the case of the Swiss roll d  2 and n  3 it\\nlocally resembles a 2D plane but it is rolled in the third dimension\\nMany dimensionality reduction algorithms work by modeling the manifold  on which\\nthe training instances lie this is called Manifold Learning  It relies on the manifold\\nassumption  also called the manifold hypothesis  which holds that most realworld\\nhighdimensional datasets lie close to a much lowerdimensional manifold This\\nassumption is very often empirically observed\\nOnce again think about the MNIST dataset all handwritten digit images have some\\nsimilarities They are made of connected lines the borders are white they are more\\nor', 'mages have some\\nsimilarities They are made of connected lines the borders are white they are more\\nor less centered and so on If you randomly generated images only a ridiculously\\ntiny fraction of them would look like handwritten digits In other words the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted These constraints tend to squeeze the dataset into a lower\\ndimensional manifold\\nThe manifold assumption is often accompanied by another implicit assumption that\\nthe task at hand eg classification or regression will be simpler if expressed in the\\nlowerdimensional space of the manifold For example in the top row of Figure 86\\nthe Swiss roll is split into two classes in the 3D space on the left the decision\\n220  Chapter 8 Dimensionality Reductionboundary would be fairly complex but in the 2D unrolled manifold space on the\\nright the decision boundary is a', 'ould be fairly complex but in the 2D unrolled manifold space on the\\nright the decision boundary is a simple straight line\\nHowever this assumption does not always hold For example in the bottom row of\\nFigure 86  the decision boundary is located at x1  5 This decision boundary looks\\nvery simple in the original 3D space a vertical plane but it looks more complex in\\nthe unrolled manifold a collection of four independent line segments\\nIn short if you reduce the dimensionality of your training set before training a\\nmodel it will usually speed up training but it may not always lead to a better or sim\\npler solution it all depends on the dataset\\nHopefully you now have a good sense of what the curse of dimensionality is and how\\ndimensionality reduction algorithms can fight it especially when the manifold\\nassumption holds The rest of this chapter will go through some of the most popular\\nalgorithms\\nFigure 86 The decision boundary may not always be simpler with lower dimensions\\nMain Approaches for ', 'Figure 86 The decision boundary may not always be simpler with lower dimensions\\nMain Approaches for Dimensionality Reduction  2214On Lines and Planes of Closest Fit to Systems of Points in Space  K Pearson 1901PCA\\nPrincipal Component Analysis  PCA is by far the most popular dimensionality reduc\\ntion algorithm First it identifies the hyperplane that lies closest to the data and then\\nit projects the data onto it just like in Figure 82 \\nPreserving the Variance\\nBefore you can project the training set onto a lowerdimensional hyperplane you\\nfirst need to choose the right hyperplane For example a simple 2D dataset is repre\\nsented on the left of Figure 87  along with three different axes ie onedimensional\\nhyperplanes On the right is the result of the projection of the dataset onto each of\\nthese axes As you can see the projection onto the solid line preserves the maximum\\nvariance while the projection onto the dotted line preserves very little variance and\\nthe projection onto the dashed line pre', 'tion onto the dotted line preserves very little variance and\\nthe projection onto the dashed line preserves an intermediate amount of variance\\nFigure 87 Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var\\niance as it will most likely lose less information than the other projections Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis\\ntance between the original dataset and its projection onto that axis This is the rather\\nsimple idea behind PCA 4\\n222  Chapter 8 Dimensionality ReductionPrincipal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train\\ning set In Figure 87  it is the solid line It also finds a second axis orthogonal to the\\nfirst one that accounts for the largest amount of remaining variance In this 2D\\nexample there is no choice it is the dotted line If it were a higherdimensional data\\nset PCA would also find a third axis o', 'ice it is the dotted line If it were a higherdimensional data\\nset PCA would also find a third axis orthogonal to both previous axes and a fourth\\na fifth and so onas many axes as the number of dimensions in the dataset\\nThe unit vector that defines the ith axis is called the ith principal component  PC In\\nFigure 87  the 1st PC is c1 and the 2nd PC is c2 In Figure 82  the first two PCs are\\nrepresented by the orthogonal arrows in the plane and the third PC would be\\northogonal to the plane pointing up or down\\nThe direction of the principal components is not stable if you per\\nturb the training set slightly and run PCA again some of the new\\nPCs may point in the opposite direction of the original PCs How\\never they will generally still lie on the same axes In some cases a\\npair of PCs may even rotate or swap but the plane they define will\\ngenerally remain the same\\nSo how can you find the principal components of a training set Luckily there is a\\nstandard matrix factorization technique called Sing', ' components of a training set Luckily there is a\\nstandard matrix factorization technique called Singular Value Decomposition  SVD\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U  VT where V contains all the principal components that we are looking\\nfor as shown in Equation 81 \\nEquation 81 Principal components matrix\\nV  \\nc1c2cn\\n  \\nThe following Python code uses NumPys svd  function to obtain all the principal\\ncomponents of the training set then extracts the first two PCs\\nXcentered   X  Xmeanaxis0\\nU s Vt  nplinalgsvdXcentered \\nc1  VtT 0\\nc2  VtT 1\\nPCA  223PCA assumes that the dataset is centered around the origin As we\\nwill see ScikitLearns PCA classes take care of centering the data\\nfor you However if you implement PCA yourself as in the pre\\nceding example or if you use other libraries dont forget to center\\nthe data first\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components you can reduce the dimen\\nsionality ', 'Dimensions\\nOnce you have identified all the principal components you can reduce the dimen\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible For example in Figure 82  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com\\nponents preserving a large part of the datasets variance As a result the 2D projec\\ntion looks very much like the original 3D dataset\\nTo project the training set onto the hyperplane you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd defined as the matrix\\ncontaining the first d principal components ie the matrix composed of the first d\\ncolumns of V as shown in Equation 82 \\nEquation 82 Projecting the training set down to d dimensions\\nXdprojXWd\\nThe following Python code projects the training set onto the plane defined by the f', 'nsions\\nXdprojXWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components\\nW2  VtT 2\\nX2D  Xcentered dotW2\\nThere you have it Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions while preserving as much variance as possible\\nUsing ScikitLearn\\nScikitLearns PCA class implements PCA using SVD decomposition just like we did\\nbefore The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions note that it automatically takes care of centering the data\\nfrom sklearndecomposition  import PCA\\npca  PCAncomponents   2\\nX2D  pcafittransform X\\nAfter fitting the PCA transformer to the dataset you can access the principal compo\\nnents using the components  variable note that it contains the PCs as horizontal vec\\n224  Chapter 8 Dimensionality Reductiontors so for example the first principal component is equal to pcacomponentsT\\n0\\nExplained Variance Ratio\\nAnother very useful p', 'irst principal component is equal to pcacomponentsT\\n0\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin\\ncipal component available via the explainedvarianceratio  variable It indicates\\nthe proportion of the datasets variance that lies along the axis of each principal com\\nponent For example lets look at the explained variance ratios of the first two compo\\nnents of the 3D dataset represented in Figure 82 \\n pcaexplainedvarianceratio\\narray084248607 014631839\\nThis tells you that 842 of the datasets variance lies along the first axis and 146\\nlies along the second axis This leaves less than 12 for the third axis so it is reason\\nable to assume that it probably carries little information\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance eg 95 Unless o', ' the number of dimensions that add up to a sufficiently\\nlarge portion of the variance eg 95 Unless of course you are reducing dimen\\nsionality for data visualizationin that case you will generally want to reduce the\\ndimensionality down to 2 or 3\\nThe following code computes PCA without reducing dimensionality then computes\\nthe minimum number of dimensions required to preserve 95 of the training sets\\nvariance\\npca  PCA\\npcafitXtrain\\ncumsum  npcumsumpcaexplainedvarianceratio \\nd  npargmaxcumsum  095  1\\nY ou could then set ncomponentsd  and run PCA again However there is a much\\nbetter option instead of specifying the number of principal components you want to\\npreserve you can set ncomponents  to be a float between 00 and 10 indicating the\\nratio of variance you wish to preserve\\npca  PCAncomponents 095\\nXreduced   pcafittransform Xtrain\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions simply plot cumsum  see Figure 88  There will usually be an elbow ', 'ction of the number of\\ndimensions simply plot cumsum  see Figure 88  There will usually be an elbow in the\\ncurve where the explained variance stops growing fast Y ou can think of this as the\\nintrinsic dimensionality of the dataset In this case you can see that reducing the\\nPCA  225dimensionality down to about 100 dimensions wouldnt lose too much explained var\\niance\\nFigure 88 Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction the training set takes up much less space\\nFor example try applying PCA to the MNIST dataset while preserving 95 of its var\\niance Y ou should find that each instance will have just over 150 features instead of\\nthe original 784 features So while most of the variance is preserved the dataset is\\nnow less than 20 of its original size This is a reasonable compression ratio and you\\ncan see how this can speed up a classification algorithm such as an SVM classifier\\ntremendously\\nIt is also possible to dec', 'peed up a classification algorithm such as an SVM classifier\\ntremendously\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection Of course this wont give\\nyou back the original data since the projection lost a bit of information within the\\n5 variance that was dropped but it will likely be quite close to the original data\\nThe mean squared distance between the original data and the reconstructed data\\ncompressed and then decompressed is called the reconstruction error  For example\\nthe following code compresses the MNIST dataset down to 154 dimensions then uses\\nthe inversetransform  method to decompress it back to 784 dimensions\\nFigure 89  shows a few digits from the original training set on the left and the cor\\nresponding digits after compression and decompression Y ou can see that there is a\\nslight image quality loss but the digits are still mostly intact\\npca  PCAncomponents   154\\nXreduced   pcafittransform', 'ity loss but the digits are still mostly intact\\npca  PCAncomponents   154\\nXreduced   pcafittransform Xtrain\\nXrecovered   pcainversetransform Xreduced \\n226  Chapter 8 Dimensionality ReductionFigure 89 MNIST compression preserving 95 of the variance\\nThe equation of the inverse transformation is shown in Equation 83 \\nEquation 83 PCA inverse transformation back to the original number of\\ndimensions\\nXrecoveredXdprojWdT\\nRandomized PCA\\nIf you set the svdsolver  hyperparameter to randomized  ScikitLearn uses a sto\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components Its computational complexity is Om  d2  Od3\\ninstead of Om  n2  On3 for the full SVD approach so it is dramatically faster\\nthan full SVD when d is much smaller than n\\nrndpca  PCAncomponents 154 svdsolver randomized \\nXreduced   rndpcafittransform Xtrain\\nBy default svdsolver  is actually set to auto  ScikitLearn automatically uses the\\nrandomized PCA algorithm if m or n is grea', ' actually set to auto  ScikitLearn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80 of m\\nor n or else it uses the full SVD approach If you want to force ScikitLearn to use full\\nSVD you can set the svdsolver  hyperparameter to full \\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run Fortunately\\nIncremental PCA  IPCA algorithms have been developed you can split the training\\nset into minibatches and feed an IPCA algorithm one minibatch at a time This is\\nPCA  2275ScikitLearn uses the algorithm described in Incremental Learning for Robust Visual Tracking  D Ross et al\\n2007useful for large training sets and also to apply PCA online ie on the fly as new\\ninstances arrive\\nThe following code splits the MNIST dataset into 100 minibatches using NumPys\\narraysplit  function and feeds them to ScikitLearns IncrementalPCA  class5 to \\nreduce', 's using NumPys\\narraysplit  function and feeds them to ScikitLearns IncrementalPCA  class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions just like\\nbefore Note that you must call the partialfit  method with each minibatch\\nrather than the fit  method with the whole training set\\nfrom sklearndecomposition  import IncrementalPCA\\nnbatches   100\\nincpca  IncrementalPCA ncomponents 154\\nfor Xbatch in nparraysplit Xtrain nbatches \\n    incpcapartialfit Xbatch\\nXreduced   incpcatransform Xtrain\\nAlternatively you can use NumPys memmap  class which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory the class\\nloads only the data it needs in memory when it needs it Since the IncrementalPCA\\nclass uses only a small part of the array at any given time the memory usage remains\\nunder control This makes it possible to call the usual fit  method as you can see\\nin the following code\\nXmm  npmemmapfilename  dtypefloat32  modereadonly ', ' fit  method as you can see\\nin the following code\\nXmm  npmemmapfilename  dtypefloat32  modereadonly  shapem n\\nbatchsize   m  nbatches\\nincpca  IncrementalPCA ncomponents 154 batchsize batchsize \\nincpcafitXmm\\nKernel PCA\\nIn Chapter 5  we discussed the kernel trick a mathematical technique that implicitly\\nmaps instances into a very highdimensional space called the feature space  enabling\\nnonlinear classification and regression with Support Vector Machines Recall that a\\nlinear decision boundary in the highdimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space \\nIt turns out that the same trick can be applied to PCA making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction This is called Kernel\\n228  Chapter 8 Dimensionality Reduction6Kernel Principal Component Analysis  B Schlkopf A Smola K Mller 1999PCA  kPCA 6 It is often good at preserving clusters of instances after projection or\\nsometimes even unrolling datasets', 'often good at preserving clusters of instances after projection or\\nsometimes even unrolling datasets that lie close to a twisted manifold\\nFor example the following code uses ScikitLearns KernelPCA  class to perform kPCA\\nwith an RBF kernel see Chapter 5  for more details about the RBF kernel and the\\nother kernels\\nfrom sklearndecomposition  import KernelPCA\\nrbfpca  KernelPCA ncomponents   2 kernelrbf gamma004\\nXreduced   rbfpcafittransform X\\nFigure 810  shows the Swiss roll reduced to two dimensions using a linear kernel\\nequivalent to simply using the PCA class an RBF kernel and a sigmoid kernel\\nLogistic\\nFigure 810 Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values However\\ndimensionality reduction is often a preparation step for a supervised learning task\\neg classification so you can simply us', 'on is often a preparation step for a supervised learning task\\neg classification so you can simply use grid search to select the kernel and hyper\\nparameters that lead to the best performance on that task For example the following\\ncode creates a twostep pipeline first reducing dimensionality to two dimensions\\nusing kPCA then applying Logistic Regression for classification Then it uses Grid\\nSearchCV  to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline\\nfrom sklearnmodelselection  import GridSearchCV\\nfrom sklearnlinearmodel  import LogisticRegression\\nfrom sklearnpipeline  import Pipeline\\nKernel PCA  229clf  Pipeline \\n        kpca KernelPCA ncomponents 2\\n        logreg  LogisticRegression \\n    \\nparamgrid   \\n        kpcagamma  nplinspace 003 005 10\\n        kpcakernel  rbf sigmoid \\n    \\ngridsearch   GridSearchCV clf paramgrid  cv3\\ngridsearch fitX y\\nThe best kernel and hyperparameters are then available through the bestpar', 'id  cv3\\ngridsearch fitX y\\nThe best kernel and hyperparameters are then available through the bestparams\\nvariable\\n printgridsearch bestparams \\nkpcagamma 0043333333333333335 kpcakernel rbf\\nAnother approach this time entirely unsupervised is to select the kernel and hyper\\nparameters that yield the lowest reconstruction error However reconstruction is not\\nas easy as with linear PCA Heres why Figure 811  shows the original Swiss roll 3D\\ndataset top left and the resulting 2D dataset after kPCA is applied using an RBF\\nkernel top right Thanks to the kernel trick this is mathematically equivalent to\\nmapping the training set to an infinitedimensional feature space bottom right\\nusing the feature map   then projecting the transformed training set down to 2D\\nusing linear PCA Notice that if we could invert the linear PCA step for a given\\ninstance in the reduced space the reconstructed point would lie in feature space not\\nin the original space eg like the one represented by an x in the diagram Since ', 'in feature space not\\nin the original space eg like the one represented by an x in the diagram Since the\\nfeature space is infinitedimensional we cannot compute the reconstructed point\\nand therefore we cannot compute the true reconstruction error Fortunately it is pos\\nsible to find a point in the original space that would map close to the reconstructed\\npoint This is called the reconstruction preimage  Once you have this preimage you\\ncan measure its squared distance to the original instance Y ou can then select the ker\\nnel and hyperparameters that minimize this reconstruction preimage error\\n230  Chapter 8 Dimensionality Reduction7ScikitLearn uses the algorithm based on Kernel Ridge Regression described in Gokhan H Bakr Jason\\nWeston and Bernhard Scholkopf Learning to Find Preimages  Tubingen Germany Max Planck Institute\\nfor Biological Cybernetics 2004\\nFigure 811 Kernel PCA and the reconstruction preimage error\\nY ou may be wondering how to perform this reconstruction One solution is to trai', 'tion preimage error\\nY ou may be wondering how to perform this reconstruction One solution is to train a\\nsupervised regression model with the projected instances as the training set and the\\noriginal instances as the targets ScikitLearn will do this automatically if you set\\nfitinversetransformTrue  as shown in the following code7\\nrbfpca  KernelPCA ncomponents   2 kernelrbf gamma00433\\n                    fitinversetransform True\\nXreduced   rbfpcafittransform X\\nXpreimage   rbfpcainversetransform Xreduced \\nBy default fitinversetransformFalse  and KernelPCA  has no\\ninversetransform  method This method only gets created\\nwhen you set fitinversetransformTrue \\nKernel PCA  2318Nonlinear Dimensionality Reduction by Locally Linear Embedding  S Roweis L Saul 2000Y ou can then compute the reconstruction preimage error\\n from sklearnmetrics  import meansquarederror\\n meansquarederror X Xpreimage \\n32786308795766132\\nNow you can use grid search with crossvalidation to find the kernel and hyperpara\\nmeters t', '795766132\\nNow you can use grid search with crossvalidation to find the kernel and hyperpara\\nmeters that minimize this preimage reconstruction error\\nLLE\\nLocally Linear Embedding  LLE8 is another very powerful nonlinear dimensionality\\nreduction  NLDR technique It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms In a nutshell LLE works by first measur\\ning how each training instance linearly relates to its closest neighbors cn and then\\nlooking for a lowdimensional representation of the training set where these local\\nrelationships are best preserved more details shortly This makes it particularly\\ngood at unrolling twisted manifolds especially when there is not too much noise\\nFor example the following code uses ScikitLearns LocallyLinearEmbedding  class to\\nunroll the Swiss roll The resulting 2D dataset is shown in Figure 812  As you can\\nsee the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved Howev', 'iss roll is completely unrolled and the distances between instances are\\nlocally well preserved However distances are not preserved on a larger scale the left\\npart of the unrolled Swiss roll is stretched while the right part is squeezed Neverthe\\nless LLE did a pretty good job at modeling the manifold\\nfrom sklearnmanifold  import LocallyLinearEmbedding\\nlle  LocallyLinearEmbedding ncomponents 2 nneighbors 10\\nXreduced   llefittransform X\\n232  Chapter 8 Dimensionality ReductionFigure 812 Unrolled Swiss roll using LLE\\nHeres how LLE works first for each training instance xi the algorithm identifies its\\nk closest neighbors in the preceding code k  10 then tries to reconstruct xi as a\\nlinear function of these neighbors More specifically it finds the weights wij such that\\nthe squared distance between xi and j 1mwijxj is as small as possible assuming wij\\n 0 if xj is not one of the k closest neighbors of xi Thus the first step of LLE is the\\nconstrained optimization problem described in Equation 84', 'rs of xi Thus the first step of LLE is the\\nconstrained optimization problem described in Equation 84  where W is the weight\\nmatrix containing all the weights wij The second constraint simply normalizes the\\nweights for each training instance xi\\nLLE  233Equation 84 LLE step 1 linearly modeling local relationships\\nW argmin\\nW\\ni 1m\\nxi\\nj 1m\\nwijxj2\\nsubject towij 0 if xjis not one of the kcn of xi\\n\\nj 1m\\nwij 1 for i 1 2m\\nAfter this step the weight matrix W containing the weights wij encodes the local\\nlinear relationships between the training instances Now the second step is to map the\\ntraining instances into a ddimensional space where d  n while preserving these\\nlocal relationships as much as possible If zi is the image of xi in this ddimensional\\nspace then we want the squared distance between zi and j 1mwijzj to be as small\\nas possible This idea leads to the unconstrained optimization problem described in\\nEquation 85  It looks very similar to the first step but instead of keeping the instan\\nce', 'scribed in\\nEquation 85  It looks very similar to the first step but instead of keeping the instan\\nces fixed and finding the optimal weights we are doing the reverse keeping the\\nweights fixed and finding the optimal position of the instances images in the low\\ndimensional space Note that Z is the matrix containing all zi\\nEquation 85 LLE step 2 reducing dimensionality while preserving relationships\\nZ argmin\\nZ\\ni 1m\\nzi\\nj 1m\\nwijzj2\\nScikitLearns LLE implementation has the following computational complexity\\nOm logmn logk for finding the k nearest neighbors Omnk3 for optimizing the\\nweights and Odm2 for constructing the lowdimensional representations Unfortu\\nnately the m2 in the last term makes this algorithm scale poorly to very large datasets\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques several of which are\\navailable in ScikitLearn Here are some of the most popular\\nMultidimensional Scaling  MDS reduces dimensionality while trying to preserv', 'ome of the most popular\\nMultidimensional Scaling  MDS reduces dimensionality while trying to preserve\\nthe distances between the instances see Figure 813 \\n234  Chapter 8 Dimensionality Reduction9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodesIsomap  creates a graph by connecting each instance to its nearest neighbors then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances\\ntDistributed Stochastic Neighbor Embedding  tSNE reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart It is\\nmostly used for visualization in particular to visualize clusters of instances in\\nhighdimensional space eg to visualize the MNIST images in 2D\\nLinear Discriminant Analysis  LDA is actually a classification algorithm but dur\\ning training it learns the most discriminative axes between the classes and these\\naxes can then be used to define a hyperplane onto which ', 'minative axes between the classes and these\\naxes can then be used to define a hyperplane onto which to project the data The\\nbenefit is that the projection will keep classes as far apart as possible so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier\\nFigure 813 Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1What are the main motivations for reducing a datasets dimensionality What are\\nthe main drawbacks\\n2What is the curse of dimensionality\\n3Once a datasets dimensionality has been reduced is it possible to reverse the\\noperation If so how If not why\\n4Can PCA be used to reduce the dimensionality of a highly nonlinear dataset\\n5Suppose you perform PCA on a 1000dimensional dataset setting the explained\\nvariance ratio to 95 How many dimensions will the resulting dataset have\\nExercises  2356In what cases would you use vanilla PCA Incremental PCA Randomized PCA\\nor Kernel PCA\\n7How can you evaluate the perf', 'ould you use vanilla PCA Incremental PCA Randomized PCA\\nor Kernel PCA\\n7How can you evaluate the performance of a dimensionality reduction algorithm\\non your dataset\\n8Does it make any sense to chain two different dimensionality reduction algo\\nrithms\\n9Load the MNIST dataset introduced in Chapter 3  and split it into a training set\\nand a test set take the first 60000 instances for training and the remaining\\n10000 for testing Train a Random Forest classifier on the dataset and time how\\nlong it takes then evaluate the resulting model on the test set Next use PCA to\\nreduce the datasets dimensionality with an explained variance ratio of 95\\nTrain a new Random Forest classifier on the reduced dataset and see how long it\\ntakes Was training much faster Next evaluate the classifier on the test set how\\ndoes it compare to the previous classifier\\n10Use tSNE to reduce the MNIST dataset down to two dimensions and plot the\\nresult using Matplotlib Y ou can use a scatterplot using 10 different colors to re', 'ions and plot the\\nresult using Matplotlib Y ou can use a scatterplot using 10 different colors to rep\\nresent each images target class Alternatively you can write colored digits at the\\nlocation of each instance or even plot scaleddown versions of the digit images\\nthemselves if you plot all digits the visualization will be too cluttered so you\\nshould either draw a random sample or plot an instance only if no other instance\\nhas already been plotted at a close distance Y ou should get a nice visualization\\nwith wellseparated clusters of digits Try using other dimensionality reduction\\nalgorithms such as PCA LLE or MDS and compare the resulting visualizations\\nSolutions to these exercises are available in \\n236  Chapter 8 Dimensionality ReductionCHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these t', 'he writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 9 in the final\\nrelease of the book\\nAlthough most of the applications of Machine Learning today are based on super\\nvised learning and as a result this is where most of the investments go to the vast\\nmajority of the available data is actually unlabeled we have the input features X but\\nwe do not have the labels y Y ann LeCun famously said that if intelligence was a cake\\nunsupervised learning would be the cake supervised learning would be the icing on\\nthe cake and reinforcement learning would be the cherry on the cake  In other\\nwords there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into\\nFor example say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective Y ou can\\nfairly easily create a system that will take pictures', ' and detect which items are defective Y ou can\\nfairly easily create a system that will take pictures automatically and this might give\\nyou thousands of pictures every day Y ou can then build a reasonably large dataset in\\njust a few weeks But wait there are no labels If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not you will need to label\\nevery single picture as defective or normal  This will generally require human\\nexperts to sit down and manually go through all the pictures This is a long costly\\nand tedious task so it will usually only be done on a small subset of the available pic\\ntures As a result the labeled dataset will be quite small and the classifiers perfor\\nmance will be disappointing Moreover every time the company makes any change to\\nits products the whole process will need to be started over from scratch Wouldnt it\\n237be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every pic', 'eat if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture Enter unsupervised learning\\nIn Chapter 8  we looked at the most common unsupervised learning task dimension\\nality reduction In this chapter we will look at a few more unsupervised learning tasks\\nand algorithms\\nClustering  the goal is to group similar instances together into clusters  This is a\\ngreat tool for data analysis customer segmentation recommender systems\\nsearch engines image segmentation semisupervised learning dimensionality\\nreduction and more\\nAnomaly detection  the objective is to learn what normal data looks like and\\nuse this to detect abnormal instances such as defective items on a production\\nline or a new trend in a time series\\nDensity estimation  this is the task of estimating the probability density function\\nPDF of the random process that generated the dataset This is commonly used\\nfor anomaly detection instances located in very lowdensity regions are likely to\\nbe an', 'commonly used\\nfor anomaly detection instances located in very lowdensity regions are likely to\\nbe anomalies It is also useful for data analysis and visualization\\nReady for some cake We will start with clustering using KMeans and DBSCAN\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation clustering and anomaly detection\\nClustering\\nAs you enjoy a hike in the mountains you stumble upon a plant you have never seen\\nbefore Y ou look around and you notice a few more They are not perfectly identical\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species or at least the same genus Y ou may need a botanist to tell you what\\nspecies that is but you certainly dont need an expert to identify groups of similar\\nlooking objects This is called clustering  it is the task of identifying similar instances\\nand assigning them to clusters  ie groups of similar instances\\nJust like in classification each instance gets a', 'ng them to clusters  ie groups of similar instances\\nJust like in classification each instance gets assigned to a group However this is an\\nunsupervised task Consider Figure 91  on the left is the iris dataset introduced in\\nChapter 4  where each instances species ie its class is represented with a different\\nmarker It is a labeled dataset for which classification algorithms such as Logistic\\nRegression SVMs or Random Forest classifiers are well suited On the right is the\\nsame dataset but without the labels so you cannot use a classification algorithm any\\nmore This is where clustering algorithms step in many of them can easily detect the\\ntop left cluster It is also quite easy to see with our own eyes but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct subclusters That\\nsaid the dataset actually has two additional features sepal length and width not\\n238  Chapter 9 Unsupervised Learning Techniquesrepresented here and clustering algorithms can make good us', 'hapter 9 Unsupervised Learning Techniquesrepresented here and clustering algorithms can make good use of all features so in\\nfact they identify the three clusters fairly well eg using a Gaussian mixture model\\nonly 5 instances out of 150 are assigned to the wrong cluster\\nFigure 91 Classification  left  versus clustering right\\nClustering is used in a wide variety of applications including\\nFor customer segmentation you can cluster your customers based on their pur\\nchases their activity on your website and so on This is useful to understand who\\nyour customers are and what they need so you can adapt your products and\\nmarketing campaigns to each segment For example this can be useful in recom\\nmender systems  to suggest content that other users in the same cluster enjoyed\\nFor data analysis when analyzing a new dataset it is often useful to first discover\\nclusters of similar instances as it is often easier to analyze clusters separately\\nAs a dimensionality reduction technique once a dataset has', 'ten easier to analyze clusters separately\\nAs a dimensionality reduction technique once a dataset has been clustered it is\\nusually possible to measure each instances affinity  with each cluster affinity is\\nany measure of how well an instance fits into a cluster Each instances feature\\nvector x can then be replaced with the vector of its cluster affinities If there are k\\nclusters then this vector is k dimensional This is typically much lower dimen\\nsional than the original feature vector but it can preserve enough information for\\nfurther processing\\nFor anomaly detection  also called outlier detection  any instance that has a low\\naffinity to all the clusters is likely to be an anomaly For example if you have clus\\ntered the users of your website based on their behavior you can detect users with\\nunusual behavior such as an unusual number of requests per second and so on\\nAnomaly detection is particularly useful in detecting defects in manufacturing or\\nfor fraud detection \\nFor semisupervised le', 'ticularly useful in detecting defects in manufacturing or\\nfor fraud detection \\nFor semisupervised learning if you only have a few labels you could perform\\nclustering and propagate the labels to all the instances in the same cluster This\\ncan greatly increase the amount of labels available for a subsequent supervised\\nlearning algorithm and thus improve its performance\\nClustering  2391Least square quantization in PCM  Stuart P  Lloyd 1982For search engines for example some search engines let you search for images\\nthat are similar to a reference image To build such a system you would first\\napply a clustering algorithm to all the images in your database similar images\\nwould end up in the same cluster Then when a user provides a reference image\\nall you need to do is to find this images cluster using the trained clustering\\nmodel and you can then simply return all the images from this cluster\\nTo segment an image by clustering pixels according to their color then replacing\\neach pixels color wit', ' segment an image by clustering pixels according to their color then replacing\\neach pixels color with the mean color of its cluster it is possible to reduce the\\nnumber of different colors in the image considerably This technique is used in\\nmany object detection and tracking systems as it makes it easier to detect the\\ncontour of each object\\nThere is no universal definition of what a cluster is it really depends on the context\\nand different algorithms will capture different kinds of clusters For example some\\nalgorithms look for instances centered around a particular point called a centroid \\nOthers look for continuous regions of densely packed instances these clusters can\\ntake on any shape Some algorithms are hierarchical looking for clusters of clusters\\nAnd the list goes on\\nIn this section we will look at two popular clustering algorithms KMeans and\\nDBSCAN and we will show some of their applications such as nonlinear dimen\\nsionality reduction semisupervised learning and anomaly detection', 'plications such as nonlinear dimen\\nsionality reduction semisupervised learning and anomaly detection\\nKMeans\\nConsider the unlabeled dataset represented in Figure 92  you can clearly see 5 blobs\\nof instances The KMeans algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently often in just a few iterations It was pro\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulsecode modula\\ntion but it was only published outside of the company in 1982 in a paper titled\\nLeast square quantization in PCM 1 By then in 1965 Edward W  Forgy had pub\\nlished virtually the same algorithm so KMeans is sometimes referred to as Lloyd\\nForgy\\n240  Chapter 9 Unsupervised Learning TechniquesFigure 92 An unlabeled dataset composed of five blobs of instances\\nLets train a KMeans clusterer on this dataset It will try to find each blobs center and\\nassign each instance to the closest blob\\nfrom sklearncluster  import KMeans\\nk  5\\nkmeans  KMeansnclusters k\\nypre', ' instance to the closest blob\\nfrom sklearncluster  import KMeans\\nk  5\\nkmeans  KMeansnclusters k\\nypred  kmeansfitpredict X\\nNote that you have to specify the number of clusters k that the algorithm must find\\nIn this example it is pretty obvious from looking at the data that k should be set to 5\\nbut in general it is not that easy We will discuss this shortly\\nEach instance was assigned to one of the 5 clusters In the context of clustering an\\ninstances label  is the index of the cluster that this instance gets assigned to by the\\nalgorithm this is not to be confused with the class labels in classification remember\\nthat clustering is an unsupervised learning task The KMeans  instance preserves a\\ncopy of the labels of the instances it was trained on available via the labels  instance\\nvariable\\n ypred\\narray4 0 1  2 1 0 dtypeint32\\n ypred is kmeanslabels\\nTrue\\nWe can also take a look at the 5 centroids that the algorithm found\\n kmeansclustercenters\\narray280389616  180117999\\n        020876306  22555', 'ds that the algorithm found\\n kmeansclustercenters\\narray280389616  180117999\\n        020876306  225551336\\n       279290307  279641063\\n       146679593  228585348\\n       280037642  130082566\\nOf course you can easily assign new instances to the cluster whose centroid is closest\\nClustering  241 Xnew  nparray0 2 3 2 3 3 3 25\\n kmeanspredictXnew\\narray1 1 2 2 dtypeint32\\nIf you plot the clusters decision boundaries you get a Voronoi tessellation see\\nFigure 93  where each centroid is represented with an X\\nFigure 93 KMeans decision boundaries Voronoi tessellation\\nThe vast majority of the instances were clearly assigned to the appropriate cluster but\\na few instances were probably mislabeled especially near the boundary between the\\ntop left cluster and the central cluster Indeed the KMeans algorithm does not\\nbehave very well when the blobs have very different diameters since all it cares about\\nwhen assigning an instance to a cluster is the distance to the centroid\\nInstead of assigning each instance', 'ssigning an instance to a cluster is the distance to the centroid\\nInstead of assigning each instance to a single cluster which is called hard clustering  it\\ncan be useful to just give each instance a score per cluster this is called soft clustering \\nFor example the score can be the distance between the instance and the centroid or\\nconversely it can be a similarity score or affinity such as the Gaussian Radial Basis\\nFunction introduced in Chapter 5  In the KMeans  class the transform  method\\nmeasures the distance from each instance to every centroid\\n kmeanstransform Xnew\\narray281093633 032995317 29042344  149439034 288633901\\n       580730058 280290755 584739223 44759332  584236351\\n       121475352 329399768 029040966 169136631 171086031\\n       072581411 321806371 036159148 154808703 121567622\\nIn this example the first instance in Xnew  is located at a distance of 281 from the\\nfirst centroid 033 from the second centroid 290 from the third centroid 149 from\\nthe fourth centroid and 287 fro', 'id 033 from the second centroid 290 from the third centroid 149 from\\nthe fourth centroid and 287 from the fifth centroid If you have a highdimensional\\ndataset and you transform it this way you end up with a kdimensional dataset this\\ncan be a very efficient nonlinear dimensionality reduction technique\\n242  Chapter 9 Unsupervised Learning Techniques2This can be proven by pointing out that the mean squared distance between the instances and their closest\\ncentroid can only go down at each stepThe KMeans Algorithm\\nSo how does the algorithm work Well it is really quite simple Suppose you were\\ngiven the centroids you could easily label all the instances in the dataset by assigning\\neach of them to the cluster whose centroid is closest Conversely if you were given all\\nthe instance labels you could easily locate all the centroids by computing the mean of\\nthe instances for each cluster But you are given neither the labels nor the centroids\\nso how can you proceed Well just start by placing the cen', 'given neither the labels nor the centroids\\nso how can you proceed Well just start by placing the centroids randomly eg by\\npicking k instances at random and using their locations as centroids Then label the\\ninstances update the centroids label the instances update the centroids and so on\\nuntil the centroids stop moving The algorithm is guaranteed to converge in a finite\\nnumber of steps usually quite small it will not oscillate forever2 Y ou can see the\\nalgorithm in action in Figure 94  the centroids are initialized randomly top left\\nthen the instances are labeled top right then the centroids are updated center left\\nthe instances are relabeled center right and so on As you can see in just 3 itera\\ntions the algorithm has reached a clustering that seems close to optimal\\nFigure 94 The KMeans algorithm\\nClustering  243The computational complexity of the algorithm is generally linear\\nwith regards to the number of instances m the number of clusters\\nk and the number of dimensions n However this ', 's to the number of instances m the number of clusters\\nk and the number of dimensions n However this is only true when\\nthe data has a clustering structure If it does not then in the worst\\ncase scenario the complexity can increase exponentially with the\\nnumber of instances In practice however this rarely happens and\\nKMeans is generally one of the fastest clustering algorithms\\nUnfortunately although the algorithm is guaranteed to converge it may not converge\\nto the right solution ie it may converge to a local optimum this depends on the\\ncentroid initialization For example Figure 95  shows two suboptimal solutions that\\nthe algorithm can converge to if you are not lucky with the random initialization step\\nFigure 95 Suboptimal solutions due to unlucky centroid initializations\\nLets look at a few ways you can mitigate this risk by improving the centroid initializa\\ntion\\nCentroid Initialization Methods\\nIf you happen to know approximately where the centroids should be eg if you ran\\nanother cluste', 'thods\\nIf you happen to know approximately where the centroids should be eg if you ran\\nanother clustering algorithm earlier then you can set the init  hyperparameter to a\\nNumPy array containing the list of centroids and set ninit  to 1\\ngoodinit   nparray3 3 3 2 3 1 1 2 0 2\\nkmeans  KMeansnclusters 5 initgoodinit  ninit1\\nAnother solution is to run the algorithm multiple times with different random initial\\nizations and keep the best solution This is controlled by the ninit  hyperparameter\\nby default it is equal to 10 which means that the whole algorithm described earlier\\nactually runs 10 times when you call fit  and ScikitLearn keeps the best solution\\nBut how exactly does it know which solution is the best Well of course it uses a per\\nformance metric It is called the models inertia  this is the mean squared distance\\nbetween each instance and its closest centroid It is roughly equal to 2233 for the\\nmodel on the left of Figure 95  2375 for the model on the right of Figure 95  and\\n2116 for th', 'or the\\nmodel on the left of Figure 95  2375 for the model on the right of Figure 95  and\\n2116 for the model in Figure 93  The KMeans  class runs the algorithm ninit  times\\nand keeps the model with the lowest inertia in this example the model in Figure 93\\nwill be selected unless we are very unlucky with ninit  consecutive random initiali\\n244  Chapter 9 Unsupervised Learning Techniques3kmeans The advantages of careful seeding  David Arthur and Sergei Vassilvitskii 2006\\n4Using the Triangle Inequality to Accelerate kMeans  Charles Elkan 2003zations If you are curious a models inertia is accessible via the inertia  instance\\nvariable\\n kmeansinertia\\n21159853725816856\\nThe score  method returns the negative inertia Why negative Well it is because a\\npredictors score  method must always respect the  great is better  rule\\n kmeansscoreX\\n21159853725816856\\nAn important improvement to the KMeans algorithm called KMeans  was pro\\nposed in a 2006 paper  by David Arthur and Sergei Vassilvitskii3 they intr', 'hm called KMeans  was pro\\nposed in a 2006 paper  by David Arthur and Sergei Vassilvitskii3 they introduced a\\nsmarter initialization step that tends to select centroids that are distant from one\\nanother and this makes the KMeans algorithm much less likely to converge to a sub\\noptimal solution They showed that the additional computation required for the\\nsmarter initialization step is well worth it since it makes it possible to drastically\\nreduce the number of times the algorithm needs to be run to find the optimal solu\\ntion Here is the KMeans initialization algorithm\\nTake one centroid c1 chosen uniformly at random from the dataset\\nTake a new centroid ci choosing an instance xi with probability Di2\\nj 1mDj2 where D xi is the distance between the instance xi and the closest\\ncentroid that was already chosen This probability distribution ensures that\\ninstances further away from already chosen centroids are much more likely be\\nselected as centroids\\nRepeat the previous step until all k centroid', 'entroids are much more likely be\\nselected as centroids\\nRepeat the previous step until all k centroids have been chosen\\nThe KMeans  class actually uses this initialization method by default If you want to\\nforce it to use the original method ie picking k instances randomly to define the\\ninitial centroids then you can set the init  hyperparameter to random  Y ou will\\nrarely need to do this\\nAccelerated KMeans and Minibatch KMeans\\nAnother important improvement to the KMeans algorithm was proposed in a 2003\\npaper  by Charles Elkan4 It considerably accelerates the algorithm by avoiding many\\nunnecessary distance calculations this is achieved by exploiting the triangle inequal\\nClustering  2455The triangle inequality is AC  AB  BC where A B and C are three points and AB AC and BC are the\\ndistances between these points\\n6WebScale KMeans Clustering  David Sculley 2010ity ie the straight line is always the shortest5 and by keeping track of lower and\\nupper bounds for distances between instances and c', 's the shortest5 and by keeping track of lower and\\nupper bounds for distances between instances and centroids This is the algorithm\\nused by default by the KMeans  class but you can force it to use the original algorithm\\nby setting the algorithm  hyperparameter to full  although you probably will\\nnever need to\\nY et another important variant of the KMeans algorithm was proposed in a 2010\\npaper  by David Sculley6 Instead of using the full dataset at each iteration the algo\\nrithm is capable of using minibatches moving the centroids just slightly at each iter\\nation This speeds up the algorithm typically by a factor of 3 or 4 and makes it\\npossible to cluster huge datasets that do not fit in memory ScikitLearn implements\\nthis algorithm in the MiniBatchKMeans  class Y ou can just use this class like the\\nKMeans  class\\nfrom sklearncluster  import MiniBatchKMeans\\nminibatchkmeans   MiniBatchKMeans nclusters 5\\nminibatchkmeans fitX\\nIf the dataset does not fit in memory the simplest option is to use t', 'lusters 5\\nminibatchkmeans fitX\\nIf the dataset does not fit in memory the simplest option is to use the memmap  class as\\nwe did for incremental PCA in Chapter 8  Alternatively you can pass one minibatch\\nat a time to the partialfit  method but this will require much more work since\\nyou will need to perform multiple initializations and select the best one yourself see\\nthe notebook for an example\\nAlthough the Minibatch KMeans algorithm is much faster than the regular K\\nMeans algorithm its inertia is generally slightly worse especially as the number of\\nclusters increases Y ou can see this in Figure 96  the plot on the left compares the\\ninertias of Minibatch KMeans and regular KMeans models trained on the previous\\ndataset using various numbers of clusters k The difference between the two curves\\nremains fairly constant but this difference becomes more and more significant as k\\nincreases since the inertia becomes smaller and smaller However in the plot on the\\nright you can see that Minibatch K', 'he inertia becomes smaller and smaller However in the plot on the\\nright you can see that Minibatch KMeans is much faster than regular KMeans and\\nthis difference increases with k\\n246  Chapter 9 Unsupervised Learning TechniquesFigure 96 Minibatch KMeans vs KMeans worse inertia as k increases left  but\\nmuch faster right\\nFinding the Optimal Number of Clusters\\nSo far we have set the number of clusters k to 5 because it was obvious by looking at\\nthe data that this is the correct number of clusters But in general it will not be so\\neasy to know how to set k and the result might be quite bad if you set it to the wrong\\nvalue For example as you can see in Figure 97  setting k to 3 or 8 results in fairly\\nbad models\\nFigure 97 Bad choices for the number of clusters\\nY ou might be thinking that we could just pick the model with the lowest inertia\\nright Unfortunately it is not that simple The inertia for k3 is 6532 which is much\\nhigher than for k5 which was 2116 but with k8 the inertia is just 1191 The', ' k3 is 6532 which is much\\nhigher than for k5 which was 2116 but with k8 the inertia is just 1191 The\\ninertia is not a good performance metric when trying to choose k since it keeps get\\nting lower as we increase k Indeed the more clusters there are the closer each\\ninstance will be to its closest centroid and therefore the lower the inertia will be Lets\\nplot the inertia as a function of k see Figure 98 \\nClustering  247Figure 98 Selecting the number of clusters k using the elbow rule\\nAs you can see the inertia drops very quickly as we increase k up to 4 but then it\\ndecreases much more slowly as we keep increasing k This curve has roughly the\\nshape of an arm and there is an elbow at k4 so if we did not know better it would\\nbe a good choice any lower value would be dramatic while any higher value would\\nnot help much and we might just be splitting perfectly good clusters in half for no\\ngood reason\\nThis technique for choosing the best value for the number of clusters is rather coarse\\nA more p', 'ason\\nThis technique for choosing the best value for the number of clusters is rather coarse\\nA more precise approach but also more computationally expensive is to use the sil\\nhouette score  which is the mean silhouette coefficient  over all the instances An instan\\nces silhouette coefficient is equal to  b  a  max a b where a is the mean distance\\nto the other instances in the same cluster it is the mean intracluster distance and b\\nis the mean nearestcluster distance that is the mean distance to the instances of the\\nnext closest cluster defined as the one that minimizes b excluding the instances own\\ncluster The silhouette coefficient can vary between 1 and 1 a coefficient close to\\n1 means that the instance is well inside its own cluster and far from other clusters\\nwhile a coefficient close to 0 means that it is close to a cluster boundary and finally a\\ncoefficient close to 1 means that the instance may have been assigned to the wrong\\ncluster To compute the silhouette score you can use Sci', 'instance may have been assigned to the wrong\\ncluster To compute the silhouette score you can use ScikitLearns silhou\\nettescore  function giving it all the instances in the dataset and the labels they\\nwere assigned\\n from sklearnmetrics  import silhouettescore\\n silhouettescore X kmeanslabels\\n0655517642572828\\nLets compare the silhouette scores for different numbers of clusters see Figure 99 \\n248  Chapter 9 Unsupervised Learning TechniquesFigure 99 Selecting the number of clusters k using the silhouette score\\nAs you can see this visualization is much richer than the previous one in particular\\nalthough it confirms that k4 is a very good choice it also underlines the fact that\\nk5 is quite good as well and much better than k6 or 7 This was not visible when\\ncomparing inertias\\nAn even more informative visualization is obtained when you plot every instances\\nsilhouette coefficient sorted by the cluster they are assigned to and by the value of the\\ncoefficient This is called a silhouette diagram  s', 'ster they are assigned to and by the value of the\\ncoefficient This is called a silhouette diagram  see Figure 910 \\nFigure 910 Silouhette analysis comparing the silhouette diagrams for various values of\\nk\\nThe vertical dashed lines represent the silhouette score for each number of clusters\\nWhen most of the instances in a cluster have a lower coefficient than this score ie if\\nmany of the instances stop short of the dashed line ending to the left of it then the\\ncluster is rather bad since this means its instances are much too close to other clus\\nClustering  249ters We can see that when k3 and when k6 we get bad clusters But when k4 or\\nk5 the clusters look pretty good  most instances extend beyond the dashed line to\\nthe right and closer to 10 When k4 the cluster at index 1 the third from the top\\nis rather big while when k5 all clusters have similar sizes so even though the over\\nall silhouette score from k4 is slightly greater than for k5 it seems like a good idea\\nto use k5 to get clusters o', ' score from k4 is slightly greater than for k5 it seems like a good idea\\nto use k5 to get clusters of similar sizes\\nLimits of KMeans\\nDespite its many merits most notably being fast and scalable KMeans is not perfect\\nAs we saw it is necessary to run the algorithm several times to avoid suboptimal sol\\nutions plus you need to specify the number of clusters which can be quite a hassle\\nMoreover KMeans does not behave very well when the clusters have varying sizes\\ndifferent densities or nonspherical shapes For example Figure 911  shows how K\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions\\ndensities and orientations\\nFigure 911 KMeans fails to cluster these ellipsoidal blobs properly\\nAs you can see neither of these solutions are any good The solution on the left is\\nbetter but it still chops off 25 of the middle cluster and assigns it to the cluster on\\nthe right The solution on the right is just terrible even though its inertia is lower So\\ndepending on the', 'ight The solution on the right is just terrible even though its inertia is lower So\\ndepending on the data different clustering algorithms may perform better For exam\\nple on these types of elliptical clusters Gaussian mixture models work great\\nIt is important to scale the input features before you run KMeans\\nor else the clusters may be very stretched and KMeans will per\\nform poorly Scaling the features does not guarantee that all the\\nclusters will be nice and spherical but it generally improves things\\nNow lets look at a few ways we can benefit from clustering We will use KMeans but\\nfeel free to experiment with other clustering algorithms\\n250  Chapter 9 Unsupervised Learning TechniquesUsing clustering for image segmentation\\nImage segmentation  is the task of partitioning an image into multiple segments In\\nsemantic segmentation  all pixels that are part of the same object type get assigned to\\nthe same segment For example in a selfdriving cars vision system all pixels that are\\npart of a pe', 'to\\nthe same segment For example in a selfdriving cars vision system all pixels that are\\npart of a pedestrians image might be assigned to the pedestrian segment there\\nwould just be one segment containing all the pedestrians In instance segmentation \\nall pixels that are part of the same individual object are assigned to the same segment\\nIn this case there would be a different segment for each pedestrian The state of the\\nart in semantic or instance segmentation today is achieved using complex architec\\ntures based on convolutional neural networks see Chapter 14  Here we are going to\\ndo something much simpler color segmentation  We will simply assign pixels to the\\nsame segment if they have a similar color In some applications this may be sufficient\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region color segmentation may be just fine\\nFirst lets load the image see the upper left image in Figure 912  using Matplotlibs\\nimread  functio', '\\nFirst lets load the image see the upper left image in Figure 912  using Matplotlibs\\nimread  function\\n from matplotlibimage  import imread   you could also use imageioimread\\n image  imreadospathjoinimages clustering ladybugpng \\n imageshape\\n533 800 3\\nThe image is represented as a 3D array the first dimensions size is the height the\\nsecond is the width and the third is the number of color channels in this case red\\ngreen and blue RGB In other words for each pixel there is a 3D vector containing\\nthe intensities of red green and blue each between 00 and 10 or between 0 and 255\\nif you use imageioimread  Some images may have less channels such as gray\\nscale images one channel or more channels such as images with an additional\\nalpha channel  for transparency or satellite images which often contain channels for\\nmany light frequencies eg infrared The following code reshapes the array to get a\\nlong list of RGB colors then it clusters these colors using KMeans For example it\\nmay identify a color c', ' list of RGB colors then it clusters these colors using KMeans For example it\\nmay identify a color cluster for all shades of green Next for each color eg dark\\ngreen it looks for the mean color of the pixels color cluster For example all shades\\nof green may be replaced with the same light green color assuming the mean color of\\nthe green cluster is light green Finally it reshapes this long list of colors to get the\\nsame shape as the original image And were done\\nX  imagereshape1 3\\nkmeans  KMeansnclusters 8fitX\\nsegmentedimg   kmeansclustercenters kmeanslabels\\nsegmentedimg   segmentedimg reshapeimageshape\\nThis outputs the image shown in the upper right of Figure 912  Y ou can experiment\\nwith various numbers of clusters as shown in the figure When you use less than 8\\nclusters notice that the ladybugs flashy red color fails to get a cluster of its own it\\nClustering  251gets merged with colors from the environment This is due to the fact that the lady\\nbug is quite small much smaller than the r', 'rom the environment This is due to the fact that the lady\\nbug is quite small much smaller than the rest of the image so even though its color is\\nflashy KMeans fails to dedicate a cluster to it as mentioned earlier KMeans prefers\\nclusters of similar sizes\\nFigure 912 Image segmentation using KMeans with various numbers of color clusters\\nThat was not too hard was it Now lets look at another application of clustering pre\\nprocessing\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction in particular as a\\npreprocessing step before a supervised learning algorithm For example lets tackle\\nthe digits dataset  which is a simple MNISTlike dataset containing 1797 grayscale 88\\nimages representing digits 0 to 9 First lets load the dataset\\nfrom sklearndatasets  import loaddigits\\nXdigits  ydigits   loaddigits returnXy True\\nNow lets split it into a training set and a test set\\nfrom sklearnmodelselection  import traintestsplit\\nXtrain Xtest ytrain ytest  tra', ' set and a test set\\nfrom sklearnmodelselection  import traintestsplit\\nXtrain Xtest ytrain ytest  traintestsplit Xdigits  ydigits \\nNext lets fit a Logistic Regression model\\nfrom sklearnlinearmodel  import LogisticRegression\\nlogreg  LogisticRegression randomstate 42\\nlogregfitXtrain ytrain\\nLets evaluate its accuracy on the test set\\n logregscoreXtest ytest\\n09666666666666667\\n252  Chapter 9 Unsupervised Learning TechniquesOkay thats our baseline 967 accuracy Lets see if we can do better by using K\\nMeans as a preprocessing step We will create a pipeline that will first cluster the\\ntraining set into 50 clusters and replace the images with their distances to these 50\\nclusters then apply a logistic regression model\\nAlthough it is tempting to define the number of clusters to 10\\nsince there are 10 different digits it is unlikely to perform well\\nbecause there are several different ways to write each digit\\nfrom sklearnpipeline  import Pipeline\\npipeline   Pipeline \\n    kmeans  KMeansnclusters 50\\n    ', 'digit\\nfrom sklearnpipeline  import Pipeline\\npipeline   Pipeline \\n    kmeans  KMeansnclusters 50\\n    logreg  LogisticRegression \\n\\npipeline fitXtrain ytrain\\nNow lets evaluate this classification pipeline\\n pipeline scoreXtest ytest\\n09822222222222222\\nHow about that We almost divided the error rate by a factor of 2\\nBut we chose the number of clusters k completely arbitrarily we can surely do better\\nSince KMeans is just a preprocessing step in a classification pipeline finding a good\\nvalue for k is much simpler than earlier theres no need to perform silhouette analysis\\nor minimize the inertia the best value of k is simply the one that results in the best\\nclassification performance during crossvalidation Lets use GridSearchCV  to find the\\noptimal number of clusters\\nfrom sklearnmodelselection  import GridSearchCV\\nparamgrid   dictkmeansnclusters range2 100\\ngridclf   GridSearchCV pipeline  paramgrid  cv3 verbose2\\ngridclf fitXtrain ytrain\\nLets look at best value for k and the performance of the r', 'id  cv3 verbose2\\ngridclf fitXtrain ytrain\\nLets look at best value for k and the performance of the resulting pipeline\\n gridclf bestparams\\nkmeansnclusters 90\\n gridclf scoreXtest ytest\\n09844444444444445\\nWith k90 clusters we get a small accuracy boost reaching 984 accuracy on the\\ntest set Cool\\nClustering  253Using Clustering for SemiSupervised Learning\\nAnother use case for clustering is in semisupervised learning when we have plenty\\nof unlabeled instances and very few labeled instances Lets train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset\\nnlabeled   50\\nlogreg  LogisticRegression \\nlogregfitXtrainnlabeled  ytrainnlabeled \\nWhat is the performance of this model on the test set\\n logregscoreXtest ytest\\n08266666666666667\\nThe accuracy is just 827 it should come as no surprise that this is much lower than\\nearlier when we trained the model on the full training set Lets see how we can do\\nbetter First lets cluster the training set into 50 clusters then for', 'ning set Lets see how we can do\\nbetter First lets cluster the training set into 50 clusters then for each cluster lets find\\nthe image closest to the centroid We will call these images the representative images\\nk  50\\nkmeans  KMeansnclusters k\\nXdigitsdist   kmeansfittransform Xtrain\\nrepresentativedigitidx   npargminXdigitsdist  axis0\\nXrepresentativedigits   Xtrainrepresentativedigitidx \\nFigure 913  shows these 50 representative images\\nFigure 913 Fifty  representative digit images one per cluster\\nNow lets look at each image and manually label it\\nyrepresentativedigits   nparray4 8 0 6 8 3  7 6 2 3 1 1\\nNow we have a dataset with just 50 labeled instances but instead of being completely\\nrandom instances each of them is a representative image of its cluster Lets see if the\\nperformance is any better\\n logreg  LogisticRegression \\n logregfitXrepresentativedigits  yrepresentativedigits \\n logregscoreXtest ytest\\n09244444444444444\\nWow We jumped from 827 accuracy to 924 although we are still only trai', 'Xtest ytest\\n09244444444444444\\nWow We jumped from 827 accuracy to 924 although we are still only training\\nthe model on 50 instances Since it is often costly and painful to label instances espe\\n254  Chapter 9 Unsupervised Learning Techniquescially when it has to be done manually by experts it is a good idea to label representa\\ntive instances rather than just random instances\\nBut perhaps we can go one step further what if we propagated the labels to all the\\nother instances in the same cluster This is called label propagation \\nytrainpropagated   npemptylenXtrain dtypenpint32\\nfor i in rangek\\n    ytrainpropagated kmeanslabelsi  yrepresentativedigits i\\nNow lets train the model again and look at its performance\\n logreg  LogisticRegression \\n logregfitXtrain ytrainpropagated \\n logregscoreXtest ytest\\n09288888888888889\\nWe got a tiny little accuracy boost Better than nothing but not astounding The\\nproblem is that we propagated each representative instances label to all the instances\\nin the same clu', 'oblem is that we propagated each representative instances label to all the instances\\nin the same cluster including the instances located close to the cluster boundaries\\nwhich are more likely to be mislabeled Lets see what happens if we only propagate\\nthe labels to the 20 of the instances that are closest to the centroids\\npercentileclosest   20\\nXclusterdist   Xdigitsdist nparangelenXtrain kmeanslabels\\nfor i in rangek\\n    incluster   kmeanslabels  i\\n    clusterdist   Xclusterdist incluster \\n    cutoffdistance   nppercentile clusterdist  percentileclosest \\n    abovecutoff   Xclusterdist   cutoffdistance \\n    Xclusterdist incluster   abovecutoff   1\\npartiallypropagated   Xclusterdist   1\\nXtrainpartiallypropagated   Xtrainpartiallypropagated \\nytrainpartiallypropagated   ytrainpropagated partiallypropagated \\nNow lets train the model again on this partially propagated dataset\\n logreg  LogisticRegression \\n logregfitXtrainpartiallypropagated  ytrainpartiallypropagated \\n logregscoreXtest ytest\\n0', 'egression \\n logregfitXtrainpartiallypropagated  ytrainpartiallypropagated \\n logregscoreXtest ytest\\n09422222222222222\\nNice With just 50 labeled instances only 5 examples per class on average we got\\n942 performance which is pretty close to the performance of logistic regression on\\nthe fully labeled digits dataset which was 967 This is because the propagated\\nlabels are actually pretty good their accuracy is very close to 99\\nClustering  255 npmeanytrainpartiallypropagated   ytrainpartiallypropagated \\n09896907216494846\\nActive Learning\\nTo continue improving your model and your training set the next step could be to do\\na few rounds of active learning  this is when a human expert interacts with the learn\\ning algorithm providing labels when the algorithm needs them There are many dif\\nferent strategies for active learning but one of the most common ones is called\\nuncertainty sampling \\nThe model is trained on the labeled instances gathered so far and this model is\\nused to make predictions on all ', ' trained on the labeled instances gathered so far and this model is\\nused to make predictions on all the unlabeled instances\\nThe instances for which the model is most uncertain ie when its estimated\\nprobability is lowest must be labeled by the expert\\nThen you just iterate this process again and again until the performance\\nimprovement stops being worth the labeling effort\\nOther strategies include labeling the instances that would result in the largest model\\nchange or the largest drop in the models validation error or the instances that differ\\nent models disagree on eg an SVM a Random Forest and so on\\nBefore we move on to Gaussian mixture models lets take a look at DBSCAN\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation This approach allows the algorithm to identify clusters of\\narbitrary shapes\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density It is actually\\nquite simple\\nFor each instance the algor', 'usters as continuous regions of high density It is actually\\nquite simple\\nFor each instance the algorithm counts how many instances are located within a\\nsmall distance  epsilon from it This region is called the instances \\nneighborhood \\nIf an instance has at least minsamples  instances in its neighborhood includ\\ning itself then it is considered a core instance  In other words core instances are\\nthose that are located in dense regions\\nAll instances in the neighborhood of a core instance belong to the same cluster\\nThis may include other core instances therefore a long sequence of neighboring\\ncore instances forms a single cluster\\n256  Chapter 9 Unsupervised Learning TechniquesAny instance that is not a core instance and does not have one in its neighbor\\nhood is considered an anomaly\\nThis algorithm works well if all the clusters are dense enough and they are well sepa\\nrated by lowdensity regions The DBSCAN  class in ScikitLearn is as simple to use as\\nyou might expect Lets test it on the moon', 'ns The DBSCAN  class in ScikitLearn is as simple to use as\\nyou might expect Lets test it on the moons dataset introduced in Chapter 5 \\nfrom sklearncluster  import DBSCAN\\nfrom sklearndatasets  import makemoons\\nX y  makemoons nsamples 1000 noise005\\ndbscan  DBSCANeps005 minsamples 5\\ndbscanfitX\\nThe labels of all the instances are now available in the labels  instance variable\\n dbscanlabels\\narray 0  2 1 1  1  0  0  0   3  2  3  3  4  2  6  3\\nNotice that some instances have a cluster index equal to 1 this means that they are\\nconsidered as anomalies by the algorithm The indices of the core instances are avail\\nable in the coresampleindices  instance variable and the core instances them\\nselves are available in the components  instance variable\\n lendbscancoresampleindices \\n808\\n dbscancoresampleindices\\narray 0  4  5  6  7  8 10 11  992 993 995 997 998 999\\n dbscancomponents\\narray002137124  040618608\\n       084192557  053058695\\n                  \\n       094355873  03278936 \\n        079419406  06077', '\\n       084192557  053058695\\n                  \\n       094355873  03278936 \\n        079419406  060777171\\nThis clustering is represented in the left plot of Figure 914  As you can see it identi\\nfied quite a lot of anomalies plus 7 different clusters How disappointing Fortunately\\nif we widen each instances neighborhood by increasing eps to 02 we get the cluster\\ning on the right which looks perfect Lets continue with this model\\nFigure 914 DBSCAN clustering using two different  neighborhood radiuses\\nClustering  257Somewhat surprisingly the DBSCAN class does not have a predict  method\\nalthough it has a fitpredict  method In other words it cannot predict which\\ncluster a new instance belongs to The rationale for this decision is that several classi\\nfication algorithms could make sense here and it is easy enough to train one for\\nexample a KNeighborsClassifier \\nfrom sklearnneighbors  import KNeighborsClassifier\\nknn  KNeighborsClassifier nneighbors 50\\nknnfitdbscancomponents  dbscanlabelsdbscanco', 'hborsClassifier\\nknn  KNeighborsClassifier nneighbors 50\\nknnfitdbscancomponents  dbscanlabelsdbscancoresampleindices \\nNow given a few new instances we can predict which cluster they most likely belong\\nto and even estimate a probability for each cluster Note that we only trained them on\\nthe core instances but we could also have chosen to train them on all the instances\\nor all but the anomalies this choice depends on the final task\\n Xnew  nparray05 0 0 05 1 01 2 1\\n knnpredictXnew\\narray1 0 1 0\\n knnpredictproba Xnew\\narray018 082\\n       1   0  \\n       012 088\\n       1   0  \\nThe decision boundary is represented on Figure 915  the crosses represent the 4\\ninstances in Xnew  Notice that since there is no anomaly in the KNNs training set\\nthe classifier always chooses a cluster even when that cluster is far away However it\\nis fairly straightforward to introduce a maximum distance in which case the two\\ninstances that are far away from both clusters are classified as anomalies To do this\\nwe can use ', 'wo\\ninstances that are far away from both clusters are classified as anomalies To do this\\nwe can use the kneighbors  method of the KNeighborsClassifier  given a set of\\ninstances it returns the distances and the indices of the k nearest neighbors in the\\ntraining set two matrices each with k columns\\n ydist ypredidx   knnkneighbors Xnew nneighbors 1\\n ypred  dbscanlabelsdbscancoresampleindices ypredidx \\n ypredydist  02  1\\n ypredravel\\narray1  0  1 1\\n258  Chapter 9 Unsupervised Learning TechniquesFigure 915 clusterclassificationdiagram\\nIn short DBSCAN is a very simple yet powerful algorithm capable of identifying any\\nnumber of clusters of any shape it is robust to outliers and it has just two hyper\\nparameters  eps and minsamples  However if the density varies significantly across\\nthe clusters it can be impossible for it to capture all the clusters properly Moreover\\nits computational complexity is roughly O m log m making it pretty close to linear\\nwith regards to the number of instances Howeve', 'is roughly O m log m making it pretty close to linear\\nwith regards to the number of instances However ScikitLearns implementation can\\nrequire up to O m2 memory if eps is large\\nOther Clustering Algorithms\\nScikitLearn implements several more clustering algorithms that you should take a\\nlook at We cannot cover them all in detail here but here is a brief overview\\nAgglomerative clustering  a hierarchy of clusters is built from the bottom up\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until theres just one big group of bubbles Similarly at each iteration\\nagglomerative clustering connects the nearest pair of clusters starting with indi\\nvidual instances If you draw a tree with a branch for every pair of clusters that\\nmerged you get a binary tree of clusters where the leaves are the individual\\ninstances This approach scales very well to large numbers of instances or clus\\nters it can capture clusters of various shapes it produces a flexible and informa\\ntive', 'ances or clus\\nters it can capture clusters of various shapes it produces a flexible and informa\\ntive cluster tree instead of forcing you to choose a particular cluster scale and it\\ncan be used with any pairwise distance It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors eg returned by\\nsklearnneighborskneighborsgraph  Without a connectivity matrix the\\nalgorithm does not scale well to large datasets\\nBirch  this algorithm was designed specifically for very large datasets and it can\\nbe faster than batch KMeans with similar results as long as the number of fea\\ntures is not too large 20 It builds a tree structure during training containing\\nClustering  259just enough information to quickly assign each new instance to a cluster without\\nhaving to store all the instances in the tree this allows it to use limited memory\\nwhile handle huge datasets\\nMeanshift  this algorithm start', 'tree this allows it to use limited memory\\nwhile handle huge datasets\\nMeanshift  this algorithm starts by placing a circle centered on each instance\\nthen for each circle it computes the mean of all the instances located within it\\nand it shifts the circle so that it is centered on the mean Next it iterates this\\nmeanshift step until all the circles stop moving ie until each of them is cen\\ntered on the mean of the instances it contains This algorithm shifts the circles\\nin the direction of higher density until each of them has found a local density\\nmaximum Finally all the instances whose circles have settled in the same place\\nor close enough are assigned to the same cluster This has some of the same fea\\ntures as DBSCAN in particular it can find any number of clusters of any shape it\\nhas just one hyperparameter the radius of the circles called the bandwidth and\\nit relies on local density estimation However it tends to chop clusters into pieces\\nwhen they have internal density variations Unfor', 'ation However it tends to chop clusters into pieces\\nwhen they have internal density variations Unfortunately its computational\\ncomplexity is O m2 so it is not suited for large datasets\\nAffinity  propagation  this algorithm uses a voting system where instances vote for\\nsimilar instances to be their representatives and once the algorithm converges\\neach representative and its voters form a cluster This algorithm can detect any\\nnumber of clusters of different sizes Unfortunately this algorithm has a compu\\ntational complexity of O m2 so it is not suited for large datasets\\nSpectral clustering  this algorithm takes a similarity matrix between the instances\\nand creates a lowdimensional embedding from it ie it reduces its dimension\\nality then it uses another clustering algorithm in this lowdimensional space\\nScikitLearns implementation uses KMeans Spectral clustering can capture\\ncomplex cluster structures and it can also be used to cut graphs eg to identify\\nclusters of friends on a social networ', 'ructures and it can also be used to cut graphs eg to identify\\nclusters of friends on a social network however it does not scale well to large\\nnumber of instances and it does not behave well when the clusters have very dif\\nferent sizes\\nNow lets dive into Gaussian mixture models which can be used for density estima\\ntion clustering and anomaly detection\\nGaussian Mixtures\\nA Gaussian mixture model  GMM is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown All the instances generated from a single Gaussian distri\\nbution form a cluster that typically looks like an ellipsoid Each cluster can have a dif\\nferent ellipsoidal shape size density and orientation just like in Figure 911  When\\nyou observe an instance you know it was generated from one of the Gaussian distri\\n260  Chapter 9 Unsupervised Learning Techniques7Phi  or  is the 21st letter of the Greek alphabet\\n8Most of these notations are standard ', ' Techniques7Phi  or  is the 21st letter of the Greek alphabet\\n8Most of these notations are standard but a few additional notations were taken from the Wikipedia article on\\nplate notation butions but you are not told which one and you do not know what the parameters of\\nthese distributions are\\nThere are several GMM variants in the simplest variant implemented in the Gaus\\nsianMixture  class you must know in advance the number k of Gaussian distribu\\ntions The dataset X is assumed to have been generated through the following\\nprobabilistic process\\nFor each instance a cluster is picked randomly among k clusters The probability\\nof choosing the jth cluster is defined by the clusters weight j7 The index of the\\ncluster chosen for the ith instance is noted zi\\nIf zij meaning the ith instance has been assigned to the jth cluster the location\\nxi of this instance is sampled randomly from the Gaussian distribution with\\nmean j and covariance matrix j This is noted i  jj\\nThis generative process can be re', 'stribution with\\nmean j and covariance matrix j This is noted i  jj\\nThis generative process can be represented as a graphical model  see Figure 916 \\nThis is a graph which represents the structure of the conditional dependencies\\nbetween random variables\\nFigure 916 Gaussian mixture model\\nHere is how to interpret it8\\nThe circles represent random variables\\nThe squares represent fixed values ie parameters of the model\\nGaussian Mixtures  261The large rectangles are called plates  they indicate that their content is repeated\\nseveral times\\nThe number indicated at the bottom right hand side of each plate indicates how\\nmany times its content is repeated so there are m random variables zi from z1\\nto zm and m random variables xi and k means j and k covariance matrices\\nj but just one weight vector  containing all the weights 1 to k\\nEach variable zi is drawn from the categorical distribution  with weights  Each\\nvariable xi is drawn from the normal distribution with the mean and covariance\\nmatrix defi', 'hts  Each\\nvariable xi is drawn from the normal distribution with the mean and covariance\\nmatrix defined by its cluster zi\\nThe solid arrows represent conditional dependencies For example the probabil\\nity distribution for each random variable zi depends on the weight vector \\nNote that when an arrow crosses a plate boundary it means that it applies to all\\nthe repetitions of that plate so for example the weight vector  conditions the\\nprobability distributions of all the random variables x1 to xm\\nThe squiggly arrow from zi to xi represents a switch depending on the value of\\nzi the instance xi will be sampled from a different Gaussian distribution For\\nexample if zij then i  jj\\nShaded nodes indicate that the value is known so in this case only the random\\nvariables xi have known values they are called observed variables  The unknown\\nrandom variables zi are called latent variables \\nSo what can you do with such a model Well given the dataset X you typically want\\nto start by estimating the weight', 'u do with such a model Well given the dataset X you typically want\\nto start by estimating the weights  and all the distribution parameters 1 to k and\\n1 to k ScikitLearns GaussianMixture  class makes this trivial\\nfrom sklearnmixture  import GaussianMixture\\ngm  GaussianMixture ncomponents 3 ninit10\\ngmfitX\\nLets look at the parameters that the algorithm estimated\\n gmweights\\narray020965228 04000662  039028152\\n gmmeans\\narray 339909717  105933727\\n       140763984  142710194\\n        005135313  007524095\\n gmcovariances\\narray 114807234 003270354\\n        003270354  095496237\\n        063478101  072969804\\n         072969804  11609872 \\n262  Chapter 9 Unsupervised Learning Techniques        068809572  079608475\\n         079608475  121234145\\nGreat it worked fine Indeed the weights that were used to generate the data were\\n02 04 and 04 and similarly the means and covariance matrices were very close to\\nthose found by the algorithm But how This class relies on the Expectation\\nMaximization  EM algorithm wh', 'se found by the algorithm But how This class relies on the Expectation\\nMaximization  EM algorithm which has many similarities with the KMeans algo\\nrithm it also initializes the cluster parameters randomly then it repeats two steps\\nuntil convergence first assigning instances to clusters this is called the expectation\\nstep then updating the clusters this is called the maximization step  Sounds famil\\niar Indeed in the context of clustering you can think of EM as a generalization of K\\nMeans which not only finds the cluster centers  1 to k but also their size shape\\nand orientation  1 to k as well as their relative weights  1 to k Unlike K\\nMeans EM uses soft cluster assignments rather than hard assignments for each\\ninstance during the expectation step the algorithm estimates the probability that it\\nbelongs to each cluster based on the current cluster parameters Then during the\\nmaximization step each cluster is updated using all the instances in the dataset with\\neach instance weighted by the ', 'p each cluster is updated using all the instances in the dataset with\\neach instance weighted by the estimated probability that it belongs to that cluster\\nThese probabilities are called the responsibilities  of the clusters for the instances Dur\\ning the maximization step each clusters update will mostly be impacted by the\\ninstances it is most responsible for\\nUnfortunately just like KMeans EM can end up converging to\\npoor solutions so it needs to be run several times keeping only the\\nbest solution This is why we set ninit  to 10 Be careful by default\\nninit  is only set to 1\\nY ou can check whether or not the algorithm converged and how many iterations it\\ntook\\n gmconverged\\nTrue\\n gmniter\\n3\\nOkay now that you have an estimate of the location size shape orientation and rela\\ntive weight of each cluster the model can easily assign each instance to the most likely\\ncluster hard clustering or estimate the probability that it belongs to a particular\\ncluster soft clustering For this just use the pred', 'e the probability that it belongs to a particular\\ncluster soft clustering For this just use the predict  method for hard clustering\\nor the predictproba  method for soft clustering\\n gmpredictX\\narray2 2 1  0 0 0\\n gmpredictproba X\\narray232389467e02 677397850e07 976760376e01\\n       164685609e02 675361303e04 982856078e01\\nGaussian Mixtures  263       201535333e06 999923053e01 749319577e05\\n       \\n       999999571e01 213946075e26 428788333e07\\n       100000000e00 146454409e41 512459171e16\\n       100000000e00 802006365e41 227626238e15\\nIt is a generative model  meaning you can actually sample new instances from it note\\nthat they are ordered by cluster index\\n Xnew ynew  gmsample6\\n Xnew\\narray 295400315  263680992\\n       116654575  162792705\\n       139477712 148511338\\n        027221525  0690366  \\n        054095936  048591934\\n        038064009 056240465\\n ynew\\narray0 1 2 2 2 2\\nIt is also possible to estimate the density of the model at any given location This is\\nachieved using the scoresamples  metho', 'timate the density of the model at any given location This is\\nachieved using the scoresamples  method for each instance it is given this\\nmethod estimates the log of the probability density function  PDF at that location\\nThe greater the score the higher the density\\n gmscoresamples X\\narray260782346 357106041 333003479  351352783\\n       439802535 380743859\\nIf you compute the exponential of these scores you get the value of the PDF at the\\nlocation of the given instances These are not probabilities but probability densities \\nthey can take on any positive value not just between 0 and 1 To estimate the proba\\nbility that an instance will fall within a particular region you would have to integrate\\nthe PDF over that region if you do so over the entire space of possible instance loca\\ntions the result will be 1\\nFigure 917  shows the cluster means the decision boundaries dashed lines and the\\ndensity contours of this model\\n264  Chapter 9 Unsupervised Learning TechniquesFigure 917 Cluster means decis', 'contours of this model\\n264  Chapter 9 Unsupervised Learning TechniquesFigure 917 Cluster means decision boundaries and density contours of a trained Gaus\\nsian mixture model\\nNice The algorithm clearly found an excellent solution Of course we made its task\\neasy by actually generating the data using a set of 2D Gaussian distributions unfortu\\nnately real life data is not always so Gaussian and lowdimensional and we also gave\\nthe algorithm the correct number of clusters When there are many dimensions or\\nmany clusters or few instances EM can struggle to converge to the optimal solution\\nY ou might need to reduce the difficulty of the task by limiting the number of parame\\nters that the algorithm has to learn one way to do this is to limit the range of shapes\\nand orientations that the clusters can have This can be achieved by imposing con\\nstraints on the covariance matrices To do this just set the covariancetype  hyper\\nparameter to one of the following values\\nspherical  all clusters must be sph', 'e covariancetype  hyper\\nparameter to one of the following values\\nspherical  all clusters must be spherical but they can have different diameters\\nie different variances\\ndiag  clusters can take on any ellipsoidal shape of any size but the ellipsoids\\naxes must be parallel to the coordinate axes ie the covariance matrices must be\\ndiagonal\\ntied  all clusters must have the same ellipsoidal shape size and orientation\\nie all clusters share the same covariance matrix\\nBy default covariancetype  is equal to full  which means that each cluster can\\ntake on any shape size and orientation it has its own unconstrained covariance\\nmatrix Figure 918  plots the solutions found by the EM algorithm when cova\\nriancetype  is set to tied  or spherical \\nGaussian Mixtures  265Figure 918 covariancetypediagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m the number of\\ndimensions n the number of clusters k and the constraints on the\\ncovariance matrices If cov', 'umber of\\ndimensions n the number of clusters k and the constraints on the\\ncovariance matrices If covariancetype  is spherical  or diag \\nit is O kmn  assuming the data has a clustering structure If cova\\nriancetype  is tied  or full  it is O kmn2  kn3 so it will not\\nscale to large numbers of features\\nGaussian mixture models can also be used for anomaly detection Lets see how\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection  also called outlier detection  is the task of detecting instances that\\ndeviate strongly from the norm These instances are of course called anomalies  or\\noutliers  while the normal instances are called inliers  Anomaly detection is very use\\nful in a wide variety of applications for example in fraud detection or for detecting\\ndefective products in manufacturing or to remove outliers from a dataset before\\ntraining another model which can significantly improve the performance of the\\nresulting model\\nUsing a Gaussian mixture model for anomaly detection is quite s', 'e performance of the\\nresulting model\\nUsing a Gaussian mixture model for anomaly detection is quite simple any instance\\nlocated in a lowdensity region can be considered an anomaly Y ou must define what\\ndensity threshold you want to use For example in a manufacturing company that\\ntries to detect defective products the ratio of defective products is usually well\\nknown Say it is equal to 4 then you can set the density threshold to be the value\\nthat results in having 4 of the instances located in areas below that threshold den\\nsity If you notice that you get too many false positives ie perfectly good products\\nthat are flagged as defective you can lower the threshold Conversely if you have too\\nmany false negatives ie defective products that the system does not flag as defec\\ntive you can increase the threshold This is the usual precisionrecall tradeoff see\\nChapter 3  Here is how you would identify the outliers using the 4th percentile low\\n266  Chapter 9 Unsupervised Learning Techniquesest den', 'ify the outliers using the 4th percentile low\\n266  Chapter 9 Unsupervised Learning Techniquesest density as the threshold ie approximately 4 of the instances will be flagged as\\nanomalies\\ndensities   gmscoresamples X\\ndensitythreshold   nppercentile densities  4\\nanomalies   Xdensities   densitythreshold \\nThese anomalies are represented as stars on Figure 919 \\nFigure 919 Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection  it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a clean dataset uncontaminated by outliers\\nwhereas anomaly detection does not make this assumption Indeed outlier detection\\nis often precisely used to clean up a dataset\\nGaussian mixture models try to fit all the data including the outli\\ners so if you have too many of them this will bias the models view\\nof normality some outliers may wrongly be considered as nor\\nmal If this happens you can try to fit the model once use it to\\ndetect and remove the ', 'idered as nor\\nmal If this happens you can try to fit the model once use it to\\ndetect and remove the most extreme outliers then fit the model\\nagain on the cleaned up dataset Another approach is to use robust\\ncovariance estimation methods see the EllipticEnvelope  class\\nJust like KMeans the GaussianMixture  algorithm requires you to specify the num\\nber of clusters So how can you find it\\nSelecting the Number of Clusters\\nWith KMeans you could use the inertia or the silhouette score to select the appro\\npriate number of clusters but with Gaussian mixtures it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif\\nferent sizes Instead you can try to find the model that minimizes a theoretical infor\\nGaussian Mixtures  267mation criterion  such as the Bayesian information criterion  BIC or the Akaike\\ninformation criterion  AIC defined in Equation 91 \\nEquation 91 Bayesian information criterion BIC and Akaike information\\ncriterion AIC\\nBIC', 'Equation 91 \\nEquation 91 Bayesian information criterion BIC and Akaike information\\ncriterion AIC\\nBIC  log mp 2 log L\\nAIC  2 p 2 log L\\nm is the number of instances as always\\np is the number of parameters learned by the model\\nL is the maximized value of the likelihood function  of the model\\nBoth the BIC and the AIC penalize models that have more parameters to learn eg\\nmore clusters and reward models that fit the data well They often end up selecting\\nthe same model but when they differ the model selected by the BIC tends to be sim\\npler fewer parameters than the one selected by the AIC but it does not fit the data\\nquite as well this is especially true for larger datasets\\nLikelihood function\\nThe terms probability and likelihood are often used interchangeably in the\\nEnglish language but they have very different meanings in statistics given a statistical\\nmodel with some parameters  the word probability is used to describe how plausi\\nble a future outcome x is knowing the parameter values  whil', 'bability is used to describe how plausi\\nble a future outcome x is knowing the parameter values  while the word likeli\\nhood is used to describe how plausible a particular set of parameter values  are\\nafter the outcome x is known\\nConsider a onedimensional mixture model of two Gaussian distributions centered at\\n4 and 1 For simplicity this toy model has a single parameter  that controls the\\nstandard deviations of both distributions The top left contour plot in Figure 920\\nshows the entire model fx  as a function of both x and  To estimate the probabil\\nity distribution of a future outcome x you need to set the model parameter  For\\nexample if you set it to 13 the horizontal line you get the probability density\\nfunction fx 13 shown in the lower left plot Say you want to estimate the proba\\nbility that x will fall between 2 and 2 you must calculate the integral of the PDF on\\nthis range ie the surface of the shaded region On the other hand if you have\\nobserved a single instance x25 the vertical l', 'ace of the shaded region On the other hand if you have\\nobserved a single instance x25 the vertical line in the upper left plot you get the\\nlikelihood function noted x25f x25  represented in the upper right plot\\nIn short the PDF is a function of x with  fixed while the likelihood function is a\\nfunction of  with x fixed It is important to understand that the likelihood function\\nis not a probability distribution if you integrate a probability distribution over all\\n268  Chapter 9 Unsupervised Learning Techniquespossible values of x you always get 1 but if you integrate the likelihood function over\\nall possible values of  the result can be any positive value\\nFigure 920 A models parametric function top left  and some derived functions a PDF\\nlower left  a likelihood function top right and a log likelihood function lower right\\nGiven a dataset X a common task is to try to estimate the most likely values for the\\nmodel parameters To do this you must find the values that maximize the likelihood\\nfu', ' values for the\\nmodel parameters To do this you must find the values that maximize the likelihood\\nfunction given X In this example if you have observed a single instance x25 the\\nmaximum likelihood estimate  MLE of  is 15 If a prior probability distribution g\\nover  exists it is possible to take it into account by maximizing xg rather\\nthan just maximizing x This is called maximum aposteriori MAP estimation\\nSince MAP constrains the parameter values you can think of it as a regularized ver\\nsion of MLE\\nNotice that it is equivalent to maximize the likelihood function or to maximize its\\nlogarithm represented in the lower right hand side of Figure 920  indeed the loga\\nrithm is a strictly increasing function so if  maximizes the log likelihood it also\\nmaximizes the likelihood It turns out that it is generally easier to maximize the log\\nlikelihood For example if you observed several independent instances x1 to xm you\\nwould need to find the value of  that maximizes the product of the individual l', 'stances x1 to xm you\\nwould need to find the value of  that maximizes the product of the individual likeli\\nhood functions But it is equivalent and much simpler to maximize the sum not the\\nproduct of the log likelihood functions thanks to the magic of the logarithm which\\nconverts products into sums log ablog alog b\\nOnce you have estimated  the value of  that maximizes the likelihood function\\nthen you are ready to compute L This is the value which is used to com\\npute the AIC and BIC you can think of it as a measure of how well the model fits the\\ndata\\nTo compute the BIC and AIC just call the bic  or aic  methods\\nGaussian Mixtures  269 gmbicX\\n818974345832983\\n gmaicX\\n8102518178214792\\nFigure 921  shows the BIC for different numbers of clusters k As you can see both\\nthe BIC and the AIC are lowest when k3 so it is most likely the best choice Note\\nthat we could also search for the best value for the covariancetype  hyperparameter\\nFor example if it is spherical  rather than full  then the model h', 'he covariancetype  hyperparameter\\nFor example if it is spherical  rather than full  then the model has much fewer\\nparameters to learn but it does not fit the data as well\\nFigure 921 AIC and BIC for different  numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters it is possible to\\nuse instead the BayesianGaussianMixture  class which is capable of giving weights\\nequal or close to zero to unnecessary clusters Just set the number of clusters ncom\\nponents  to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters this assumes some minimal knowledge about the problem at\\nhand and the algorithm will eliminate the unnecessary clusters automatically For\\nexample lets set the number of clusters to 10 and see what happens\\n from sklearnmixture  import BayesianGaussianMixture\\n bgm  BayesianGaussianMixture ncomponents 10 ninit10 randomstate 42\\n bgmfitX\\n nproundbgmweights  2\\narray04  021 04  0   0   ', 'xture ncomponents 10 ninit10 randomstate 42\\n bgmfitX\\n nproundbgmweights  2\\narray04  021 04  0   0   0   0   0   0   0  \\nPerfect the algorithm automatically detected that only 3 clusters are needed and the\\nresulting clusters are almost identical to the ones in Figure 917 \\nIn this model the cluster parameters including the weights means and covariance\\nmatrices are not treated as fixed model parameters anymore but as latent random\\nvariables like the cluster assignments see Figure 922  So z now includes both the\\ncluster parameters and the cluster assignments\\n270  Chapter 9 Unsupervised Learning TechniquesFigure 922 Bayesian Gaussian mixture model\\nPrior knowledge about the latent variables z can be encoded in a probability distribu\\ntion pz called the prior  For example we may have a prior belief that the clusters are\\nlikely to be few low concentration or conversely that they are more likely to be\\nplentiful high concentration This can be adjusted using the weightconcentra\\ntionprior  hyperpar', 'o be\\nplentiful high concentration This can be adjusted using the weightconcentra\\ntionprior  hyperparameter Setting it to 001 or 1000 gives very different clusterings\\nsee Figure 923  However the more data we have the less the priors matter In fact\\nto plot diagrams with such large differences you must use very strong priors and lit\\ntle data\\nFigure 923 Using different  concentration priors\\nThe fact that you see only 3 regions in the right plot although there\\nare 4 centroids is not a bug the weight of the topright cluster is\\nmuch larger than the weight of the lowerright cluster so the prob\\nability that any given point in this region belongs to the topright\\ncluster is greater than the probability that it belongs to the lower\\nright cluster even near the lowerright cluster\\nGaussian Mixtures  271Bayes theorem  Equation 92  tells us how to update the probability distribution over\\nthe latent variables after we observe some data X It computes the posterior  distribu\\ntion pzX which is the conditio', 'bles after we observe some data X It computes the posterior  distribu\\ntion pzX which is the conditional probability of z given X\\nEquation 92 Bayes theorem\\npzX Posterior LikelihoodPrior\\nEvidencepXzpz\\npX\\nUnfortunately in a Gaussian mixture model and many other problems the denomi\\nnator px is intractable as it requires integrating over all the possible values of z\\nEquation 93  This means considering all possible combinations of cluster parame\\nters and cluster assignments\\nEquation 93 The evidence p X is often  intractable\\npXpXzpzdz\\nThis is one of the central problems in Bayesian statistics and there are several\\napproaches to solving it One of them is variational inference  which picks a family of\\ndistributions qz  with its own variational parameters   lambda then it optimizes\\nthese parameters to make qz a good approximation of pzX This is achieved by\\nfinding the value of  that minimizes the KL divergence from qz to pzX noted\\nDKLqp The KL divergence equation is shown in see Equation 94  and', 'KL divergence from qz to pzX noted\\nDKLqp The KL divergence equation is shown in see Equation 94  and it can be\\nrewritten as the log of the evidence log pX minus the evidence lower bound\\nELBO Since the log of the evidence does not depend on q it is a constant term so\\nminimizing the KL divergence just requires maximizing the ELBO\\nEquation 94 KL divergence from q z to p zX\\nDKLqpqlogqz\\npzX\\nqlogqz log pzX\\nqlogqz logpzX\\npX\\nqlogqz log pzX log pX\\nqlogqzqlogpzXqlogpX\\nqlog pXqlog pzXqlogqz\\n log pX ELBO\\nwhere ELBO  qlog pzXqlogqz\\n272  Chapter 9 Unsupervised Learning TechniquesIn practice there are different techniques to maximize the ELBO In mean field varia\\ntional inference  it is necessary to pick the family of distributions qz  and the prior\\npz very carefully to ensure that the equation for the ELBO simplifies to a form that\\ncan actually be computed Unfortunately there is no general way to do this it\\ndepends on the task and requires some mathematical skills For example the distribu\\ntions and l', 'is it\\ndepends on the task and requires some mathematical skills For example the distribu\\ntions and lower bound equations used in ScikitLearns BayesianGaussianMixture\\nclass are presented in the documentation  From these equations it is possible to derive\\nupdate equations for the cluster parameters and assignment variables these are then\\nused very much like in the ExpectationMaximization algorithm In fact the compu\\ntational complexity of the BayesianGaussianMixture  class is similar to that of the\\nGaussianMixture  class but generally significantly slower A simpler approach to\\nmaximizing the ELBO is called black box stochastic variational inference  BBSVI at\\neach iteration a few samples are drawn from q and they are used to estimate the gra\\ndients of the ELBO with regards to the variational parameters  which are then used\\nin a gradient ascent step This approach makes it possible to use Bayesian inference\\nwith any kind of model provided it is differentiable even deep neural networks this\\ni', 'sian inference\\nwith any kind of model provided it is differentiable even deep neural networks this\\nis called Bayesian deep learning\\nIf you want to dive deeper into Bayesian statistics check out the\\nBayesian Data Analysis  book  by Andrew Gelman John Carlin Hal\\nStern David Dunson Aki Vehtari and Donald Rubin\\nGaussian mixture models work great on clusters with ellipsoidal shapes but if you try\\nto fit a dataset with different shapes you may have bad surprises For example lets\\nsee what happens if we use a Bayesian Gaussian mixture model to cluster the moons\\ndataset see Figure 924 \\nFigure 924 moonsvsbgmdiagram\\nOops the algorithm desperately searched for ellipsoids so it found 8 different clus\\nters instead of 2 The density estimation is not too bad so this model could perhaps\\nbe used for anomaly detection but it failed to identify the two moons Lets now look\\nat a few clustering algorithms capable of dealing with arbitrarily shaped clusters\\nGaussian Mixtures  273Other Anomaly Detection and No', 'ble of dealing with arbitrarily shaped clusters\\nGaussian Mixtures  273Other Anomaly Detection and Novelty Detection Algorithms\\nScikitLearn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection\\nFastMCD  minimum covariance determinant implemented by the EllipticEn\\nvelope  class this algorithm is useful for outlier detection in particular to\\ncleanup a dataset It assumes that the normal instances inliers are generated\\nfrom a single Gaussian distribution not a mixture but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution When it estimates the parameters of the Gaussian distribution ie\\nthe shape of the elliptic envelope around the inliers it is careful to ignore the\\ninstances that are most likely outliers This gives a better estimation of the elliptic\\nenvelope and thus makes it better at identifying the outliers\\nIsolation forest  this is an efficient algorithm for outlier detection especially ', 'ying the outliers\\nIsolation forest  this is an efficient algorithm for outlier detection especially in\\nhighdimensional datasets The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly at each node it picks a feature randomly then\\nit picks a random threshold value between the min and max value to split the\\ndataset in two The dataset gradually gets chopped into pieces this way until all\\ninstances end up isolated from the other instances An anomaly is usually far\\nfrom other instances so on average across all the Decision Trees it tends to get\\nisolated in less steps than normal instances\\nLocal outlier factor  LOF this algorithm is also good for outlier detection It\\ncompares the density of instances around a given instance to the density around\\nits neighbors An anomaly is often more isolated than its k nearest neighbors\\nOneclass SVM  this algorithm is better suited for novelty detection Recall that a\\nkernelized SVM classifier separates two classes by first implici', 'for novelty detection Recall that a\\nkernelized SVM classifier separates two classes by first implicitly mapping all\\nthe instances to a highdimensional space then separating the two classes using a\\nlinear SVM classifier within this highdimensional space see Chapter 5  Since\\nwe just have one class of instances the oneclass SVM algorithm instead tries to\\nseparate the instances in highdimensional space from the origin In the original\\nspace this will correspond to finding a small region that encompasses all the\\ninstances If a new instance does not fall within this region it is an anomaly\\nThere are a few hyperparameters to tweak the usual ones for a kernelized SVM\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel when it is in fact normal It works\\ngreat especially with highdimensional datasets but just like all SVMs it does\\nnot scale to large datasets\\n274  Chapter 9 Unsupervised Learning TechniquesPART II\\nNeural Networks a', 'not scale to large datasets\\n274  Chapter 9 Unsupervised Learning TechniquesPART II\\nNeural Networks and Deep Learning1Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models as long as they work well\\nCHAPTER 10\\nIntroduction to Artificial  Neural Networks\\nwith Keras\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 10 in the final\\nrelease of the book\\nBirds inspired us to fly burdock plants inspired velcro and countless more inven\\ntions were inspired by nature It seems only logical then to look at the brains archi\\ntecture for inspiration on how to build an intelligent machine This is the key idea\\nthat sparked artificial  neural networks  ANNs However although planes were\\ninspired by birds they dont have to flap t', 'icial  neural networks  ANNs However although planes were\\ninspired by birds they dont have to flap their wings Similarly ANNs have gradually\\nbecome quite different from their biological cousins Some researchers even argue\\nthat we should drop the biological analogy altogether eg by saying units rather\\nthan neurons lest we restrict our creativity to biologically plausible systems1\\nANNs are at the very core of Deep Learning They are versatile powerful and scala\\nble making them ideal to tackle large and highly complex Machine Learning tasks\\nsuch as classifying billions of images eg Google Images powering speech recogni\\ntion services eg Apples Siri recommending the best videos to watch to hundreds\\nof millions of users every day eg Y ouTube or learning to beat the world champion\\nat the game of Go by playing millions of games against itself DeepMinds Alpha\\nZero\\n2772 A Logical Calculus of Ideas Immanent in Nervous Activity  W  McCulloch and W  Pitts 1943In the first part of this chapter we wil', 'Immanent in Nervous Activity  W  McCulloch and W  Pitts 1943In the first part of this chapter we will introduce artificial neural networks starting\\nwith a quick tour of the very first ANN architectures leading up to MultiLayer Per\\nceptrons  MLPs which are heavily used today other architectures will be explored in\\nthe next chapters In the second part we will look at how to implement neural net\\nworks using the popular Keras API This is a beautifully designed and simple high\\nlevel API for building training evaluating and running neural networks But dont be\\nfooled by its simplicity it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures In fact it will probably be sufficient for most\\nof your use cases Moreover should you ever need extra flexibility you can always\\nwrite custom Keras components using its lowerlevel API as we will see in Chap\\nter 12 \\nBut first lets go back in time to see how artificial neural networks came to be\\nFrom Biological to ', '\\nBut first lets go back in time to see how artificial neural networks came to be\\nFrom Biological to Artificial  Neurons\\nSurprisingly ANNs have been around for quite a while they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts In their landmark paper 2  A Logical Calculus of Ideas Immanent in\\nNervous Activity  McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic  This was the first artificial neural network\\narchitecture Since then many other architectures have been invented as we will see\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines When it became clear that\\nthis promise would go unfulfilled at least for quite a while funding flew elsewhere\\nand ANNs entered a long winter In the early 1980s there was a re', 'uite a while funding flew elsewhere\\nand ANNs entered a long winter In the early 1980s there was a revival of interest in \\nconnectionism  the study of neural networks as new architectures were invented and\\nbetter training techniques were developed But progress was slow and by the 1990s\\nother powerful Machine Learning techniques were invented such as Support Vector\\nMachines see Chapter 5  These techniques seemed to offer better results and stron\\nger theoretical foundations than ANNs so once again the study of neural networks\\nentered a long winter\\nFinally we are now witnessing yet another wave of interest in ANNs Will this wave\\ndie out like the previous ones did Well there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives\\n278  Chapter 10 Introduction to Artificial  Neural Networks with KerasThere is now a huge quantity of data available to train neural networks and\\nANNs frequently outperform other ML techniques on ', 'ty of data available to train neural networks and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems\\nThe tremendous increase in computing power since the 1990s now makes it pos\\nsible to train large neural networks in a reasonable amount of time This is in\\npart due to Moores Law but also thanks to the gaming industry which has pro\\nduced powerful GPU cards by the millions\\nThe training algorithms have been improved To be fair they are only slightly dif\\nferent from the ones used in the 1990s but these relatively small tweaks have a\\nhuge positive impact\\nSome theoretical limitations of ANNs have turned out to be benign in practice\\nFor example many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima but it turns out that this is\\nrather rare in practice or when it is the case they are usually fairly close to the\\nglobal optimum\\nANNs seem to have entered a virtuous circle of funding and progress Amazing\\nprod', ' the\\nglobal optimum\\nANNs seem to have entered a virtuous circle of funding and progress Amazing\\nproducts based on ANNs regularly make the headline news which pulls more\\nand more attention and funding toward them resulting in more and more pro\\ngress and even more amazing products\\nBiological Neurons\\nBefore we discuss artificial neurons lets take a quick look at a biological neuron rep\\nresented in Figure 101  It is an unusuallooking cell mostly found in animal cerebral\\ncortexes eg your brain composed of a cell body  containing the nucleus and most\\nof the cells complex components and many branching extensions called dendrites \\nplus one very long extension called the axon  The axons length may be just a few\\ntimes longer than the cell body or up to tens of thousands of times longer Near its\\nextremity the axon splits off into many branches called telodendria  and at the tip of\\nthese branches are minuscule structures called synaptic terminals  or simply synap\\nses which are connected to the den', 'e minuscule structures called synaptic terminals  or simply synap\\nses which are connected to the dendrites or directly to the cell body of other neu\\nrons Biological neurons receive short electrical impulses called signals  from other\\nneurons via these synapses When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds it fires its own signals\\nFrom Biological to Artificial  Neurons  2793Image by Bruce Blaus  Creative Commons 30  Reproduced from httpsenwikipediaorgwikiNeuron \\n4In the context of Machine Learning the phrase neural networks generally refers to ANNs not BNNs\\n5Drawing of a cortical lamination by S Ramon y Cajal public domain Reproduced from httpsenwikipe\\ndiaorgwikiCerebralcortex \\nFigure 101 Biological neuron3\\nThus individual biological neurons seem to behave in a rather simple way but they\\nare organized in a vast network of billions of neurons each neuron typically connec\\nted to thousands of other neurons Highly complex computations can', 'urons each neuron typically connec\\nted to thousands of other neurons Highly complex computations can be performed\\nby a vast network of fairly simple neurons much like a complex anthill can emerge\\nfrom the combined efforts of simple ants The architecture of biological neural net\\nworks BNN4 is still the subject of active research but some parts of the brain have\\nbeen mapped and it seems that neurons are often organized in consecutive layers as \\nshown in Figure 102 \\nFigure 102 Multiple layers in a biological neural network human cortex5\\n280  Chapter 10 Introduction to Artificial  Neural Networks with KerasLogical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron which later became known as an artificial  neuron  it has one or more binary\\nonoff inputs and one binary output The artificial neuron simply activates its out\\nput when more than a certain number of its inputs are active McCulloch and Pitts\\nshowed that even with such a', 'more than a certain number of its inputs are active McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want For example lets\\nbuild a few ANNs that perform various logical computations see Figure 103 \\nassuming that a neuron is activated when at least two of its inputs are active\\nFigure 103 ANNs performing simple logical computations\\nThe first network on the left is simply the identity function if neuron A is activa\\nted then neuron C gets activated as well since it receives two input signals from\\nneuron A but if neuron A is off then neuron C is off as well\\nThe second network performs a logical AND neuron C is activated only when\\nboth neurons A and B are activated a single input signal is not enough to acti\\nvate neuron C\\nThe third network performs a logical OR neuron C gets activated if either neu\\nron A or neuron B is activated or both\\nFinally if we suppose that an input connec', 'ated if either neu\\nron A or neuron B is activated or both\\nFinally if we suppose that an input connection can inhibit the neurons activity\\nwhich is the case with biological neurons then the fourth network computes a\\nslightly more complex logical proposition neuron C is activated only if neuron A\\nis active and if neuron B is off If neuron A is active all the time then you get a\\nlogical NOT neuron C is active when neuron B is off and vice versa\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions see the exercises at the end of the chapter\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures invented in 1957 by Frank\\nRosenblatt It is based on a slightly different artificial neuron see Figure 104  called \\nFrom Biological to Artificial  Neurons  2816The name Perceptron  is sometimes used to mean a tiny network with a single TLUa threshold logic unit  TLU or sometimes a linear threshold unit  LTU the inputs\\nand output are now nu', 'threshold logic unit  TLU or sometimes a linear threshold unit  LTU the inputs\\nand output are now numbers instead of binary onoff values and each input con\\nnection is associated with a weight The TLU computes a weighted sum of its inputs\\nz  w1 x1  w2 x2    wn xn  xT w then applies a step function  to that sum and\\noutputs the result hwx  step z where z  xT w\\nFigure 104 Threshold  logic unit\\nThe most common step function used in Perceptrons is the Heaviside step function\\nsee Equation 101  Sometimes the sign function is used instead\\nEquation 101 Common step functions used in Perceptrons\\nheaviside z0 if z 0\\n1 if z 0sgn z1 if z 0\\n0 if z 0\\n1 if z 0\\nA single TLU can be used for simple linear binary classification It computes a linear\\ncombination of the inputs and if the result exceeds a threshold it outputs the positive\\nclass or else outputs the negative class just like a Logistic Regression classifier or a\\nlinear SVM For example you could use a single TLU to classify iris flowers based on\\nth', 'assifier or a\\nlinear SVM For example you could use a single TLU to classify iris flowers based on\\nthe petal length and width also adding an extra bias feature x0  1 just like we did in\\nprevious chapters Training a TLU in this case means finding the right values for w0\\nw1 and w2 the training algorithm is discussed shortly\\nA Perceptron is simply composed of a single layer of TLUs6 with each TLU connected\\nto all the inputs When all the neurons in a layer are connected to every neuron in the\\nprevious layer ie its input neurons it is called a fully connected layer  or a dense\\nlayer  To represent the fact that each input is sent to every TLU it is common to draw\\nspecial passthrough neurons called input neurons  they just output whatever input\\nthey are fed All the input neurons form the input layer  Moreover an extra bias fea\\n282  Chapter 10 Introduction to Artificial  Neural Networks with Kerasture is generally added  x0  1 it is typically represented using a special type of neu\\nron called a', 'sture is generally added  x0  1 it is typically represented using a special type of neu\\nron called a bias neuron  which just outputs 1 all the time A Perceptron with two\\ninputs and three outputs is represented in Figure 105  This Perceptron can classify\\ninstances simultaneously into three different binary classes which makes it a multi\\noutput classifier\\nFigure 105 Perceptron diagram\\nThanks to the magic of linear algebra it is possible to efficiently compute the outputs\\nof a layer of artificial neurons for several instances at once by using Equation 102 \\nEquation 102 Computing the outputs of a fully connected layer\\nhWbXXW b\\nAs always X represents the matrix of input features It has one row per instance\\none column per feature\\nThe weight matrix W contains all the connection weights except for the ones\\nfrom the bias neuron It has one row per input neuron and one column per artifi\\ncial neuron in the layer\\nThe bias vector b contains all the connection weights between the bias neuron\\nand the ', ' in the layer\\nThe bias vector b contains all the connection weights between the bias neuron\\nand the artificial neurons It has one bias term per artificial neuron\\nThe function  is called the activation function  when the artificial neurons are\\nTLUs it is a step function but we will discuss other activation functions shortly\\nSo how is a Perceptron trained The Perceptron training algorithm proposed by\\nFrank Rosenblatt was largely inspired by Hebbs rule  In his book The Organization of\\nBehavior  published in 1949 Donald Hebb suggested that when a biological neuron\\noften triggers another neuron the connection between these two neurons grows\\nstronger This idea was later summarized by Siegrid Lwel in this catchy phrase\\nCells that fire together wire together  This rule later became known as Hebbs rule \\nFrom Biological to Artificial  Neurons  2837Note that this solution is generally not unique in general when the data are linearly separable there is an\\ninfinity of hyperplanes that can separate ', ' general when the data are linearly separable there is an\\ninfinity of hyperplanes that can separate themor Hebbian learning  that is the connection weight between two neurons is\\nincreased whenever they have the same output Perceptrons are trained using a var\\niant of this rule that takes into account the error made by the network it reinforces\\nconnections that help reduce the error More specifically the Perceptron is fed one\\ntraining instance at a time and for each instance it makes its predictions For every\\noutput neuron that produced a wrong prediction it reinforces the connection\\nweights from the inputs that would have contributed to the correct prediction The\\nrule is shown in Equation 103 \\nEquation 103 Perceptron learning rule weight update\\nwijnext stepwijyjyjxi\\nwi j is the connection weight between the ith input neuron and the jth output neu\\nron\\nxi is the ith input value of the current training instance\\nyj is the output of the jth output neuron for the current training instance\\nyj ', 'nt training instance\\nyj is the output of the jth output neuron for the current training instance\\nyj is the target output of the jth output neuron for the current training instance\\n is the learning rate\\nThe decision boundary of each output neuron is linear so Perceptrons are incapable\\nof learning complex patterns just like Logistic Regression classifiers However if the\\ntraining instances are linearly separable Rosenblatt demonstrated that this algorithm\\nwould converge to a solution7 This is called the Perceptron convergence theorem \\nScikitLearn provides a Perceptron  class that implements a single TLU network It\\ncan be used pretty much as you would expectfor example on the iris dataset intro\\nduced in Chapter 4 \\nimport numpy as np\\nfrom sklearndatasets  import loadiris\\nfrom sklearnlinearmodel  import Perceptron\\niris  loadiris \\nX  irisdata 2 3   petal length petal width\\ny  iristarget  0astypenpint   Iris Setosa\\nperclf  Perceptron \\nperclffitX y\\n284  Chapter 10 Introduction to Artificial  Ne', 'enpint   Iris Setosa\\nperclf  Perceptron \\nperclffitX y\\n284  Chapter 10 Introduction to Artificial  Neural Networks with Kerasypred  perclfpredict2 05\\nY ou may have noticed the fact that the Perceptron learning algorithm strongly resem\\nbles Stochastic Gradient Descent In fact ScikitLearns Perceptron  class is equivalent\\nto using an SGDClassifier  with the following hyperparameters lossperceptron \\nlearningrateconstant  eta01  the learning rate and penaltyNone  no regu\\nlarization\\nNote that contrary to Logistic Regression classifiers Perceptrons do not output a class\\nprobability rather they just make predictions based on a hard threshold This is one\\nof the good reasons to prefer Logistic Regression over Perceptrons\\nIn their 1969 monograph titled Perceptrons  Marvin Minsky and Seymour Papert\\nhighlighted a number of serious weaknesses of Perceptrons in particular the fact that\\nthey are incapable of solving some trivial problems eg the Exclusive OR  XOR\\nclassification problem see the left side', 'e of solving some trivial problems eg the Exclusive OR  XOR\\nclassification problem see the left side of Figure 106  Of course this is true of any\\nother linear classification model as well such as Logistic Regression classifiers but\\nresearchers had expected much more from Perceptrons and their disappointment\\nwas great and many researchers dropped neural networks altogether in favor of\\nhigherlevel problems such as logic problem solving and search\\nHowever it turns out that some of the limitations of Perceptrons can be eliminated by\\nstacking multiple Perceptrons The resulting ANN is called a MultiLayer Perceptron\\nMLP In particular an MLP can solve the XOR problem as you can verify by com\\nputing the output of the MLP represented on the right of Figure 106  with inputs 0\\n0 or 1 1 the network outputs 0 and with inputs 0 1 or 1 0 it outputs 1 All\\nconnections have a weight equal to 1 except the four connections where the weight is\\nshown Try verifying that this network indeed solves the XOR prob', 'our connections where the weight is\\nshown Try verifying that this network indeed solves the XOR problem\\nFigure 106 XOR classification  problem and an MLP that solves it\\nFrom Biological to Artificial  Neurons  2858In the 1990s an ANN with more than two hidden layers was considered deep Nowadays it is common to see\\nANNs with dozens of layers or even hundreds so the definition of deep is quite fuzzy\\n9Learning Internal Representations by Error Propagation  D Rumelhart G Hinton R Williams 1986\\nMultiLayer Perceptron and Backpropagation\\nAn MLP is composed of one passthrough input layer  one or more layers of TLUs\\ncalled hidden layers  and one final layer of TLUs called the output layer  see\\nFigure 107  The layers close to the input layer are usually called the lower layers\\nand the ones close to the outputs are usually called the upper layers Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer\\nFigure 107 MultiLayer Perceptron\\nThe signal flows onl', 'euron and is fully connected to the next layer\\nFigure 107 MultiLayer Perceptron\\nThe signal flows only in one direction from the inputs to the out\\nputs so this architecture is an example of a feedforward neural net\\nwork  FNN\\nWhen an ANN contains a deep stack of hidden layers8 it is called a deep neural net\\nwork  DNN The field of Deep Learning studies DNNs and more generally models\\ncontaining deep stacks of computations However many people talk about Deep\\nLearning whenever neural networks are involved even shallow ones\\nFor many years researchers struggled to find a way to train MLPs without success\\nBut in 1986 David Rumelhart Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation  training algorithm which is\\nstill used today In short it is simply Gradient Descent introduced in Chapter 4 \\n286  Chapter 10 Introduction to Artificial  Neural Networks with Keras10This technique was actually independently invented several times by various research', 'rks with Keras10This technique was actually independently invented several times by various researchers in different fields\\nstarting with P  Werbos in 1974\\nusing an efficient technique for computing the gradients automatically10 in just two\\npasses through the network one forward one backward the backpropagation algo\\nrithm is able to compute the gradient of the networks error with regards to every sin\\ngle model parameter In other words it can find out how each connection weight and\\neach bias term should be tweaked in order to reduce the error Once it has these gra\\ndients it just performs a regular Gradient Descent step and the whole process is\\nrepeated until the network converges to the solution\\nAutomatically computing gradients is called automatic differentia\\ntion or autodiff  There are various autodiff techniques with differ\\nent pros and cons The one used by backpropagation is called\\nreversemode autodiff  It is fast and precise and is well suited when\\nthe function to differentiate has', 'versemode autodiff  It is fast and precise and is well suited when\\nthe function to differentiate has many variables eg connection\\nweights and few outputs eg one loss If you want to learn more\\nabout autodiff check out \\nLets run through this algorithm in a bit more detail\\nIt handles one minibatch at a time for example containing 32 instances each\\nand it goes through the full training set multiple times Each pass is called an\\nepoch  as we saw in Chapter 4 \\nEach minibatch is passed to the networks input layer which just sends it to the\\nfirst hidden layer The algorithm then computes the output of all the neurons in\\nthis layer for every instance in the minibatch The result is passed on to the\\nnext layer its output is computed and passed to the next layer and so on until we\\nget the output of the last layer the output layer This is the forward pass  it is\\nexactly like making predictions except all intermediate results are preserved\\nsince they are needed for the backward pass\\nNext the algorithm', 'll intermediate results are preserved\\nsince they are needed for the backward pass\\nNext the algorithm measures the networks output error ie it uses a loss func\\ntion that compares the desired output and the actual output of the network and\\nreturns some measure of the error\\nThen it computes how much each output connection contributed to the error\\nThis is done analytically by simply applying  the chain rule  perhaps the most fun\\ndamental rule in calculus which makes this step fast and precise\\nThe algorithm then measures how much of these error contributions came from\\neach connection in the layer below again using the chain ruleand so on until\\nthe algorithm reaches the input layer As we explained earlier this reverse pass\\nefficiently measures the error gradient across all the connection weights in the\\nFrom Biological to Artificial  Neurons  287network by propagating the error gradient backward through the network hence\\nthe name of the algorithm\\nFinally the algorithm performs a Gradient Desc', 'd through the network hence\\nthe name of the algorithm\\nFinally the algorithm performs a Gradient Descent step to tweak all the connec\\ntion weights in the network using the error gradients it just computed\\nThis algorithm is so important its worth summarizing it again for each training\\ninstance the backpropagation algorithm first makes a prediction forward pass\\nmeasures the error then goes through each layer in reverse to measure the error con\\ntribution from each connection reverse pass and finally slightly tweaks the connec\\ntion weights to reduce the error Gradient Descent step\\nIt is important to initialize all the hidden layers connection weights\\nrandomly or else training will fail For example if you initialize all\\nweights and biases to zero then all neurons in a given layer will be\\nperfectly identical and thus backpropagation will affect them in\\nexactly the same way so they will remain identical In other words\\ndespite having hundreds of neurons per layer your model will act\\nas if it ha', 'dentical In other words\\ndespite having hundreds of neurons per layer your model will act\\nas if it had only one neuron per layer it wont be too smart If\\ninstead you randomly initialize the weights you break the symme\\ntry and allow backpropagation to train a diverse team of neurons\\nIn order for this algorithm to work properly the authors made a key change to the\\nMLPs architecture they replaced the step function with the logistic function z \\n1  1  exp z This was essential because the step function contains only flat seg\\nments so there is no gradient to work with Gradient Descent cannot move on a flat\\nsurface while the logistic function has a welldefined nonzero derivative every\\nwhere allowing Gradient Descent to make some progress at every step In fact the\\nbackpropagation algorithm works well with many other activation functions  not just\\nthe logistic function Two other popular activation functions are\\nThe hyperbolic tangent function tanhz  22z  1\\nJust like the logistic function it is Ssh', 'unctions are\\nThe hyperbolic tangent function tanhz  22z  1\\nJust like the logistic function it is Sshaped continuous and differentiable but its\\noutput value ranges from 1 to 1 instead of 0 to 1 in the case of the logistic func\\ntion which tends to make each layers output more or less centered around 0 at\\nthe beginning of training This often helps speed up convergence\\nThe Rectified  Linear Unit function ReLUz  max0 z\\nIt is continuous but unfortunately not differentiable at z  0 the slope changes\\nabruptly which can make Gradient Descent bounce around and its derivative is\\n0 for z  0 However in practice it works very well and has the advantage of being\\n288  Chapter 10 Introduction to Artificial  Neural Networks with Keras11Biological neurons seem to implement a roughly sigmoid Sshaped activation function so researchers stuck\\nto sigmoid functions for a very long time But it turns out that ReLU generally works better in ANNs This is\\none of the cases where the biological analogy was misleading', ' generally works better in ANNs This is\\none of the cases where the biological analogy was misleadingfast to compute11 Most importantly the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent we will\\ncome back to this in Chapter 11 \\nThese popular activation functions and their derivatives are represented in\\nFigure 108  But wait Why do we need activation functions in the first place Well if\\nyou chain several linear transformations all you get is a linear transformation For\\nexample say f x  2 x  3 and g x  5 x  1 then chaining these two linear functions\\ngives you another linear function fg x  25 x  1  3  10 x  1 So if you dont\\nhave some nonlinearity between layers then even a deep stack of layers is equivalent\\nto a single layer you cannot solve very complex problems with that\\nFigure 108 Activation functions and their derivatives\\nOkay So now you know where neural nets came from what their architecture is and\\nhow to compute their outputs a', 'w you know where neural nets came from what their architecture is and\\nhow to compute their outputs and you also learned about the backpropagation algo\\nrithm But what exactly can you do with them\\nRegression MLPs\\nFirst MLPs can be used for regression tasks If you want to predict a single value eg\\nthe price of a house given many of its features then you just need a single output\\nneuron its output is the predicted value For multivariate regression ie to predict\\nmultiple values at once you need one output neuron per output dimension For\\nexample to locate the center of an object on an image you need to predict 2D coordi\\nnates so you need two output neurons If you also want to place a bounding box\\naround the object then you need two more numbers the width and the height of the\\nobject So you end up with 4 output neurons\\nFrom Biological to Artificial  Neurons  289In general when building an MLP for regression you do not want to use any activa\\ntion function for the output neurons so they are fre', 'or regression you do not want to use any activa\\ntion function for the output neurons so they are free to output any range of values\\nHowever if you want to guarantee that the output will always be positive then you\\ncan use the ReLU activation function or the softplus  activation function in the output\\nlayer Finally if you want to guarantee that the predictions will fall within a given\\nrange of values then you can use the logistic function or the hyperbolic tangent and\\nscale the labels to the appropriate range 0 to 1 for the logistic function or 1 to 1 for\\nthe hyperbolic tangent\\nThe loss function to use during training is typically the mean squared error but if you\\nhave a lot of outliers in the training set you may prefer to use the mean absolute\\nerror instead Alternatively you can use the Huber loss which is a combination of\\nboth\\nThe Huber loss is quadratic when the error is smaller than a thres\\nhold  typically 1 but linear when the error is larger than  This\\nmakes it less sensitive to ', 'a thres\\nhold  typically 1 but linear when the error is larger than  This\\nmakes it less sensitive to outliers than the mean squared error and\\nit is often more precise and converges faster than the mean abso\\nlute error\\nTable 101  summarizes the typical architecture of a regression MLP \\nTable 101 Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n input neurons One per input feature eg 28 x 28  784 for MNIST\\n hidden layers Depends on the problem Typically 1 to 5\\n neurons per hidden layer Depends on the problem Typically 10 to 100\\n output neurons 1 per prediction dimension\\nHidden activation ReLU or SELU see Chapter 11 \\nOutput activation None or ReLUSoftplus if positive outputs or LogisticTanh if bounded outputs\\nLoss function MSE or MAEHuber if outliers\\nClassification  MLPs\\nMLPs can also be used for classification tasks For a binary classification problem\\nyou just need a single output neuron using the logistic activation function the output\\nwill be a number between 0 and 1 whi', 'output neuron using the logistic activation function the output\\nwill be a number between 0 and 1 which you can interpret as the estimated probabil\\nity of the positive class Obviously the estimated probability of the negative class is\\nequal to one minus that number\\nMLPs can also easily handle multilabel binary classification tasks see Chapter 3  For\\nexample you could have an email classification system that predicts whether each\\nincoming email is ham or spam and simultaneously predicts whether it is an urgent\\n290  Chapter 10 Introduction to Artificial  Neural Networks with Kerasor nonurgent email In this case you would need two output neurons both using\\nthe logistic activation function the first would output the probability that the email is\\nspam and the second would output the probability that it is urgent More generally\\nyou would dedicate one output neuron for each positive class Note that the output\\nprobabilities do not necessarily add up to one This lets the model output any combi\\nn', 'hat the output\\nprobabilities do not necessarily add up to one This lets the model output any combi\\nnation of labels you can have nonurgent ham urgent ham nonurgent spam and\\nperhaps even urgent spam although that would probably be an error\\nIf each instance can belong only to a single class out of 3 or more possible classes\\neg classes 0 through 9 for digit image classification then you need to have one\\noutput neuron per class and you should use the softmax  activation function for the\\nwhole output layer see Figure 109  The softmax function introduced in Chapter 4 \\nwill ensure that all the estimated probabilities are between 0 and 1 and that they add\\nup to one which is required if the classes are exclusive This is called multiclass clas\\nsification\\nFigure 109 A modern MLP including ReLU and softmax  for classification\\nRegarding the loss function since we are predicting probability distributions the\\ncrossentropy also called the log loss see Chapter 4  is generally a good choice\\nTable 102  s', 'ons the\\ncrossentropy also called the log loss see Chapter 4  is generally a good choice\\nTable 102  summarizes the typical architecture of a classification MLP \\nTable 102 Typical Classification  MLP Architecture\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nInput and hidden layers Same as regression Same as regression Same as regression\\n output neurons 1 1 per label 1 per class\\nOutput layer activation Logistic Logistic Softmax\\nFrom Biological to Artificial  Neurons  29112Project ONEIROS Openended NeuroElectronic Intelligent Robot Operating System\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function CrossEntropy CrossEntropy CrossEntropy\\nBefore we go on I recommend you go through exercise 1 at the\\nend of this chapter Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play\\nground  This will be very useful to better understand MLP', 'ze their outputs using the TensorFlow Play\\nground  This will be very useful to better understand MLPs for\\nexample the effects of all the hyperparameters number of layers\\nand neurons activation functions and more\\nNow you have all the concepts you need to start implementing MLPs with Keras\\nImplementing MLPs with Keras\\nKeras is a highlevel Deep Learning API that allows you to easily build train evaluate\\nand execute all sorts of neural networks Its documentation or specification is avail\\nable at httpskerasio  The reference implementation is simply called Keras as well so\\nto avoid any confusion we will call it kerasteam since it is available at https\\ngithubcomkerasteamkeras  It was developed by Franois Chollet as part of a\\nresearch project12 and released as an open source project in March 2015 It quickly\\ngained popularity owing to its easeofuse flexibility and beautiful design To per\\nform the heavy computations required by neural networks kerasteam relies on a\\ncomputation backend At the pre', ' heavy computations required by neural networks kerasteam relies on a\\ncomputation backend At the present you can choose from three popular open\\nsource deep learning libraries TensorFlow Microsoft Cognitive Toolkit CNTK or\\nTheano\\nMoreover since late 2016 other implementations have been released Y ou can now\\nrun Keras on Apache MXNet Apples Core ML Javascript or Typescript to run Keras\\ncode in a web browser or PlaidML which can run on all sorts of GPU devices not\\njust Nvidia Moreover TensorFlow itself now comes bundled with its own Keras\\nimplementation called tfkeras It only supports TensorFlow as the backend but it has\\nthe advantage of offering some very useful extra features see Figure 1010  for\\nexample it supports TensorFlows Data API which makes it quite easy to load and\\npreprocess data efficiently For this reason we will use tfkeras in this book However\\nin this chapter we will not use any of the TensorFlowspecific features so the code\\nshould run fine on other Keras implementations a', ' any of the TensorFlowspecific features so the code\\nshould run fine on other Keras implementations as well at least in Python with only\\nminor modifications such as changing the imports\\n292  Chapter 10 Introduction to Artificial  Neural Networks with KerasFigure 1010 Two Keras implementations kerasteam left  and tfkeras right\\nAs tfkeras is bundled with TensorFlow lets install TensorFlow\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and ScikitLearn by following the installation instruc\\ntions in Chapter 2  you can simply use pip to install TensorFlow If you created an\\nisolated environment using virtualenv you first need to activate it\\n cd MLPATH               Your ML working directory eg HOMEml\\n source envbinactivate   on Linux or MacOSX\\n envScriptsactivate    on Windows\\nNext install TensorFlow 2 if you are not using a virtualenv you will need adminis\\ntrator rights or to add the user  option\\n python3 m pip install upgrade tensorflow\\nFor GPU support you need to install tensorflowg', 'er  option\\n python3 m pip install upgrade tensorflow\\nFor GPU support you need to install tensorflowgpu  instead of\\ntensorflow  and there are other libraries to install See https\\ntensorfloworginstallgpu  for more details\\nTo test your installation open a Python shell or a Jupyter notebook then import Ten\\nsorFlow and tfkeras and print their versions\\n import tensorflow  as tf\\n from tensorflow  import keras\\n tfversion\\n200\\n kerasversion\\n224tf\\nImplementing MLPs with Keras  293The second version is the version of the Keras API implemented by tfkeras Note that\\nit ends with tf highlighting the fact that tfkeras implements the Keras API plus\\nsome extra TensorFlowspecific features\\nNow lets use tfkeras Lets start by building a simple image classifier\\nBuilding an Image Classifier  Using the Sequential API\\nFirst we need to load a dataset We will tackle Fashion MNIST  which is a dropin\\nreplacement of MNIST introduced in Chapter 3  It has the exact same format as\\nMNIST 70000 grayscale images of 2828 pi', 'IST introduced in Chapter 3  It has the exact same format as\\nMNIST 70000 grayscale images of 2828 pixels each with 10 classes but the\\nimages represent fashion items rather than handwritten digits so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST\\nFor example a simple linear model reaches about 92 accuracy on MNIST but only\\nabout 83 on Fashion MNIST\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets including\\nMNIST Fashion MNIST the original California housing dataset and more Lets load\\nFashion MNIST\\nfashionmnist   kerasdatasets fashionmnist\\nXtrainfull  ytrainfull  Xtest ytest  fashionmnist loaddata \\nWhen loading MNIST or Fashion MNIST using Keras rather than ScikitLearn one\\nimportant difference is that every image is represented as a 2828 array rather than a\\n1D array of size 784 Moreover the pixel intensities are represented as integers from\\n0 to 255 rather than floats from 00 to 2', 'over the pixel intensities are represented as integers from\\n0 to 255 rather than floats from 00 to 2550 Here is the shape and data type of the\\ntraining set\\n Xtrainfull shape\\n60000 28 28\\n Xtrainfull dtype\\ndtypeuint8\\nNote that the dataset is already split into a training set and a test set but there is no\\nvalidation set so lets create one Moreover since we are going to train the neural net\\nwork using Gradient Descent we must scale the input features For simplicity we just\\nscale the pixel intensities down to the 01 range by dividing them by 2550 this also\\nconverts them to floats\\nXvalid Xtrain  Xtrainfull 5000  2550 Xtrainfull 5000  2550\\nyvalid ytrain  ytrainfull 5000 ytrainfull 5000\\nWith MNIST when the label is equal to 5 it means that the image represents the\\nhandwritten digit 5 Easy However for Fashion MNIST we need the list of class\\nnames to know what we are dealing with\\n294  Chapter 10 Introduction to Artificial  Neural Networks with Kerasclassnames   Tshirttop  Trouser  Pullover  Dre', 'Introduction to Artificial  Neural Networks with Kerasclassnames   Tshirttop  Trouser  Pullover  Dress Coat\\n               Sandal  Shirt Sneaker  Bag Ankle boot \\nFor example the first image in the training set represents a coat\\n classnames ytrain0\\nCoat\\nFigure 1011  shows a few samples from the Fashion MNIST dataset\\nFigure 1011 Samples from Fashion MNIST\\nCreating the Model Using the Sequential API\\nNow lets build the neural network Here is a classification MLP with two hidden lay\\ners\\nmodel  kerasmodelsSequential \\nmodeladdkeraslayersFlatteninputshape 28 28\\nmodeladdkeraslayersDense300 activation relu\\nmodeladdkeraslayersDense100 activation relu\\nmodeladdkeraslayersDense10 activation softmax \\nLets go through this code line by line\\nThe first line creates a Sequential  model This is the simplest kind of Keras\\nmodel for neural networks that are just composed of a single stack of layers con\\nnected sequentially This is called the sequential API\\nNext we build the first layer and add it to the model', 'sequentially This is called the sequential API\\nNext we build the first layer and add it to the model It is a Flatten  layer whose\\nrole is simply to convert each input image into a 1D array if it receives input data\\nX it computes Xreshape1 1  This layer does not have any parameters it is\\njust there to do some simple preprocessing Since it is the first layer in the model\\nyou should specify the inputshape  this does not include the batch size only the\\nshape of the instances Alternatively you could add a keraslayersInputLayer\\nas the first layer setting shape2828 \\nNext we add a Dense  hidden layer with 300 neurons It will use the ReLU activa\\ntion function Each Dense  layer manages its own weight matrix containing all the\\nconnection weights between the neurons and their inputs It also manages a vec\\nImplementing MLPs with Keras  295tor of bias terms one per neuron When it receives some input data it computes\\nEquation 102 \\nNext we add a second Dense  hidden layer with 100 neurons also using th', 'ta it computes\\nEquation 102 \\nNext we add a second Dense  hidden layer with 100 neurons also using the ReLU\\nactivation function\\nFinally we add a Dense  output layer with 10 neurons one per class using the\\nsoftmax activation function because the classes are exclusive\\nSpecifying activationrelu  is equivalent to activa\\ntionkerasactivationsrelu  Other activation functions are\\navailable in the kerasactivations  package we will use many of\\nthem in this book See httpskerasioactivations  for the full list\\nInstead of adding the layers one by one as we just did you can pass a list of layers\\nwhen creating the Sequential  model\\nmodel  kerasmodelsSequential \\n    keraslayersFlatteninputshape 28 28\\n    keraslayersDense300 activation relu\\n    keraslayersDense100 activation relu\\n    keraslayersDense10 activation softmax \\n\\nUsing Code Examples From kerasio\\nCode examples documented on kerasio will work fine with tfkeras but you need to\\nchange the imports For example consider this kerasio code\\nfrom keraslay', 'with tfkeras but you need to\\nchange the imports For example consider this kerasio code\\nfrom keraslayers  import Dense\\noutputlayer   Dense10\\nY ou must change the imports like this\\nfrom tensorflowkeraslayers  import Dense\\noutputlayer   Dense10\\nOr simply use full paths if you prefer\\nfrom tensorflow  import keras\\noutputlayer   keraslayersDense10\\nThis is more verbose but I use this approach in this book so you can easily see which\\npackages to use and to avoid confusion between standard classes and custom classes\\nIn production code I use the previous approach as do most people\\n296  Chapter 10 Introduction to Artificial  Neural Networks with Keras13Y ou can also generate an image of your model using kerasutilsplotmodel The models summary  method displays all the models layers13 including each layers\\nname which is automatically generated unless you set it when creating the layer its\\noutput shape  None  means the batch size can be anything and its number of parame\\nters The summary ends with the', '  None  means the batch size can be anything and its number of parame\\nters The summary ends with the total number of parameters including trainable and\\nnontrainable parameters Here we only have trainable parameters we will see exam\\nples of nontrainable parameters in Chapter 11 \\n modelsummary\\n\\nLayer type                 Output Shape              Param \\n\\nflatten1 Flatten          None 784               0\\n\\ndense3 Dense              None 300               235500\\n\\ndense4 Dense              None 100               30100\\n\\ndense5 Dense              None 10                1010\\n\\nTotal params 266610\\nTrainable params 266610\\nNontrainable params 0\\nNote that Dense  layers often have a lot of parameters For example the first hidden\\nlayer has 784  300 connection weights plus 300 bias terms which adds up to\\n235500 parameters This gives the model quite a lot of flexibility to fit the training\\ndata but it also means that the model runs the risk of overfitting especially when you\\ndo not have a lot of traini', 'so means that the model runs the risk of overfitting especially when you\\ndo not have a lot of training data We will come back to this later\\nY ou can easily get a models list of layers to fetch a layer by its index or you can fetch\\nit by name\\n modellayers\\ntensorflowpythonkeraslayerscoreFlatten at 0x132414e48\\n tensorflowpythonkeraslayerscoreDense at 0x1324149b0\\n tensorflowpythonkeraslayerscoreDense at 0x1356ba8d0\\n tensorflowpythonkeraslayerscoreDense at 0x13240d240\\n modellayers1name\\ndense3\\n modelgetlayer dense3 name\\ndense3\\nAll the parameters of a layer can be accessed using its getweights  and\\nsetweights  method For a Dense  layer this includes both the connection weights\\nand the bias terms\\nImplementing MLPs with Keras  297 weights biases  hidden1getweights \\n weights\\narray 003854964 004054524  000599282   002566582\\n         001032123  006914985\\n       \\n        002632413 005105981 000332005   004175945\\n         00443138  005558084 dtypefloat32\\n weightsshape\\n784 300\\n biases\\narray0 0 0 0 0 ', '   004175945\\n         00443138  005558084 dtypefloat32\\n weightsshape\\n784 300\\n biases\\narray0 0 0 0 0 0 0 0 0   0 0 0 dtypefloat32\\n biasesshape\\n300\\nNotice that the Dense  layer initialized the connection weights randomly which is\\nneeded to break symmetry as we discussed earlier and the biases were just initial\\nized to zeros which is fine If you ever want to use a different initialization method\\nyou can set kernelinitializer  kernel  is another name for the matrix of connec\\ntion weights or biasinitializer  when creating the layer We will discuss initializ\\ners further in Chapter 11  but if you want the full list see httpskerasioinitializers \\nThe shape of the weight matrix depends on the number of inputs\\nThis is why it is recommended to specify the inputshape  when\\ncreating the first layer in a Sequential  model However if you do\\nnot specify the input shape its okay Keras will simply wait until it\\nknows the input shape before it actually builds the model This will\\nhappen either when you fee', 'il it\\nknows the input shape before it actually builds the model This will\\nhappen either when you feed it actual data eg during training\\nor when you call its build  method Until the model is really\\nbuilt the layers will not have any weights and you will not be able\\nto do certain things such as print the model summary or save the\\nmodel so if you know the input shape when creating the model it\\nis best to specify it\\nCompiling the Model\\nAfter a model is created you must call its compile  method to specify the loss func\\ntion and the optimizer to use Optionally you can also specify a list of extra metrics to\\ncompute during training and evaluation\\nmodelcompilelosssparsecategoricalcrossentropy \\n              optimizer sgd\\n              metricsaccuracy \\n298  Chapter 10 Introduction to Artificial  Neural Networks with KerasUsing losssparsecategoricalcrossentropy  is equivalent to\\nlosskeraslossessparsecategoricalcrossentropy  Simi\\nlarly optimizersgd  is equivalent to optimizerkerasoptimiz\\nersSGD  ', 'rsecategoricalcrossentropy  Simi\\nlarly optimizersgd  is equivalent to optimizerkerasoptimiz\\nersSGD  and metricsaccuracy  is equivalent to\\nmetricskerasmetricssparsecategoricalaccuracy  when\\nusing this loss We will use many other losses optimizers and met\\nrics in this book but for the full lists see httpskerasiolosses \\nhttpskerasiooptimizers  and httpskerasiometrics \\nThis requires some explanation First we use the sparsecategoricalcrossen\\ntropy  loss because we have sparse labels ie for each instance there is just a target\\nclass index from 0 to 9 in this case and the classes are exclusive If instead we had\\none target probability per class for each instance such as onehot vectors eg 0\\n0 0 1 0 0 0 0 0 0  to represent class 3 then we would need\\nto use the categoricalcrossentropy  loss instead If we were doing binary classi\\nfication with one or more binary labels then we would use the sigmoid  ie\\nlogistic activation function in the output layer instead of the softmax  activation\\nfunction and', ' ie\\nlogistic activation function in the output layer instead of the softmax  activation\\nfunction and we would use the binarycrossentropy  loss\\nIf you want to convert sparse labels ie class indices to onehot\\nvector labels you can use the kerasutilstocategorical\\nfunction To go the other way round you can just use the nparg\\nmax  function with axis1 \\nSecondly regarding the optimizer sgd  simply means that we will train the model\\nusing simple Stochastic Gradient Descent In other words Keras will perform the\\nbackpropagation algorithm described earlier ie reversemode autodiff  Gradient\\nDescent We will discuss more efficient optimizers in Chapter 11  they improve the\\nGradient Descent part not the autodiff\\nFinally since this is a classifier its useful to measure its accuracy  during training\\nand evaluation\\nTraining and Evaluating the Model\\nNow the model is ready to be trained For this we simply need to call its fit\\nmethod We pass it the input features  Xtrain  and the target classes  ytrain  as', 'eed to call its fit\\nmethod We pass it the input features  Xtrain  and the target classes  ytrain  as\\nwell as the number of epochs to train or else it would default to just 1 which would\\ndefinitely not be enough to converge to a good solution We also pass a validation set\\nthis is optional Keras will measure the loss and the extra metrics on this set at the\\nend of each epoch which is very useful to see how well the model really performs if\\nthe performance on the training set is much better than on the validation set your\\nImplementing MLPs with Keras  299model is probably overfitting the training set or there is a bug such as a data mis\\nmatch between the training set and the validation set\\n history  modelfitXtrain ytrain epochs30\\n                     validationdata Xvalid yvalid\\n\\nTrain on 55000 samples validate on 5000 samples\\nEpoch 130\\n5500055000   3s 55ussample  loss 14948      acc 05757\\n                                           valloss 10042  valacc 07166\\nEpoch 230\\n5500055000   3s 55u', '                                           valloss 10042  valacc 07166\\nEpoch 230\\n5500055000   3s 55ussample  loss 08690      acc 07318\\n                                           valloss 07549  valacc 07616\\n\\nEpoch 5050\\n5500055000   4s 72ussample  loss 03607      acc 08752\\n                                           valloss 03706  valacc 08728\\nAnd thats it The neural network is trained At each epoch during training Keras dis\\nplays the number of instances processed so far along with a progress bar the mean\\ntraining time per sample the loss and accuracy or any other extra metrics you asked\\nfor both on the training set and the validation set Y ou can see that the training loss\\nwent down which is a good sign and the validation accuracy reached 8728 after 50\\nepochs not too far from the training accuracy so there does not seem to be much\\noverfitting going on\\nInstead of passing a validation set using the validationdata\\nargument you could instead set validationsplit  to the ratio of\\nthe training ', 'ing the validationdata\\nargument you could instead set validationsplit  to the ratio of\\nthe training set that you want Keras to use for validation eg 01\\nIf the training set was very skewed with some classes being overrepresented and oth\\ners underrepresented it would be useful to set the classweight  argument when\\ncalling the fit  method giving a larger weight to underrepresented classes and a\\nlower weight to overrepresented classes These weights would be used by Keras when\\ncomputing the loss If you need perinstance weights instead you can set the sam\\npleweight  argument it supersedes classweight  This could be useful for exam\\nple if some instances were labeled by experts while others were labeled using a\\ncrowdsourcing platform you might want to give more weight to the former Y ou can\\nalso provide sample weights but not class weights for the validation set by adding\\nthem as a third item in the validationdata  tuple\\nThe fit  method returns a History  object containing the training paramet', ' the validationdata  tuple\\nThe fit  method returns a History  object containing the training parameters  his\\ntoryparams  the list of epochs it went through  historyepoch  and most impor\\ntantly a dictionary  historyhistory  containing the loss and extra metrics it\\nmeasured at the end of each epoch on the training set and on the validation set if\\n300  Chapter 10 Introduction to Artificial  Neural Networks with Kerasany If you create a Pandas DataFrame using this dictionary and call its plot\\nmethod you get the learning curves shown in Figure 1012 \\nimport pandas as pd\\npdDataFrame historyhistoryplotfigsize8 5\\npltgridTrue\\npltgcasetylim 0 1  set the vertical range to 01\\npltshow\\nFigure 1012 Learning Curves\\nY ou can see that both the training and validation accuracy steadily increase during\\ntraining while the training and validation loss decrease Good Moreover the valida\\ntion curves are quite close to the training curves which means that there is not too\\nmuch overfitting In this particular case', 'se to the training curves which means that there is not too\\nmuch overfitting In this particular case the model performed better on the valida\\ntion set than on the training set at the beginning of training this sometimes happens\\nby chance especially when the validation set is fairly small However the training set\\nperformance ends up beating the validation performance as is generally the case\\nwhen you train for long enough Y ou can tell that the model has not quite converged\\nyet as the validation loss is still going down so you should probably continue train\\ning Its as simple as calling the fit  method again since Keras just continues train\\ning where it left off you should be able to reach close to 89 validation accuracy\\nIf you are not satisfied with the performance of your model you should go back and\\ntune the models hyperparameters for example the number of layers the number of\\nneurons per layer the types of activation functions we use for each hidden layer the\\nImplementing MLPs with K', 'er layer the types of activation functions we use for each hidden layer the\\nImplementing MLPs with Keras  301number of training epochs the batch size it can be set in the fit  method using the\\nbatchsize  argument which defaults to 32 We will get back to hyperparameter\\ntuning at the end of this chapter Once you are satisfied with your models validation\\naccuracy you should evaluate it on the test set to estimate the generalization error\\nbefore you deploy the model to production Y ou can easily do this using the evalu\\nate  method it also supports several other arguments such as batchsize  or sam\\npleweight  please check the documentation for more details\\n modelevaluate Xtest ytest\\n883210000   ETA 0s  loss 04074  acc 08540\\n040738476498126985 0854\\nAs we saw in Chapter 2  it is common to get slightly lower performance on the test set\\nthan on the validation set because the hyperparameters are tuned on the validation\\nset not the test set however in this example we did not do any hyperparameter ', 'ned on the validation\\nset not the test set however in this example we did not do any hyperparameter tun\\ning so the lower accuracy is just bad luck Remember to resist the temptation to\\ntweak the hyperparameters on the test set or else your estimate of the generalization\\nerror will be too optimistic\\nUsing the Model to Make Predictions\\nNext we can use the models predict  method to make predictions on new instan\\nces Since we dont have actual new instances we will just use the first 3 instances of\\nthe test set\\n Xnew  Xtest3\\n yproba  modelpredictXnew\\n yprobaround2\\narray0   0   0   0   0   009 0   012 0   079\\n       0   0   094 0   002 0   004 0   0   0  \\n       0   1   0   0   0   0   0   0   0   0  \\n      dtypefloat32\\nAs you can see for each instance the model estimates one probability per class from\\nclass 0 to class 9 For example for the first image it estimates that the probability of\\nclass 9 ankle boot is 79 the probability of class 7 sneaker is 12 the probability\\nof class 5 sandal is 9 ', ' 9 ankle boot is 79 the probability of class 7 sneaker is 12 the probability\\nof class 5 sandal is 9 and the other classes are negligible In other words it\\nbelieves its footwear probably ankle boots but its not entirely sure it might be\\nsneakers or sandals instead If you only care about the class with the highest estima\\nted probability even if that probability is quite low then you can use the pre\\ndictclasses  method instead\\n ypred  modelpredictclasses Xnew\\n ypred\\narray9 2 1\\n nparrayclassnames ypred\\narrayAnkle boot Pullover Trouser dtypeU11\\nAnd the classifier actually classified all three images correctly\\n302  Chapter 10 Introduction to Artificial  Neural Networks with Keras ynew  ytest3\\n ynew\\narray9 2 1\\nNow you know how to build train evaluate and use a classification MLP using the\\nSequential API But what about regression\\nBuilding a Regression MLP Using the Sequential API\\nLets switch to the California housing problem and tackle it using a regression neural\\nnetwork For simplicity we wil', 'the California housing problem and tackle it using a regression neural\\nnetwork For simplicity we will use ScikitLearns fetchcaliforniahousing\\nfunction to load the data this dataset is simpler than the one we used in Chapter 2 \\nsince it contains only numerical features there is no oceanproximity  feature and\\nthere is no missing value After loading the data we split it into a training set a vali\\ndation set and a test set and we scale all the features\\nfrom sklearndatasets  import fetchcaliforniahousing\\nfrom sklearnmodelselection  import traintestsplit\\nfrom sklearnpreprocessing  import StandardScaler\\nhousing  fetchcaliforniahousing \\nXtrainfull  Xtest ytrainfull  ytest  traintestsplit \\n    housingdata housingtarget\\nXtrain Xvalid ytrain yvalid  traintestsplit \\n    Xtrainfull  ytrainfull \\nscaler  StandardScaler \\nXtrainscaled   scalerfittransform Xtrain\\nXvalidscaled   scalertransform Xvalid\\nXtestscaled   scalertransform Xtest\\nBuilding training evaluating and using a regression MLP using the Se', 'tscaled   scalertransform Xtest\\nBuilding training evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification The main differ\\nences are the fact that the output layer has a single neuron since we only want to\\npredict a single value and uses no activation function and the loss function is the\\nmean squared error Since the dataset is quite noisy we just use a single hidden layer\\nwith fewer neurons than before to avoid overfitting\\nmodel  kerasmodelsSequential \\n    keraslayersDense30 activation relu inputshape Xtrainshape1\\n    keraslayersDense1\\n\\nmodelcompilelossmeansquarederror  optimizer sgd\\nhistory  modelfitXtrain ytrain epochs20\\n                    validationdata Xvalid yvalid\\nmsetest   modelevaluate Xtest ytest\\nXnew  Xtest3  pretend these are new instances\\nypred  modelpredictXnew\\nImplementing MLPs with Keras  30314Wide  Deep Learning for Recommender Systems  HengTze Cheng et al 2016As you can see the Sequential AP', 'ide  Deep Learning for Recommender Systems  HengTze Cheng et al 2016As you can see the Sequential API is quite easy to use However although sequential\\nmodels are extremely common it is sometimes useful to build neural networks with\\nmore complex topologies or with multiple inputs or outputs For this purpose Keras\\noffers the Functional API\\nBuilding Complex Models Using the Functional API\\nOne example of a nonsequential neural network is a Wide  Deep  neural network\\nThis neural network architecture was introduced in a 2016 paper  by HengTze Cheng\\net al14 It connects all or part of the inputs directly to the output layer as shown in\\nFigure 1013  This architecture makes it possible for the neural network to learn both\\ndeep patterns using the deep path and simple rules through the short path In\\ncontrast a regular MLP forces all the data to flow through the full stack of layers thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor\\nmations\\n304  Chapter 10 Intr', 'rns in the data may end up being distorted by this sequence of transfor\\nmations\\n304  Chapter 10 Introduction to Artificial  Neural Networks with KerasFigure 1013 Wide and Deep Neural Network\\nLets build such a neural network to tackle the California housing problem\\ninput  keraslayersInputshapeXtrainshape1\\nhidden1  keraslayersDense30 activation reluinput\\nhidden2  keraslayersDense30 activation reluhidden1\\nconcat  keraslayersConcatenate input hidden2\\noutput  keraslayersDense1concat\\nmodel  kerasmodelsModelinputsinput outputsoutput\\nLets go through each line of this code\\nFirst we need to create an Input  object This is needed because we may have\\nmultiple inputs as we will see later\\nNext we create a Dense  layer with 30 neurons and using the ReLU activation\\nfunction As soon as it is created notice that we call it like a function passing it\\nthe input This is why this is called the Functional API Note that we are just tell\\nImplementing MLPs with Keras  305ing Keras how it should connect the laye', 'Note that we are just tell\\nImplementing MLPs with Keras  305ing Keras how it should connect the layers together no actual data is being pro\\ncessed yet\\nWe then create a second hidden layer and again we use it as a function Note\\nhowever that we pass it the output of the first hidden layer\\nNext we create a Concatenate  layer and once again we immediately use it like\\na function to concatenate the input and the output of the second hidden layer\\nyou may prefer the keraslayersconcatenate  function which creates a Con\\ncatenate  layer and immediately calls it with the given inputs\\nThen we create the output layer with a single neuron and no activation function\\nand we call it like a function passing it the result of the concatenation\\nLastly we create a Keras Model  specifying which inputs and outputs to use\\nOnce you have built the Keras model everything is exactly like earlier so no need to\\nrepeat it here you must compile the model train it evaluate it and use it to make\\npredictions\\nBut what if y', 'it here you must compile the model train it evaluate it and use it to make\\npredictions\\nBut what if you want to send a subset of the features through the wide path and a\\ndifferent subset possibly overlapping through the deep path see Figure 1014  In\\nthis case one solution is to use multiple inputs For example suppose we want to\\nsend 5 features through the deep path features 0 to 4 and 6 features through the\\nwide path features 2 to 7\\ninputA  keraslayersInputshape5\\ninputB  keraslayersInputshape6\\nhidden1  keraslayersDense30 activation reluinputB\\nhidden2  keraslayersDense30 activation reluhidden1\\nconcat  keraslayersconcatenate inputA hidden2\\noutput  keraslayersDense1concat\\nmodel  kerasmodelsModelinputsinputA inputB outputsoutput\\n306  Chapter 10 Introduction to Artificial  Neural Networks with KerasFigure 1014 Handling Multiple Inputs\\nThe code is selfexplanatory Note that we specified inputsinputA inputB\\nwhen creating the model Now we can compile the model as usual but when we call\\nthe fit  ', 'putA inputB\\nwhen creating the model Now we can compile the model as usual but when we call\\nthe fit  method instead of passing a single input matrix Xtrain  we must pass a\\npair of matrices XtrainA XtrainB  one per input The same is true for\\nXvalid  and also for Xtest  and Xnew  when you call evaluate  or predict \\nmodelcompilelossmse optimizer sgd\\nXtrainA  XtrainB   Xtrain 5 Xtrain 2\\nXvalidA  XvalidB   Xvalid 5 Xvalid 2\\nXtestA  XtestB   Xtest 5 Xtest 2\\nXnewA XnewB  XtestA 3 XtestB 3\\nhistory  modelfitXtrainA  XtrainB  ytrain epochs20\\n                    validationdata XvalidA  XvalidB  yvalid\\nmsetest   modelevaluate XtestA  XtestB  ytest\\nypred  modelpredictXnewA XnewB\\nImplementing MLPs with Keras  307There are also many use cases in which you may want to have multiple outputs\\nThe task may demand it for example you may want to locate and classify the\\nmain object in a picture This is both a regression task finding the coordinates of\\nthe objects center as well as its width and height and a c', 'egression task finding the coordinates of\\nthe objects center as well as its width and height and a classification task\\nSimilarly you may have multiple independent tasks to perform based on the\\nsame data Sure you could train one neural network per task but in many cases\\nyou will get better results on all tasks by training a single neural network with\\none output per task This is because the neural network can learn features in the\\ndata that are useful across tasks\\nAnother use case is as a regularization technique ie a training constraint whose\\nobjective is to reduce overfitting and thus improve the models ability to general\\nize For example you may want to add some auxiliary outputs in a neural net\\nwork architecture see Figure 1015  to ensure that the underlying part of the\\nnetwork learns something useful on its own without relying on the rest of the\\nnetwork\\nFigure 1015 Handling Multiple Outputs  Auxiliary Output for Regularization\\nAdding extra outputs is quite easy just connect them to t', 'tputs  Auxiliary Output for Regularization\\nAdding extra outputs is quite easy just connect them to the appropriate layers and\\nadd them to your models list of outputs For example the following code builds the\\nnetwork represented in Figure 1015 \\n308  Chapter 10 Introduction to Artificial  Neural Networks with Keras  Same as above up to the main output layer\\noutput  keraslayersDense1concat\\nauxoutput   keraslayersDense1hidden2\\nmodel  kerasmodelsModelinputsinputA inputB\\n                           outputsoutput auxoutput \\nEach output will need its own loss function so when we compile the model we\\nshould pass a list of losses if we pass a single loss Keras will assume that the same\\nloss must be used for all outputs By default Keras will compute all these losses and\\nsimply add them up to get the final loss used for training However we care much\\nmore about the main output than about the auxiliary output as it is just used for reg\\nularization so we want to give the main outputs loss a much great', ' output as it is just used for reg\\nularization so we want to give the main outputs loss a much greater weight Fortu\\nnately it is possible to set all the loss weights when compiling the model\\nmodelcompilelossmse mse lossweights 09 01 optimizer sgd\\nNow when we train the model we need to provide some labels for each output In\\nthis example the main output and the auxiliary output should try to predict the same\\nthing so they should use the same labels So instead of passing ytrain  we just need\\nto pass ytrain ytrain  and the same goes for yvalid  and ytest \\nhistory  modelfit\\n    XtrainA  XtrainB  ytrain ytrain epochs20\\n    validationdata XvalidA  XvalidB  yvalid yvalid\\nWhen we evaluate the model Keras will return the total loss as well as all the individ\\nual losses\\ntotalloss  mainloss  auxloss   modelevaluate \\n    XtestA  XtestB  ytest ytest\\nSimilarly the predict  method will return predictions for each output\\nypredmain  ypredaux   modelpredictXnewA XnewB\\nAs you can see you can build any sor', 's for each output\\nypredmain  ypredaux   modelpredictXnewA XnewB\\nAs you can see you can build any sort of architecture you want quite easily with the\\nFunctional API Lets look at one last way you can build Keras models\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative you start by declar\\ning which layers you want to use and how they should be connected and only then\\ncan you start feeding the model some data for training or inference This has many\\nadvantages the model can easily be saved cloned shared its structure can be dis\\nplayed and analyzed the framework can infer shapes and check types so errors can\\nbe caught early ie before any data ever goes through the model Its also fairly easy\\nto debug since the whole model is just a static graph of layers But the flip side is just\\nthat its static Some models involve loops varying shapes conditional branching\\nand other dynamic behaviors For such cases or simply if you prefer a more ', 'pes conditional branching\\nand other dynamic behaviors For such cases or simply if you prefer a more impera\\ntive programming style the Subclassing API is for you\\nImplementing MLPs with Keras  30915Keras models have an output  attribute so we cannot use that name for the main output layer which is why\\nwe renamed it to mainoutput Simply subclass the Model  class create the layers you need in the constructor and use\\nthem to perform the computations you want in the call  method For example cre\\nating an instance of the following WideAndDeepModel  class gives us an equivalent\\nmodel to the one we just built with the Functional API Y ou can then compile it eval\\nuate it and use it to make predictions exactly like we just did\\nclass WideAndDeepModel kerasmodelsModel\\n    def init self units30 activation relu kwargs\\n        superinit kwargs  handles standard args eg name\\n        selfhidden1  keraslayersDenseunits activation activation \\n        selfhidden2  keraslayersDenseunits activation activation', 'rsDenseunits activation activation \\n        selfhidden2  keraslayersDenseunits activation activation \\n        selfmainoutput   keraslayersDense1\\n        selfauxoutput   keraslayersDense1\\n    def callself inputs\\n        inputA inputB  inputs\\n        hidden1  selfhidden1inputB\\n        hidden2  selfhidden2hidden1\\n        concat  keraslayersconcatenate inputA hidden2\\n        mainoutput   selfmainoutput concat\\n        auxoutput   selfauxoutput hidden2\\n        return mainoutput  auxoutput\\nmodel  WideAndDeepModel \\nThis example looks very much like the Functional API except we do not need to cre\\nate the inputs we just use the input  argument to the call  method and we separate\\nthe creation of the layers15 in the constructor from their usage in the call  method\\nHowever the big difference is that you can do pretty much anything you want in the\\ncall  method for loops if statements lowlevel TensorFlow operations your\\nimagination is the limit see Chapter 12  This makes it a great API for researcher', 'ow operations your\\nimagination is the limit see Chapter 12  This makes it a great API for researchers\\nexperimenting with new ideas\\nHowever this extra flexibility comes at a cost your models architecture is hidden\\nwithin the call  method so Keras cannot easily inspect it it cannot save or clone it\\nand when you call the summary  method you only get a list of layers without any\\ninformation on how they are connected to each other Moreover Keras cannot check\\ntypes and shapes ahead of time and it is easier to make mistakes So unless you really\\nneed that extra flexibility you should probably stick to the Sequential API or the\\nFunctional API\\n310  Chapter 10 Introduction to Artificial  Neural Networks with KerasKeras models can be used just like regular layers so you can easily\\ncompose them to build complex architectures\\nNow that you know how to build and train neural nets using Keras you will want to\\nsave them\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets\\nmo', 'nt to\\nsave them\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets\\nmodelsavemykerasmodelh5 \\nKeras will save both the models architecture including every layers hyperparame\\nters and the value of all the model parameters for every layer eg connection\\nweights and biases using the HDF5 format It also saves the optimizer including its\\nhyperparameters and any state it may have\\nY ou will typically have a script that trains a model and saves it and one or more\\nscripts or web services that load the model and use it to make predictions Loading\\nthe model is just as easy\\nmodel  kerasmodelsloadmodel mykerasmodelh5 \\nThis will work when using the Sequential API or the Functional\\nAPI but unfortunately not when using Model subclassing How\\never you can use saveweights  and loadweights  to at least\\nsave and restore the model parameters but you will need to save\\nand restore everything else yourself\\nBut what if training lasts several hours This is quite common especially when', 'erything else yourself\\nBut what if training lasts several hours This is quite common especially when train\\ning on large datasets In this case you should not only save your model at the end of\\ntraining but also save checkpoints at regular intervals during training But how can\\nyou tell the fit  method to save checkpoints The answer is using callbacks\\nUsing Callbacks\\nThe fit  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training at the start and end\\nof each epoch and even before and after processing each batch For example the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining by default at the end of each epoch\\nImplementing MLPs with Keras  311  build and compile the model\\ncheckpointcb   kerascallbacks ModelCheckpoint mykerasmodelh5 \\nhistory  modelfitXtrain ytrain epochs10 callbacks checkpointcb \\nMoreover if you use a validation set during training you ', 'in ytrain epochs10 callbacks checkpointcb \\nMoreover if you use a validation set during training you can set\\nsavebestonlyTrue  when creating the ModelCheckpoint  In this case it will only\\nsave your model when its performance on the validation set is the best so far This\\nway you do not need to worry about training for too long and overfitting the training\\nset simply restore the last model saved after training and this will be the best model\\non the validation set This is a simple way to implement early stopping introduced in\\nChapter 4 \\ncheckpointcb   kerascallbacks ModelCheckpoint mykerasmodelh5 \\n                                                savebestonly True\\nhistory  modelfitXtrain ytrain epochs10\\n                    validationdata Xvalid yvalid\\n                    callbacks checkpointcb \\nmodel  kerasmodelsloadmodel mykerasmodelh5   rollback to best model\\nAnother way to implement early stopping is to simply use the EarlyStopping  call\\nback It will interrupt training when it measures no', 'topping is to simply use the EarlyStopping  call\\nback It will interrupt training when it measures no progress on the validation set for\\na number of epochs defined by the patience  argument and it will optionally roll\\nback to the best model Y ou can combine both callbacks to both save checkpoints of\\nyour model in case your computer crashes and actually interrupt training early\\nwhen there is no more progress to avoid wasting time and resources\\nearlystoppingcb   kerascallbacks EarlyStopping patience 10\\n                                                  restorebestweights True\\nhistory  modelfitXtrain ytrain epochs100\\n                    validationdata Xvalid yvalid\\n                    callbacks checkpointcb  earlystoppingcb \\nThe number of epochs can be set to a large value since training will stop automati\\ncally when there is no more progress Moreover there is no need to restore the best\\nmodel saved in this case since the EarlyStopping  callback will keep track of the best\\nweights and resto', 'l saved in this case since the EarlyStopping  callback will keep track of the best\\nweights and restore them for us at the end of training\\nThere are many other callbacks available in the kerascallbacks\\npackage See httpskerasiocallbacks \\nIf you need extra control you can easily write your own custom callbacks For exam\\nple the following custom callback will display the ratio between the validation loss\\nand the training loss during training eg to detect overfitting\\n312  Chapter 10 Introduction to Artificial  Neural Networks with Kerasclass PrintValTrainRatioCallback kerascallbacks Callback \\n    def onepochend self epoch logs\\n        printnvaltrain 2f formatlogsvalloss   logsloss\\nAs you might expect you can implement ontrainbegin  ontrainend \\nonepochbegin  onepochbegin  onbatchend  and onbatchend \\nMoreover callbacks can also be used during evaluation and predictions should you\\never need them eg for debugging In this case you should implement\\nontestbegin  ontestend  ontestbatchbegin  or\\nonte', 'eg for debugging In this case you should implement\\nontestbegin  ontestend  ontestbatchbegin  or\\nontestbatchend  called by evaluate  or onpredictbegin  onpre\\ndictend  onpredictbatchbegin  or onpredictbatchend  called by\\npredict \\nNow lets take a look at one more tool you should definitely have in your toolbox\\nwhen using tfkeras TensorBoard\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training compare learning curves between multiple runs vis\\nualize the computation graph analyze training statistics view images generated by\\nyour model visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you and more This tool is installed automatically when\\nyou install TensorFlow so you already have it\\nTo use it you must modify your program so that it outputs the data you want to visu\\nalize to special binary log files called event files Each binary data record is called a\\nsum', 'to visu\\nalize to special binary log files called event files Each binary data record is called a\\nsummary  The TensorBoard server will monitor the log directory and it will automat\\nically pick up the changes and update the visualizations this allows you to visualize\\nlive data with a short delay such as the learning curves during training In general\\nyou want to point the TensorBoard server to a root log directory and configure your\\nprogram so that it writes to a different subdirectory every time it runs This way the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program without getting everything mixed up\\nSo lets start by defining the root log directory we will use for our TensorBoard logs\\nplus a small function that will generate a subdirectory path based on the current date\\nand time so that it is different at every run Y ou may want to include extra informa\\ntion in the log directory name such as hyperparameter values that you are', 't to include extra informa\\ntion in the log directory name such as hyperparameter values that you are testing to\\nmake it easier to know what you are looking at in TensorBoard\\nrootlogdir   ospathjoinoscurdir mylogs \\ndef getrunlogdir \\n    import time\\n    runid  timestrftime runYmdHMS\\n    return ospathjoinrootlogdir  runid\\nImplementing MLPs with Keras  313runlogdir   getrunlogdir   eg mylogsrun20190116112843\\nNext the good news is that Keras provides a nice TensorBoard  callback\\n  Build and compile your model\\ntensorboardcb   kerascallbacks TensorBoard runlogdir \\nhistory  modelfitXtrain ytrain epochs30\\n                    validationdata Xvalid yvalid\\n                    callbacks tensorboardcb \\nAnd thats all there is to it It could hardly be easier to use If you run this code the\\nTensorBoard  callback will take care of creating the log directory for you along with\\nits parent directories if needed and during training it will create event files and write\\nsummaries to them After running the pro', 'ded and during training it will create event files and write\\nsummaries to them After running the program a second time perhaps changing\\nsome hyperparameter value you will end up with a directory structure similar to\\nthis one\\nmylogs\\n run20190116165102\\n    eventsouttfevents1547628669mycomputerlocalv2\\n run20190116165650\\n     eventsouttfevents1547629020mycomputerlocalv2\\nNext you need to start the TensorBoard server If you installed TensorFlow within a\\nvirtualenv you should activate it Next run the following command at the root of the\\nproject or from anywhere else as long as you point to the appropriate log directory\\nIf your shell cannot find the tensorboard  script then you must update your PATH\\nenvironment variable so that it contains the directory in which the script was\\ninstalled alternatively you can just replace tensorboard  with python3 m tensor\\nboardmain \\n tensorboard logdir mylogs port 6006\\nTensorBoard 200 at httpmycomputerlocal6006 Press CTRLC to quit \\nFinally open up a web browse', 'rt 6006\\nTensorBoard 200 at httpmycomputerlocal6006 Press CTRLC to quit \\nFinally open up a web browser to httplocalhost6006  Y ou should see TensorBoards\\nweb interface Click on the SCALARS tab to view the learning curves see\\nFigure 1016  Notice that the training loss went down nicely during both runs but\\nthe second run went down much faster Indeed we used a larger learning rate by set\\nting optimizerkerasoptimizersSGDlr005  instead of optimizersgd \\nwhich defaults to a learning rate of 0001\\n314  Chapter 10 Introduction to Artificial  Neural Networks with KerasFigure 1016 Visualizing Learning Curves with TensorBoard\\nUnfortunately at the time of writing no other data is exported by the TensorBoard\\ncallback but this issue will probably be fixed by the time you read these lines In Ten\\nsorFlow 1 this callback exported the computation graph and many useful statistics\\ntype helpkerascallbacksTensorBoard  to see all the options\\nLets summarize what you learned so far in this chapter we saw where ne', 'Board  to see all the options\\nLets summarize what you learned so far in this chapter we saw where neural nets\\ncame from what an MLP is and how you can use it for classification and regression\\nhow to build MLPs using tfkerass Sequential API or more complex architectures\\nusing the Functional API or Model  Subclassing you learned how to save and restore a\\nmodel use callbacks for checkpointing early stopping and more and finally how to\\nuse TensorBoard for visualization Y ou can already go ahead and use neural networks\\nto tackle many problems However you may wonder how to choose the number of\\nhidden layers the number of neurons in the network and all the other hyperparame\\nters Lets look at this now\\nFineTuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks there are many\\nhyperparameters to tweak Not only can you use any imaginable network architec\\nture but even in a simple MLP you can change the number of layers the number of\\nneurons per', 'architec\\nture but even in a simple MLP you can change the number of layers the number of\\nneurons per layer the type of activation function to use in each layer the weight initi\\nFineTuning Neural Network Hyperparameters  315alization logic and much more How do you know what combination of hyperpara\\nmeters is the best for your task\\nOne option is to simply try many combinations of hyperparameters and see which\\none works best on the validation set or using Kfold crossvalidation For this one\\napproach is simply use GridSearchCV  or RandomizedSearchCV  to explore the hyper\\nparameter space as we did in Chapter 2  For this we need to wrap our Keras models\\nin objects that mimic regular ScikitLearn regressors The first step is to create a func\\ntion that will build and compile a Keras model given a set of hyperparameters\\ndef buildmodel nhidden 1 nneurons 30 learningrate 3e3 inputshape 8\\n    model  kerasmodelsSequential \\n    options  inputshape  inputshape \\n    for layer in rangenhidden \\n        mo', 'erasmodelsSequential \\n    options  inputshape  inputshape \\n    for layer in rangenhidden \\n        modeladdkeraslayersDensenneurons  activation relu options\\n        options  \\n    modeladdkeraslayersDense1 options\\n    optimizer   kerasoptimizers SGDlearningrate \\n    modelcompilelossmse optimizer optimizer \\n    return model\\nThis function creates a simple Sequential  model for univariate regression only one\\noutput neuron with the given input shape and the given number of hidden layers\\nand neurons and it compiles it using an SGD optimizer configured with the given\\nlearning rate The options  dict is used to ensure that the first layer is properly given\\nthe input shape note that if nhidden0  the first layer will be the output layer It is\\ngood practice to provide reasonable defaults to as many hyperparameters as you can\\nas ScikitLearn does\\nNext lets create a KerasRegressor  based on this buildmodel  function\\nkerasreg   keraswrappers scikitlearn KerasRegressor buildmodel \\nThe KerasRegressor  ob', 'del  function\\nkerasreg   keraswrappers scikitlearn KerasRegressor buildmodel \\nThe KerasRegressor  object is a thin wrapper around the Keras model built using\\nbuildmodel  Since we did not specify any hyperparameter when creating it it will\\njust use the default hyperparameters we defined in buildmodel  Now we can use\\nthis object like a regular ScikitLearn regressor we can train it using its fit\\nmethod then evaluate it using its score  method and use it to make predictions\\nusing its predict  method Note that any extra parameter you pass to the fit\\nmethod will simply get passed to the underlying Keras model Also note that the\\nscore will be the opposite of the MSE because ScikitLearn wants scores not losses\\nie higher should be better\\nkerasreg fitXtrain ytrain epochs100\\n              validationdata Xvalid yvalid\\n              callbacks kerascallbacks EarlyStopping patience 10\\nmsetest   kerasreg scoreXtest ytest\\nypred  kerasreg predictXnew\\n316  Chapter 10 Introduction to Artificial  Neural Ne', 'g scoreXtest ytest\\nypred  kerasreg predictXnew\\n316  Chapter 10 Introduction to Artificial  Neural Networks with KerasHowever we do not actually want to train and evaluate a single model like this we\\nwant to train hundreds of variants and see which one performs best on the validation\\nset Since there are many hyperparameters it is preferable to use a randomized search\\nrather than grid search as we discussed in Chapter 2  Lets try to explore the number\\nof hidden layers the number of neurons and the learning rate\\nfrom scipystats  import reciprocal\\nfrom sklearnmodelselection  import RandomizedSearchCV\\nparamdistribs   \\n    nhidden  0 1 2 3\\n    nneurons  nparange1 100\\n    learningrate  reciprocal 3e4 3e2\\n\\nrndsearchcv   RandomizedSearchCV kerasreg  paramdistribs  niter10 cv3\\nrndsearchcv fitXtrain ytrain epochs100\\n                  validationdata Xvalid yvalid\\n                  callbacks kerascallbacks EarlyStopping patience 10\\nAs you can see this is identical to what we did in Chapter 2  with ', 'lbacks EarlyStopping patience 10\\nAs you can see this is identical to what we did in Chapter 2  with the exception that\\nwe pass extra parameters to the fit  method they simply get relayed to the under\\nlying Keras models Note that RandomizedSearchCV  uses Kfold crossvalidation so it\\ndoes not use Xvalid  and yvalid  These are just used for early stopping\\nThe exploration may last many hours depending on the hardware the size of the\\ndataset the complexity of the model and the value of niter  and cv When it is over\\nyou can access the best parameters found the best score and the trained Keras model\\nlike this\\n rndsearchcv bestparams\\nlearningrate 00033625641252688094 nhidden 2 nneurons 42\\n rndsearchcv bestscore\\n03189529188278931\\n model  rndsearchcv bestestimator model\\nY ou can now save this model evaluate it on the test set and if you are satisfied with\\nits performance deploy it to production Using randomized search is not too hard\\nand it works well for many fairly simple problems However when ', 'ng randomized search is not too hard\\nand it works well for many fairly simple problems However when training is slow\\neg for more complex problems with larger datasets this approach will only\\nexplore a tiny portion of the hyperparameter space Y ou can partially alleviate this\\nproblem by assisting the search process manually first run a quick random search\\nusing wide ranges of hyperparameter values then run another search using smaller\\nranges of values centered on the best ones found during the first run and so on This\\nwill hopefully zoom in to a good set of hyperparameters However this is very time\\nconsuming and probably not the best use of your time\\nFortunately there are many techniques to explore a search space much more effi\\nciently than randomly Their core idea is simple when a region of the space turns out\\nFineTuning Neural Network Hyperparameters  31716Population Based Training of Neural Networks  Max Jaderberg et al 2017to be good it should be explored more This takes care of the', 'eural Networks  Max Jaderberg et al 2017to be good it should be explored more This takes care of the zooming process for\\nyou and leads to much better solutions in much less time Here are a few Python\\nlibraries you can use to optimize hyperparameters\\nHyperopt  a popular Python library for optimizing over all sorts of complex\\nsearch spaces including real values such as the learning rate or discrete values\\nsuch as the number of layers\\nHyperas  kopt  or Talos  optimizing hyperparameters for Keras model the first\\ntwo are based on Hyperopt\\nScikitOptimize  skopt a generalpurpose optimization library The Bayes\\nSearchCV  class performs Bayesian optimization using an interface similar to Grid\\nSearchCV \\nSpearmint  a Bayesian optimization library\\nSklearnDeap  a hyperparameter optimization library based on evolutionary\\nalgorithms also with a GridSearchCV like interface\\nAnd many more\\nMoreover many companies offer services for hyperparameter optimization For\\nexample Google Cloud ML Engine has a hyper', 'panies offer services for hyperparameter optimization For\\nexample Google Cloud ML Engine has a hyperparameter tuning service  Other com\\npanies provide APIs for hyperparameter optimization such as Arimo  SigOpt  Oscar\\nand many more\\nHyperparameter tuning is still an active area of research Evolutionary algorithms are\\nmaking a comeback lately For example check out DeepMinds excellent 2017 paper16\\nwhere they jointly optimize a population of models and their hyperparameters Goo\\ngle also used an evolutionary approach not just to search for hyperparameters but\\nalso to look for the best neural network architecture for the problem They call this\\nAutoML  and it is already available as a cloud service  Perhaps the days of building\\nneural networks manually will soon be over Check out Googles post  on this topic In\\nfact evolutionary algorithms have also been used successfully to train individual neu\\nral networks replacing the ubiquitous Gradient Descent See this 2017 post  by Uber\\nwhere they introd', 'ral networks replacing the ubiquitous Gradient Descent See this 2017 post  by Uber\\nwhere they introduce their Deep Neuroevolution  technique\\nDespite all this exciting progress and all these tools and services it still helps to have\\nan idea of what values are reasonable for each hyperparameter so you can build a\\nquick prototype and restrict the search space Here are a few guidelines for choosing\\nthe number of hidden layers and neurons in an MLP  and selecting good values for\\nsome of the main hyperparameters\\n318  Chapter 10 Introduction to Artificial  Neural Networks with KerasNumber of Hidden Layers\\nFor many problems you can just begin with a single hidden layer and you will get\\nreasonable results It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons For a\\nlong time these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks But they overlooked the fact that deep n', 'there was no need to investigate any\\ndeeper neural networks But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency  than shallow ones they can model complex functions\\nusing exponentially fewer neurons than shallow nets allowing them to reach much\\nbetter performance with the same amount of training data\\nTo understand why suppose you are asked to draw a forest using some drawing soft\\nware but you are forbidden to use copypaste Y ou would have to draw each tree\\nindividually branch per branch leaf per leaf If you could instead draw one leaf\\ncopypaste it to draw a branch then copypaste that branch to create a tree and\\nfinally copypaste this tree to make a forest you would be finished in no time Real\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact lower hidden layers model lowlevel struc\\ntures eg line segments of various shapes and orientations intermediate hidden\\nlayers combine thes', 'uc\\ntures eg line segments of various shapes and orientations intermediate hidden\\nlayers combine these lowlevel structures to model intermediatelevel structures eg\\nsquares circles and the highest hidden layers and the output layer combine these\\nintermediate structures to model highlevel structures eg faces\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol\\nution it also improves their ability to generalize to new datasets For example if you\\nhave already trained a model to recognize faces in pictures and you now want to\\ntrain a new neural network to recognize hairstyles then you can kickstart training by\\nreusing the lower layers of the first network Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network\\nThis way the network will not have to learn from scratch all the lowlevel structures\\nthat occur in m', 'This way the network will not have to learn from scratch all the lowlevel structures\\nthat occur in most pictures it will only have to learn the higherlevel structures eg\\nhairstyles This is called transfer learning \\nIn summary for many problems you can start with just one or two hidden layers and\\nit will work just fine eg you can easily reach above 97 accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons and above 98 accu\\nracy using two hidden layers with the same total amount of neurons in roughly the\\nsame amount of training time For more complex problems you can gradually ramp\\nup the number of hidden layers until you start overfitting the training set Very com\\nplex tasks such as large image classification or speech recognition typically require\\nnetworks with dozens of layers or even hundreds but not fully connected ones as\\nwe will see in Chapter 14  and they need a huge amount of training data However\\nyou will rarely have to train such networks from scra', 'ey need a huge amount of training data However\\nyou will rarely have to train such networks from scratch it is much more common to\\nFineTuning Neural Network Hyperparameters  31917By Vincent Vanhoucke in his Deep Learning class  on Udacitycomreuse parts of a pretrained stateoftheart network that performs a similar task\\nTraining will be a lot faster and require much less data we will discuss this in Chap\\nter 11 \\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires For example the MNIST task requires 28\\nx 28  784 input neurons and 10 output neurons\\nAs for the hidden layers it used to be a common practice to size them to form a pyra\\nmid with fewer and fewer neurons at each layerthe rationale being that many low\\nlevel features can coalesce into far fewer highlevel features For example a typical\\nneural network for MNIST may have three hidden layers the first with 300 neurons\\nthe secon', 'a typical\\nneural network for MNIST may have three hidden layers the first with 300 neurons\\nthe second with 200 and the third with 100 However this practice has been largely\\nabandoned now as it seems that simply using the same number of neurons in all hid\\nden layers performs just as well in most cases or even better and there is just one\\nhyperparameter to tune instead of one per layerfor example all hidden layers could\\nsimply have 150 neurons However depending on the dataset it can sometimes help\\nto make the first hidden layer bigger than the others\\nJust like for the number of layers you can try increasing the number of neurons grad\\nually until the network starts overfitting In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer\\nUnfortunately as you can see finding the perfect amount of neurons is still somewhat\\nof a dark art\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need then use ', '\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need then use early stopping to prevent it from overfitting and other regu\\nlarization techniques such as dropout  as we will see in Chapter 11  This has been\\ndubbed the stretch pants approach17 instead of wasting time looking for pants that\\nperfectly match your size just use large stretch pants that will shrink down to the\\nright size\\nLearning Rate Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP  Here are some of the most important ones and some tips on how\\nto set them\\nThe learning rate is arguably the most important hyperparameter In general the\\noptimal learning rate is about half of the maximum learning rate ie the learn\\n320  Chapter 10 Introduction to Artificial  Neural Networks with Keras18Practical recommendations for gradientbased training of deep architectures  Y oshua Bengio 2012ing rate above which the traini', 'for gradientbased training of deep architectures  Y oshua Bengio 2012ing rate above which the training algorithm diverges as we saw in Chapter 4  So\\na simple approach for tuning the learning rate is to start with a large value that\\nmakes the training algorithm diverge then divide this value by 3 and try again\\nand repeat until the training algorithm stops diverging At that point you gener\\nally wont be too far from the optimal learning rate That said it is sometimes\\nuseful to reduce the learning rate during training we will discuss this in Chap\\nter 11 \\nChoosing a better optimizer than plain old Minibatch Gradient Descent and\\ntuning its hyperparameters is also quite important We will discuss this in Chap\\nter 11 \\nThe batch size can also have a significant impact on your models performance\\nand the training time In general the optimal batch size will be lower than 32 in\\nApril 2018 Y ann Lecun even tweeted  Friends dont let friends use minibatches\\nlarger than 32  A small batch size ensures th', 'even tweeted  Friends dont let friends use minibatches\\nlarger than 32  A small batch size ensures that each training iteration is very\\nfast and although a large batch size will give a more precise estimate of the gradi\\nents in practice this does not matter much since the optimization landscape is\\nquite complex and the direction of the true gradients do not point precisely in\\nthe direction of the optimum However having a batch size greater than 10 helps\\ntake advantage of hardware and software optimizations in particular for matrix\\nmultiplications so it will speed up training Moreover if you use Batch Normal\\nization  see Chapter 11  the batch size should not be too small in general no less\\nthan 20\\nWe discussed the choice of the activation function earlier in this chapter in gen\\neral the ReLU activation function will be a good default for all hidden layers For\\nthe output layer it really depends on your task\\nIn most cases the number of training iterations does not actually need to be\\ntweak', 'ends on your task\\nIn most cases the number of training iterations does not actually need to be\\ntweaked just use early stopping instead\\nFor more best practices make sure to read Y oshua Bengios great 2012 paper18 which\\npresents many practical recommendations for deep networks\\nThis concludes this introduction to artificial neural networks and their implementa\\ntion with Keras In the next few chapters we will discuss techniques to train very\\ndeep nets we will see how to customize your models using TensorFlows lowerlevel\\nAPI and how to load and preprocess data efficiently using the Data API and we will\\ndive into other popular neural network architectures convolutional neural networks\\nfor image processing recurrent neural networks for sequential data autoencoders for\\nFineTuning Neural Network Hyperparameters  32119A few extra ANN architectures are presented in representation learning and generative adversarial networks to model and generate\\ndata19\\nExercises\\n1Visit the TensorFlow Playground a', 'ative adversarial networks to model and generate\\ndata19\\nExercises\\n1Visit the TensorFlow Playground at httpsplaygroundtensorfloworg\\nLayers and patterns try training the default neural network by clicking the run\\nbutton top left Notice how it quickly finds a good solution for the classifica\\ntion task Notice that the neurons in the first hidden layer have learned simple\\npatterns while the neurons in the second hidden layer have learned to com\\nbine the simple patterns of the first hidden layer into more complex patterns\\nIn general the more layers the more complex the patterns can be\\nActivation function try replacing the Tanh activation function with the ReLU\\nactivation function and train the network again Notice that it finds a solution\\neven faster but this time the boundaries are linear This is due to the shape of\\nthe ReLU function\\nLocal minima modify the network architecture to have just one hidden layer\\nwith three neurons Train it multiple times to reset the network weights click\\nthe re', 'ne hidden layer\\nwith three neurons Train it multiple times to reset the network weights click\\nthe reset button next to the play button Notice that the training time varies a\\nlot and sometimes it even gets stuck in a local minimum\\nToo small now remove one neuron to keep just 2 Notice that the neural net\\nwork is now incapable of finding a good solution even if you try multiple\\ntimes The model has too few parameters and it systematically underfits the\\ntraining set\\nLarge enough next set the number of neurons to 8 and train the network sev\\neral times Notice that it is now consistently fast and never gets stuck This\\nhighlights an important finding in neural network theory large neural net\\nworks almost never get stuck in local minima and even when they do these\\nlocal optima are almost as good as the global optimum However they can still\\nget stuck on long plateaus for a long time\\nDeep net and vanishing gradients now change the dataset to be the spiral bot\\ntom right dataset under DATA  Change t', 'nishing gradients now change the dataset to be the spiral bot\\ntom right dataset under DATA  Change the network architecture to have 4\\nhidden layers with 8 neurons each Notice that training takes much longer and\\noften gets stuck on plateaus for long periods of time Also notice that the neu\\nrons in the highest layers ie on the right tend to evolve faster than the neu\\nrons in the lowest layers ie on the left This problem called the vanishing\\ngradients problem can be alleviated using better weight initialization and\\n322  Chapter 10 Introduction to Artificial  Neural Networks with Kerasother techniques better optimizers such as AdaGrad or Adam or using\\nBatch Normalization\\nMore go ahead and play with the other parameters to get a feel of what they\\ndo In fact you should definitely play with this UI for at least one hour it will\\ngrow your intuitions about neural networks significantly\\n2Draw an ANN using the original artificial neurons like the ones in Figure 103 \\nthat computes A  B where  repr', 'NN using the original artificial neurons like the ones in Figure 103 \\nthat computes A  B where  represents the XOR operation Hint A  B  A\\n  B   A  B\\n3Why is it generally preferable to use a Logistic Regression classifier rather than a\\nclassical Perceptron ie a single layer of threshold logic units trained using the\\nPerceptron training algorithm How can you tweak a Perceptron to make it\\nequivalent to a Logistic Regression classifier\\n4Why was the logistic activation function a key ingredient in training the first\\nMLPs\\n5Name three popular activation functions Can you draw them\\n6Suppose you have an MLP composed of one input layer with 10 passthrough\\nneurons followed by one hidden layer with 50 artificial neurons and finally one\\noutput layer with 3 artificial neurons All artificial neurons use the ReLU activa\\ntion function\\nWhat is the shape of the input matrix X\\nWhat about the shape of the hidden layers weight vector Wh and the shape of\\nits bias vector bh\\nWhat is the shape of the output lay', 'dden layers weight vector Wh and the shape of\\nits bias vector bh\\nWhat is the shape of the output layers weight vector Wo and its bias vector bo\\nWhat is the shape of the networks output matrix Y\\nWrite the equation that computes the networks output matrix Y as a function\\nof X Wh bh Wo and bo\\n7How many neurons do you need in the output layer if you want to classify email\\ninto spam or ham What activation function should you use in the output layer\\nIf instead you want to tackle MNIST how many neurons do you need in the out\\nput layer using what activation function Answer the same questions for getting\\nyour network to predict housing prices as in Chapter 2 \\n8What is backpropagation and how does it work What is the difference between\\nbackpropagation and reversemode autodiff\\n9Can you list all the hyperparameters you can tweak in an MLP If the MLP over\\nfits the training data how could you tweak these hyperparameters to try to solve\\nthe problem\\nExercises  32310Train a deep MLP on the MNIST datase', 'ese hyperparameters to try to solve\\nthe problem\\nExercises  32310Train a deep MLP on the MNIST dataset and see if you can get over 98 preci\\nsion Try adding all the bells and whistles ie save checkpoints use early stop\\nping plot learning curves using TensorBoard and so on\\nSolutions to these exercises are available in \\n324  Chapter 10 Introduction to Artificial  Neural Networks with KerasCHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 11 in the final\\nrelease of the book\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks But they were very shallow nets with just a few hidden layers What\\nif you need to tackle a very complex problem such as detecting hundreds of types of\\nobjects in highresolution images Y ou m', ' very complex problem such as detecting hundreds of types of\\nobjects in highresolution images Y ou may need to train a much deeper DNN per\\nhaps with 10 layers or much more each containing hundreds of neurons connected\\nby hundreds of thousands of connections This would not be a walk in the park\\nFirst you would be faced with the tricky vanishing gradients  problem or the\\nrelated exploding gradients  problem that affects deep neural networks and makes\\nlower layers very hard to train\\nSecond you might not have enough training data for such a large network or it\\nmight be too costly to label\\nThird training may be extremely slow\\nFourth a model with millions of parameters would severely risk overfitting the\\ntraining set especially if there are not enough training instances or they are too\\nnoisy\\nIn this chapter we will go through each of these problems in turn and present techni\\nques to solve them We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular ', ' We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem Next we will look at\\ntransfer learning and unsupervised pretraining which can help you tackle complex\\n3251Understanding the Difficulty of Training Deep Feedforward Neural Networks  X Glorot Y Bengio 2010tasks even when you have little labeled data Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent Finally we will go through a few popular regularization techniques for large\\nneural networks\\nWith these tools you will be able to train very deep nets welcome to Deep Learning\\nVanishingExploding Gradients Problems\\nAs we discussed in Chapter 10  the backpropagation algorithm works by going from\\nthe output layer to the input layer propagating the error gradient on the way Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network it uses these', 'mputed the gradient of the cost function with regards to each\\nparameter in the network it uses these gradients to update each parameter with a\\nGradient Descent step\\nUnfortunately gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers As a result the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged and training never converges to a good\\nsolution This is called the vanishing gradients  problem In some cases the opposite\\ncan happen the gradients can grow bigger and bigger so many layers get insanely\\nlarge weight updates and the algorithm diverges This is the exploding gradients  prob\\nlem which is mostly encountered in recurrent neural networks see  More gener\\nally deep neural networks suffer from unstable gradients different layers may learn at\\nwidely different speeds\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\nit was one of the reasons why deep neural networks were mostly aba', 'ically observed for quite a while\\nit was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time it is only around 2010 that significant progress was made in understand\\ning it A paper titled Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks  by Xavier Glorot and Y oshua Bengio1 found a few suspects includ\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time namely random ini\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1\\nIn short they showed that with this activation function and this initialization scheme\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs Going forward in the network the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers This is actually made worse by\\nthe fact that the logistic function has a mean of ', ' at the top layers This is actually made worse by\\nthe fact that the logistic function has a mean of 05 not 0 the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks\\n326  Chapter 11 Training Deep Neural Networks2Heres an analogy if you set a microphone amplifiers knob too close to zero people wont hear your voice but\\nif you set it too close to the max your voice will be saturated and people wont understand what you are say\\ning Now imagine a chain of such amplifiers they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain Y our voice has to come out of each amplifier at the same amplitude\\nas it came inLooking at the logistic activation function see Figure 111  you can see that when\\ninputs become large negative or positive the function saturates at 0 or 1 with a\\nderivative extremely close to 0 Thus when backpropagation kicks in it has virtually\\nno gradient to propagate back t', 'emely close to 0 Thus when backpropagation kicks in it has virtually\\nno gradient to propagate back through the network and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers so\\nthere is really nothing left for the lower layers\\nFigure 111 Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper Glorot and Bengio propose a way to significantly alleviate this prob\\nlem We need the signal to flow properly in both directions in the forward direction\\nwhen making predictions and in the reverse direction when backpropagating gradi\\nents We dont want the signal to die out nor do we want it to explode and saturate\\nFor the signal to flow properly the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction please check out the paper if you are ', 're and after flowing through a layer in the\\nreverse direction please check out the paper if you are interested in the mathematical\\ndetails It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons these numbers are called the fanin  and fanout  of the\\nlayer but they proposed a good compromise that has proven to work very well in\\npractice the connection weights of each layer must be initialized randomly as\\nVanishingExploding Gradients Problems  3273Such as Delving Deep into Rectifiers Surpassing HumanLevel Performance on ImageNet Classification  K\\nHe et al 2015described in Equation 111  where f anavgf aninf anout2 This initialization\\nstrategy is called Xavier initialization  after the authors first name or Glorot initiali\\nzation  after his last name\\nEquation 111 Glorot initialization when using the logistic activation function\\nNormal distribution with mean 0 and variance\\xa0 21\\nfanavg\\nOr a uniform distribution between\\xa0 r\\xa0and\\xa0  r with\\xa0 r3\\nfanav', 'tion with mean 0 and variance\\xa0 21\\nfanavg\\nOr a uniform distribution between\\xa0 r\\xa0and\\xa0  r with\\xa0 r3\\nfanavg\\nIf you just replace fanavg with fanin in Equation 111  you get an initialization strategy\\nthat was actually already proposed by Y ann LeCun in the 1990s called LeCun initiali\\nzation  which was even recommended in the 1998 book Neural Networks Tricks of the\\nTrade  by Genevieve Orr and KlausRobert Mller Springer It is equivalent to\\nGlorot initialization when fanin  fanout It took over a decade for researchers to realize\\njust how important this trick really is Using Glorot initialization can speed up train\\ning considerably and it is one of the tricks that led to the current success of Deep\\nLearning\\nSome papers3 have provided similar strategies for different activation functions\\nThese strategies differ only by the scale of the variance and whether they use fanavg or\\nfanin as shown in Table 111  for the uniform distribution just compute r32\\nThe initialization strategy for the ReLU activatio', '11  for the uniform distribution just compute r32\\nThe initialization strategy for the ReLU activation function and its variants includ\\ning the ELU activation described shortly is sometimes called He initialization  after\\nthe last name of its author The SELU activation function will be explained later in\\nthis chapter It should be used with LeCun initialization preferably with a normal\\ndistribution as we will see\\nTable 111 Initialization parameters for each type of activation function\\nInitialization Activation functions  Normal\\nGlorot None Tanh Logistic Softmax 1  fan avg\\nHe ReLU  variants 2  fan in\\nLeCun SELU 1  fan in\\nBy default Keras uses Glorot initialization with a uniform distribution Y ou can\\nchange this to He initialization by setting kernelinitializerheuniform  or ker\\nnelinitializerhenormal  when creating a layer like this\\n328  Chapter 11 Training Deep Neural Networks4Unless it is part of the first hidden layer a dead neuron may sometimes come back to life gradient descent\\nmay i', ' part of the first hidden layer a dead neuron may sometimes come back to life gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neurons\\ninputs is positive again\\n5Empirical Evaluation of Rectified Activations in Convolution Network  B Xu et al 2015keraslayersDense10 activation relu kernelinitializer henormal \\nIf you want He initialization with a uniform distribution but based on fanavg rather\\nthan fanin you can use the VarianceScaling  initializer like this\\nheavginit   kerasinitializers VarianceScaling scale2 modefanavg \\n                                                 distribution uniform \\nkeraslayersDense10 activation sigmoid  kernelinitializer heavginit \\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing\\nexploding gradients problems were in part due to a poor choice of activation func\\ntion Until then most people had assumed that if Mother Nature had chosen to ', 'oice of activation func\\ntion Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons they must be an excellent\\nchoice But it turns out that other activation functions behave much better in deep\\nneural networks in particular the ReLU activation function mostly because it does\\nnot saturate for positive values and also because it is quite fast to compute\\nUnfortunately the ReLU activation function is not perfect It suffers from a problem\\nknown as the dying ReLUs  during training some neurons effectively die meaning\\nthey stop outputting anything other than 0 In some cases you may find that half of\\nyour networks neurons are dead especially if you used a large learning rate A neu\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set When this happens it just\\nkeeps outputting 0s and gradient descent does not affect it anymore since t', 'hen this happens it just\\nkeeps outputting 0s and gradient descent does not affect it anymore since the gradi\\nent of the ReLU function is 0 when its input is negative4\\nTo solve this problem you may want to use a variant of the ReLU function such as\\nthe leaky ReLU  This function is defined as LeakyReLUz  max z z see\\nFigure 112  The hyperparameter  defines how much the function leaks it is the\\nslope of the function for z  0 and is typically set to 001 This small slope ensures\\nthat leaky ReLUs never die they can go into a long coma but they have a chance to\\neventually wake up A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function In fact setting   02 huge leak seemed to\\nresult in better performance than   001 small leak They also evaluated the\\nrandomized leaky ReLU  RReLU where  is picked randomly in a given range during\\ntraining and it is fixed to an average valu', ' RReLU where  is picked randomly in a given range during\\ntraining and it is fixed to an average value during testing It also performed fairly well\\nand seemed to act as a regularizer reducing the risk of overfitting the training set\\nVanishingExploding Gradients Problems  3296Fast and Accurate Deep Network Learning by Exponential Linear Units ELUs  D Clevert T Unterthiner\\nS Hochreiter 2015Finally they also evaluated the parametric leaky ReLU  PReLU where  is authorized\\nto be learned during training instead of being a hyperparameter it becomes a\\nparameter that can be modified by backpropagation like any other parameter This\\nwas reported to strongly outperform ReLU on large image datasets but on smaller\\ndatasets it runs the risk of overfitting the training set\\nFigure 112 Leaky ReLU\\nLast but not least a 2015 paper  by DjorkArn Clevert et al6 proposed a new activa\\ntion function called the exponential linear unit  ELU that outperformed all the ReLU\\nvariants in their experiments training time ', 'nential linear unit  ELU that outperformed all the ReLU\\nvariants in their experiments training time was reduced and the neural network per\\nformed better on the test set It is represented in Figure 113  and Equation 112\\nshows its definition\\nEquation 112 ELU activation function\\nELUzexp z 1 ifz 0\\nz ifz 0\\n330  Chapter 11 Training Deep Neural Networks7SelfNormalizing Neural Networks  G Klambauer T Unterthiner and A Mayr 2017\\nFigure 113 ELU activation function\\nIt looks a lot like the ReLU function with a few major differences\\nFirst it takes on negative values when z  0 which allows the unit to have an\\naverage output closer to 0 This helps alleviate the vanishing gradients problem\\nas discussed earlier The hyperparameter  defines the value that the ELU func\\ntion approaches when z is a large negative number It is usually set to 1 but you\\ncan tweak it like any other hyperparameter if you want\\nSecond it has a nonzero gradient for z  0 which avoids the dead neurons prob\\nlem\\nThird if  is equal to 1', 'nd it has a nonzero gradient for z  0 which avoids the dead neurons prob\\nlem\\nThird if  is equal to 1 then the function is smooth everywhere including\\naround z  0 which helps speed up Gradient Descent since it does not bounce as\\nmuch left and right of z  0\\nThe main drawback of the ELU activation function is that it is slower to compute\\nthan the ReLU and its variants due to the use of the exponential function but dur\\ning training this is compensated by the faster convergence rate However at test time\\nan ELU network will be slower than a ReLU network\\nMoreover in a 2017 paper7 by Gnter Klambauer et al called SelfNormalizing\\nNeural Networks  the authors showed that if you build a neural network composed\\nexclusively of a stack of dense layers and if all hidden layers use the SELU activation\\nfunction which is just a scaled version of the ELU activation function as its name\\nsuggests then the network will selfnormalize  the output of each layer will tend to\\npreserve mean 0 and standard deviatio', 'work will selfnormalize  the output of each layer will tend to\\npreserve mean 0 and standard deviation 1 during training which solves the vanish\\ningexploding gradients problem As a result this activation function often outper\\nVanishingExploding Gradients Problems  331forms other activation functions very significantly for such neural nets especially\\ndeep ones However there are a few conditions for selfnormalization to happen\\nThe input features must be standardized mean 0 and standard deviation 1\\nEvery hidden layers weights must also be initialized using LeCun normal initiali\\nzation In Keras this means setting kernelinitializerlecunnormal \\nThe networks architecture must be sequential Unfortunately if you try to use\\nSELU in nonsequential architectures such as recurrent networks see  or\\nnetworks with skip connections  ie connections that skip layers such as in wide\\n deep nets selfnormalization will not be guaranteed so SELU will not neces\\nsarily outperform other activation functions\\nThe pa', 'on will not be guaranteed so SELU will not neces\\nsarily outperform other activation functions\\nThe paper only guarantees selfnormalization if all layers are dense However in\\npractice the SELU activation function seems to work great with convolutional\\nneural nets as well see Chapter 14 \\nSo which activation function should you use for the hidden layers\\nof your deep neural networks Although your mileage will vary in\\ngeneral SELU  ELU  leaky ReLU and its variants  ReLU  tanh\\n logistic If the networks architecture prevents it from self\\nnormalizing then ELU may perform better than SELU since SELU\\nis not smooth at z  0 If you care a lot about runtime latency then\\nyou may prefer leaky ReLU If you dont want to tweak yet another\\nhyperparameter you may just use the default  values used by\\nKeras eg 03 for the leaky ReLU If you have spare time and\\ncomputing power you can use crossvalidation to evaluate other\\nactivation functions in particular RReLU if your network is over\\nfitting or PReLU if you hav', 'e other\\nactivation functions in particular RReLU if your network is over\\nfitting or PReLU if you have a huge training set\\nTo use the leaky ReLU activation function you must create a LeakyReLU  instance like\\nthis\\nleakyrelu   keraslayersLeakyReLU alpha02\\nlayer  keraslayersDense10 activation leakyrelu \\n                           kernelinitializer henormal \\nFor PReLU just replace LeakyRelualpha02  with PReLU  There is currently no\\nofficial implementation of RReLU in Keras but you can fairly easily implement your\\nown see the exercises at the end of Chapter 12 \\nFor SELU activation just set activationselu  and kernelinitial\\nizerlecunnormal  when creating a layer\\nlayer  keraslayersDense10 activation selu\\n                           kernelinitializer lecunnormal \\n332  Chapter 11 Training Deep Neural Networks8Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift  S Ioffe\\nand C Szegedy 2015Batch Normalization\\nAlthough using He initialization along with ELU or ', 't  S Ioffe\\nand C Szegedy 2015Batch Normalization\\nAlthough using He initialization along with ELU or any variant of ReLU can signifi\\ncantly reduce the vanishingexploding gradients problems at the beginning of train\\ning it doesnt guarantee that they wont come back during training\\nIn a 2015 paper 8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization  BN to address the vanishingexploding gradients problems\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer simply zerocentering and normalizing each\\ninput then scaling and shifting the result using two new parameter vectors per layer\\none for scaling the other for shifting In other words this operation lets the model\\nlearn the optimal scale and mean of each of the layers inputs In many cases if you\\nadd a BN layer as the very first layer of your neural network you do not need to\\nstandardize your training set eg using a StandardScaler  the BN', 'ur neural network you do not need to\\nstandardize your training set eg using a StandardScaler  the BN layer will do it\\nfor you well approximately since it only looks at one batch at a time and it can also\\nrescale and shift each input feature\\nIn order to zerocenter and normalize the inputs the algorithm needs to estimate\\neach inputs mean and standard deviation It does so by evaluating the mean and stan\\ndard deviation of each input over the current minibatch hence the name Batch\\nNormalization The whole operation is summarized in Equation 113 \\nEquation 113 Batch Normalization algorithm\\n1  B1\\nmB\\ni 1mB\\nxi\\n2  B21\\nmB\\ni 1mB\\nxiB2\\n3  xixiB\\nB2\\n4  zixi\\nB is the vector of input means evaluated over the whole minibatch B it con\\ntains one mean per input\\nVanishingExploding Gradients Problems  333B is the vector of input standard deviations also evaluated over the whole mini\\nbatch it contains one standard deviation per input\\nmB is the number of instances in the minibatch\\nxi is the vector of zerocentered', ' deviation per input\\nmB is the number of instances in the minibatch\\nxi is the vector of zerocentered and normalized inputs for instance i\\n is the output scale parameter vector for the layer it contains one scale parame\\nter per input\\n represents elementwise multiplication each input is multiplied by its corre\\nsponding output scale parameter\\n is the output shift offset parameter vector for the layer it contains one offset\\nparameter per input Each input is offset by its corresponding shift parameter\\n is a tiny number to avoid division by zero typically 105 This is called a\\nsmoothing term \\nzi is the output of the BN operation it is a rescaled and shifted version of the\\ninputs\\nSo during training BN just standardizes its inputs then rescales and offsets them\\nGood What about at test time Well it is not that simple Indeed we may need to\\nmake predictions for individual instances rather than for batches of instances in this\\ncase we will have no way to compute each inputs mean and standard deviat', 'atches of instances in this\\ncase we will have no way to compute each inputs mean and standard deviation\\nMoreover even if we do have a batch of instances it may be too small or the instan\\nces may not be independent and identically distributed IID so computing statistics\\nover the batch instances would be unreliable during training the batches should not\\nbe too small if possible more than 30 instances and all instances should be IID as we\\nsaw in Chapter 4  One solution could be to wait until the end of training then run\\nthe whole training set through the neural network and compute the mean and stan\\ndard deviation of each input of the BN layer These final input means and standard\\ndeviations can then be used instead of the batch input means and standard deviations\\nwhen making predictions However it is often preferred to estimate these final statis\\ntics during training using a moving average of the layers input means and standard\\ndeviations To sum up four parameter vectors are learned in eac', 'f the layers input means and standard\\ndeviations To sum up four parameter vectors are learned in each batchnormalized\\nlayer  the ouput scale vector and  the output offset vector are learned through\\nregular backpropagation and  the final input mean vector and  the final input\\nstandard deviation vector are estimated using an exponential moving average Note\\nthat  and  are estimated during training but they are not used at all during train\\ning only after training to replace the batch input means and standard deviations in\\nEquation 113 \\nThe authors demonstrated that this technique considerably improved all the deep\\nneural networks they experimented with leading to a huge improvement in the\\nImageNet classification task ImageNet is a large database of images classified into\\nmany classes and commonly used to evaluate computer vision systems The vanish\\n334  Chapter 11 Training Deep Neural Networksing gradients problem was strongly reduced to the point that they could use saturat\\ning activation ', 'sing gradients problem was strongly reduced to the point that they could use saturat\\ning activation functions such as the tanh and even the logistic activation function\\nThe networks were also much less sensitive to the weight initialization They were\\nable to use much larger learning rates significantly speeding up the learning process\\nSpecifically they note that  Applied to a stateoftheart image classification model\\nBatch Normalization achieves the same accuracy with 14 times fewer training steps\\nand beats the original model by a significant margin  Using an ensemble of\\nbatchnormalized networks we improve upon the best published result on ImageNet\\nclassification reaching 49 top5 validation error and 48 test error exceeding\\nthe accuracy of human raters  Finally like a gift that keeps on giving Batch Normal\\nization also acts like a regularizer reducing the need for other regularization techni\\nques such as dropout described later in this chapter\\nBatch Normalization does however add some c', 'hni\\nques such as dropout described later in this chapter\\nBatch Normalization does however add some complexity to the model although it\\ncan remove the need for normalizing the input data as we discussed earlier More\\nover there is a runtime penalty the neural network makes slower predictions due to\\nthe extra computations required at each layer So if you need predictions to be\\nlightningfast you may want to check how well plain ELU  He initialization perform\\nbefore playing with Batch Normalization\\nY ou may find that training is rather slow because each epoch takes\\nmuch more time when you use batch normalization However this\\nis usually counterbalanced by the fact that convergence is much\\nfaster with BN so it will take fewer epochs to reach the same per\\nformance All in all wall time  will usually be smaller this is the\\ntime measured by the clock on your wall\\nImplementing Batch Normalization with Keras\\nAs with most things with Keras implementing Batch Normalization is quite simple\\nJust add a ', 'th Keras\\nAs with most things with Keras implementing Batch Normalization is quite simple\\nJust add a BatchNormalization  layer before or after each hidden layers activation\\nfunction and optionally add a BN layer as well as the first layer in your model For\\nexample this model applies BN after every hidden layer and as the first layer in the\\nmodel after flattening the input images\\nmodel  kerasmodelsSequential \\n    keraslayersFlatteninputshape 28 28\\n    keraslayersBatchNormalization \\n    keraslayersDense300 activation elu kernelinitializer henormal \\n    keraslayersBatchNormalization \\n    keraslayersDense100 activation elu kernelinitializer henormal \\n    keraslayersBatchNormalization \\n    keraslayersDense10 activation softmax \\n\\nVanishingExploding Gradients Problems  3359However they are estimated during training based on the training data so arguably they are trainable In\\nKeras Nontrainable really means untouched by backpropagation Thats all In this tiny example with just two hidden layers ', 'eally means untouched by backpropagation Thats all In this tiny example with just two hidden layers its unlikely that Batch\\nNormalization will have a very positive impact but for deeper networks it can make a\\ntremendous difference\\nLets zoom in a bit If you display the model summary you can see that each BN layer\\nadds 4 parameters per input    and  for example the first BN layer adds 3136\\nparameters which is 4 times 784 The last two parameters  and  are the moving\\naverages they are not affected by backpropagation so Keras calls them Non\\ntrainable9 if you count the total number of BN parameters 3136  1200  400 and\\ndivide by two you get 2368 which is the total number of nontrainable params in\\nthis model\\n modelsummary\\nModel sequential3\\n\\nLayer type                 Output Shape              Param \\n\\nflatten3 Flatten          None 784               0\\n\\nbatchnormalizationv2 Batc None 784               3136\\n\\ndense50 Dense             None 300               235500\\n\\nbatchnormalizationv21 Ba None 30', '     3136\\n\\ndense50 Dense             None 300               235500\\n\\nbatchnormalizationv21 Ba None 300               1200\\n\\ndense51 Dense             None 100               30100\\n\\nbatchnormalizationv22 Ba None 100               400\\n\\ndense52 Dense             None 10                1010\\n\\nTotal params 271346\\nTrainable params 268978\\nNontrainable params 2368\\nLets look at the parameters of the first BN layer Two are trainable by backprop and\\ntwo are not\\n varname vartrainable  for var in modellayers1variables \\nbatchnormalizationv2gamma0 True\\n batchnormalizationv2beta0 True\\n batchnormalizationv2movingmean0 False\\n batchnormalizationv2movingvariance0 False\\nNow when you create a BN layer in Keras it also creates two operations that will be\\ncalled by Keras at each iteration during training These operations will update the\\n336  Chapter 11 Training Deep Neural Networksmoving averages Since we are using the TensorFlow backend these operations are\\nTensorFlow operations we will discuss TF operations in ', ' the TensorFlow backend these operations are\\nTensorFlow operations we will discuss TF operations in Chapter 12 \\n modellayers1updates\\ntfOperation cond2Identity typeIdentity\\n tfOperation cond3Identity typeIdentity\\nThe authors of the BN paper argued in favor of adding the BN layers before the acti\\nvation functions rather than after as we just did There is some debate about this as\\nit seems to depend on the task So thats one more thing you can experiment with to\\nsee which option works best on your dataset To add the BN layers before the activa\\ntion functions we must remove the activation function from the hidden layers and\\nadd them as separate layers after the BN layers Moreover since a Batch Normaliza\\ntion layer includes one offset parameter per input you can remove the bias term from\\nthe previous layer just pass usebiasFalse  when creating it\\nmodel  kerasmodelsSequential \\n    keraslayersFlatteninputshape 28 28\\n    keraslayersBatchNormalization \\n    keraslayersDense300 kernelinitializer h', 'atteninputshape 28 28\\n    keraslayersBatchNormalization \\n    keraslayersDense300 kernelinitializer henormal  usebias False\\n    keraslayersBatchNormalization \\n    keraslayersActivation elu\\n    keraslayersDense100 kernelinitializer henormal  usebias False\\n    keraslayersActivation elu\\n    keraslayersBatchNormalization \\n    keraslayersDense10 activation softmax \\n\\nThe BatchNormalization  class has quite a few hyperparameters you can tweak The\\ndefaults will usually be fine but you may occasionally need to tweak the momentum \\nThis hyperparameter is used when updating the exponential moving averages given a\\nnew value v ie a new vector of input means or standard deviations computed over\\nthe current batch the running average  is updated using the following equation\\nv v momentum  v1  momentum\\nA good momentum value is typically close to 1for example 09 099 or 0999 you\\nwant more 9s for larger datasets and smaller minibatches\\nAnother important hyperparameter is axis  it determines which axis should', 'ts and smaller minibatches\\nAnother important hyperparameter is axis  it determines which axis should be nor\\nmalized It defaults to 1 meaning that by default it will normalize the last axis using\\nthe means and standard deviations computed across the other  axes For example\\nwhen the input batch is 2D ie the batch shape is batch size features this means\\nthat each input feature will be normalized based on the mean and standard deviation\\ncomputed across all the instances in the batch For example the first BN layer in the\\nprevious code example will independently normalize and rescale and shift each of\\nthe 784 input features However if we move the first BN layer before the Flatten\\nVanishingExploding Gradients Problems  33710Fixup Initialization Residual Learning Without Normalization  Hongyi Zhang Y ann N Dauphin Tengyu\\nMa 2019\\n11On the difficulty of training recurrent neural networks  R Pascanu et al 2013layer then the input batches will be 3D with shape batch size height width there\\nfore th', 'u et al 2013layer then the input batches will be 3D with shape batch size height width there\\nfore the BN layer will compute 28 means and 28 standard deviations one per column\\nof pixels computed across all instances in the batch and all rows in the column and\\nit will normalize all pixels in a given column using the same mean and standard devi\\nation There will also be just 28 scale parameters and 28 shift parameters If instead\\nyou still want to treat each of the 784 pixels independently then you should set\\naxis1 2 \\nNotice that the BN layer does not perform the same computation during training and\\nafter training it uses batch statistics during training and the final statistics after\\ntraining ie the final value of the moving averages Lets take a peek at the source\\ncode of this class to see how this is handled\\nclass BatchNormalization Layer\\n    \\n    def callself inputs training None\\n        if training  is None\\n            training   kerasbackendlearningphase \\n        \\nThe call  method is t', 'f training  is None\\n            training   kerasbackendlearningphase \\n        \\nThe call  method is the one that actually performs the computations and as you\\ncan see it has an extra training  argument if it is None  it falls back to kerasback\\nendlearningphase  which returns 1 during training the fit  method ensures\\nthat Otherwise it returns 0 If you ever need to write a custom layer and it needs to\\nbehave differently during training and testing simply use the same pattern we will\\ndiscuss custom layers in Chapter 12 \\nBatch Normalization has become one of the most used layers in deep neural net\\nworks to the point that it is often omitted in the diagrams as it is assumed that BN is\\nadded after every layer However a very recent paper10 by Hongyi Zhang et al may\\nwell change this the authors show that by using a novel fixedupdate fixup weight\\ninitialization technique they manage to train a very deep neural network 10000 lay\\ners without BN achieving stateoftheart performance on complex image ', 'y deep neural network 10000 lay\\ners without BN achieving stateoftheart performance on complex image classifi\\ncation tasks\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold\\nThis is called Gradient Clipping 11 This technique is most often used in recurrent neu\\n338  Chapter 11 Training Deep Neural Networksral networks as Batch Normalization is tricky to use in RNNs as we will see in \\nFor other types of networks BN is usually sufficient\\nIn Keras implementing Gradient Clipping is just a matter of setting the clipvalue  or\\nclipnorm  argument when creating an optimizer For example\\noptimizer   kerasoptimizers SGDclipvalue 10\\nmodelcompilelossmse optimizer optimizer \\nThis will clip every component of the gradient vector to a value between 10 and 10\\nThis means that all the partial derivatives of the loss with regards to each and every\\ntrainable parameter will be clip', ' the partial derivatives of the loss with regards to each and every\\ntrainable parameter will be clipped between 10 and 10 The threshold is a hyper\\nparameter you can tune Note that it may change the orientation of the gradient vec\\ntor for example if the original gradient vector is 09 1000 it points mostly in the\\ndirection of the second axis but once you clip it by value you get 09 10 which\\npoints roughly in the diagonal between the two axes In practice however this\\napproach works well If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector you should clip by norm by setting clipnorm\\ninstead of clipvalue  This will clip the whole gradient if its 2 norm is greater than\\nthe threshold you picked For example if you set clipnorm10  then the vector 09\\n1000 will be clipped to 000899964 09999595 preserving its orientation but\\nalmost eliminating the first component If you observe that the gradients explode\\nduring training you can track the size of the gra', 'omponent If you observe that the gradients explode\\nduring training you can track the size of the gradients using TensorBoard you may\\nwant to try both clipping by value and clipping by norm with different threshold\\nand see which option performs best on the validation set\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch instead you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle we will discuss how to find them in Chapter 14 \\nthen just reuse the lower layers of this network this is called transfer learning  It will\\nnot only speed up training considerably but will also require much less training data\\nFor example suppose that you have access to a DNN that was trained to classify pic\\ntures into 100 different categories including animals plants vehicles and everyday\\nobjects Y ou now want to train a DNN to classify specific types of vehicles These\\ntasks are very similar ev', ' Y ou now want to train a DNN to classify specific types of vehicles These\\ntasks are very similar even partly overlapping so you should try to reuse parts of the\\nfirst network see Figure 114 \\nReusing Pretrained Layers  339Figure 114 Reusing pretrained layers\\nIf the input pictures of your new task dont have the same size as\\nthe ones used in the original task you will usually have to add a\\npreprocessing step to resize them to the size expected by the origi\\nnal model More generally transfer learning will work best when\\nthe inputs have similar lowlevel features\\nThe output layer of the original model should usually be replaced since it is most\\nlikely not useful at all for the new task and it may not even have the right number of\\noutputs for the new task\\nSimilarly the upper hidden layers of the original model are less likely to be as useful\\nas the lower layers since the highlevel features that are most useful for the new task\\nmay differ significantly from the ones that were most useful for t', ' are most useful for the new task\\nmay differ significantly from the ones that were most useful for the original task Y ou\\nwant to find the right number of layers to reuse\\nThe more similar the tasks are the more layers you want to reuse\\nstarting with the lower layers For very similar tasks you can try\\nkeeping all the hidden layers and just replace the output layer\\nTry freezing all the reused layers first ie make their weights nontrainable so gradi\\nent descent wont modify them then train your model and see how it performs\\nThen try unfreezing one or two of the top hidden layers to let backpropagation tweak\\nthem and see if performance improves The more training data you have the more\\n340  Chapter 11 Training Deep Neural Networkslayers you can unfreeze It is also useful to reduce the learning rate when you unfreeze\\nreused layers this will avoid wrecking their finetuned weights\\nIf you still cannot get good performance and you have little training data try drop\\nping the top hidden layers and ', 'nnot get good performance and you have little training data try drop\\nping the top hidden layers and freeze all remaining hidden layers again Y ou can\\niterate until you find the right number of layers to reuse If you have plenty of train\\ning data you may try replacing the top hidden layers instead of dropping them and\\neven add more hidden layers\\nTransfer Learning With Keras\\nLets look at an example Suppose the fashion MNIST dataset only contained 8 classes\\nfor example all classes except for sandals and shirts Someone built and trained a\\nKeras model on that set and got reasonably good performance 90 accuracy Lets\\ncall this model A Y ou now want to tackle a different task you have images of sandals\\nand shirts and you want to train a binary classifier positiveshirts negativesan\\ndals However your dataset is quite small you only have 200 labeled images When\\nyou train a new model for this task lets call it model B with the same architecture\\nas model A it performs reasonably well 972 accuracy b', 'ets call it model B with the same architecture\\nas model A it performs reasonably well 972 accuracy but since its a much easier\\ntask there are just 2 classes you were hoping for more While drinking your morn\\ning coffee you realize that your task is quite similar to task A so perhaps transfer\\nlearning can help Lets find out\\nFirst you need to load model A and create a new model based on the model A s lay\\ners Lets reuse all layers except for the output layer\\nmodelA  kerasmodelsloadmodel mymodelAh5 \\nmodelBonA   kerasmodelsSequential modelAlayers1\\nmodelBonA addkeraslayersDense1 activation sigmoid \\nNote that modelA  and modelBonA  now share some layers When you train\\nmodelBonA  it will also affect modelA  If you want to avoid that you need to clone\\nmodelA  before you reuse its layers To do this you must clone model A s architecture\\nthen copy its weights since clonemodel  does not clone the weights\\nmodelAclone   kerasmodelsclonemodel modelA\\nmodelAclone setweights modelAgetweights \\nNow we could', 'hts\\nmodelAclone   kerasmodelsclonemodel modelA\\nmodelAclone setweights modelAgetweights \\nNow we could just train modelBonA  for task B but since the new output layer was\\ninitialized randomly it will make large errors at least during the first few epochs so\\nthere will be large error gradients that may wreck the reused weights To avoid this\\none approach is to freeze the reused layers during the first few epochs giving the new\\nlayer some time to learn reasonable weights To do this simply set every layers train\\nable  attribute to False  and compile the model\\nfor layer in modelBonA layers1\\n    layertrainable   False\\nReusing Pretrained Layers  341modelBonA compilelossbinarycrossentropy  optimizer sgd\\n                     metricsaccuracy \\nY ou must always compile your model after you freeze or unfreeze\\nlayers\\nNext we can train the model for a few epochs then unfreeze the reused layers which\\nrequires compiling the model again and continue training to finetune the reused\\nlayers for task B After ', 'ires compiling the model again and continue training to finetune the reused\\nlayers for task B After unfreezing the reused layers it is usually a good idea to reduce\\nthe learning rate once again to avoid damaging the reused weights\\nhistory  modelBonA fitXtrainB  ytrainB  epochs4\\n                           validationdata XvalidB  yvalidB \\nfor layer in modelBonA layers1\\n    layertrainable   True\\noptimizer   kerasoptimizers SGDlr1e4  the default lr is 1e3\\nmodelBonA compilelossbinarycrossentropy  optimizer optimizer \\n                     metricsaccuracy \\nhistory  modelBonA fitXtrainB  ytrainB  epochs16\\n                           validationdata XvalidB  yvalidB \\nSo whats the final verdict Well this models test accuracy is 9925 which means\\nthat transfer learning reduced the error rate from 28 down to almost 07 Thats a\\nfactor of 4\\n modelBonA evaluate XtestB  ytestB \\n006887910133600235 09925\\nAre you convinced Well you shouldnt be I cheated  I tried many configurations\\nuntil I found one that dem', 'you convinced Well you shouldnt be I cheated  I tried many configurations\\nuntil I found one that demonstrated a strong improvement If you try to change the\\nclasses or the random seed you will see that the improvement generally drops or\\neven vanishes or reverses What I did is called torturing the data until it confesses \\nWhen a paper just looks too positive you should be suspicious perhaps the flashy\\nnew technique does not help much in fact it may even degrade performance but\\nthe authors tried many variants and reported only the best results which may be due\\nto shear luck without mentioning how many failures they encountered on the way\\nMost of the time this is not malicious at all but it is part of the reason why so many\\nresults in Science can never be reproduced\\nSo why did I cheat Well it turns out that transfer learning does not work very well\\nwith small dense networks it works best with deep convolutional neural networks so\\nwe will revisit transfer learning in Chapter 14  using the s', 'h deep convolutional neural networks so\\nwe will revisit transfer learning in Chapter 14  using the same techniques and this\\ntime there will be no cheating I promise\\n342  Chapter 11 Training Deep Neural NetworksUnsupervised Pretraining\\nSuppose you want to tackle a complex task for which you dont have much labeled\\ntraining data but unfortunately you cannot find a model trained on a similar task\\nDont lose all hope First you should of course try to gather more labeled training\\ndata but if this is too hard or too expensive you may still be able to perform unsuper\\nvised pretraining  see Figure 115  It is often rather cheap to gather unlabeled train\\ning examples but quite expensive to label them If you can gather plenty of unlabeled\\ntraining data you can try to train the layers one by one starting with the lowest layer\\nand then going up using an unsupervised feature detector algorithm such as Restric\\nted Boltzmann Machines  RBMs see  or autoencoders see  Each layer is\\ntrained on the output of', 'estric\\nted Boltzmann Machines  RBMs see  or autoencoders see  Each layer is\\ntrained on the output of the previously trained layers all layers except the one being\\ntrained are frozen Once all layers have been trained this way you can add the output\\nlayer for your task and finetune the final network using supervised learning ie\\nwith the labeled training examples At this point you can unfreeze all the pretrained\\nlayers or just some of the upper ones\\nFigure 115 Unsupervised pretraining\\nThis is a rather long and tedious process but it often works well in fact it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning Until 2010 unsuper\\nvised pretraining typically using RBMs was the norm for deep nets and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com\\nReusing Pretrained Layers  343mon to train DNNs purely using supervised learning However unsupervised pre\\nt', ' Pretrained Layers  343mon to train DNNs purely using supervised learning However unsupervised pre\\ntraining today typically using autoencoders rather than RBMs is still a good option\\nwhen you have a complex task to solve no similar model you can reuse and little\\nlabeled training data but plenty of unlabeled training data\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data then reuse the lower layers of that network for your actual task The\\nfirst neural networks lower layers will learn feature detectors that will likely be reusa\\nble by the second neural network\\nFor example if you want to build a system to recognize faces you may only have a\\nfew pictures of each individualclearly not enough to train a good classifier Gather\\ning hundreds of pictures of each person would not be practical However you could\\ngather a lot of pictur', ' hundreds of pictures of each person would not be practical However you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person Such a net\\nwork would learn good feature detectors for faces so reusing its lower layers would\\nallow you to train a good face classifier using little training data\\nFor natural language processing  NLP applications you can easily download millions\\nof text documents and automatically generate labeled data from it For example you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are eg it should predict that the missing word in the sentence What \\nyou saying is probably are or were If you can train a model to reach good per\\nformance on this task then it will already know quite a lot about language and you\\ncan certainly reuse it for your actual task and finetune it on your labeled data we\\nwill discuss more pretraining task', 'e it for your actual task and finetune it on your labeled data we\\nwill discuss more pretraining tasks in \\nSelfsupervised learning  is when you automatically generate the\\nlabels from the data itself then you train a model on the resulting\\nlabeled dataset using supervised learning techniques Since this\\napproach requires no human labeling whatsoever it is best classi\\nfied as a form of unsupervised learning\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow So far we have seen\\nfour ways to speed up training and reach a better solution applying a good initiali\\nzation strategy for the connection weights using a good activation function using\\nBatch Normalization and reusing parts of a pretrained network possibly built on an\\nauxiliary task or using unsupervised learning Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer In this section\\n344  Chapter 11 Training Deep Neural Networks12Some methods of speeding up ', 'ptimizer In this section\\n344  Chapter 11 Training Deep Neural Networks12Some methods of speeding up the convergence of iteration methods  B Polyak 1964we will present the most popular ones Momentum optimization Nesterov Acceler\\nated Gradient AdaGrad RMSProp and finally Adam and Nadam optimization\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface it will start\\nout slowly but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity if there is some friction or air resistance This is the very simple idea behind\\nMomentum optimization  proposed by Boris Polyak in 1964 12 In contrast regular\\nGradient Descent will simply take small regular steps down the slope so it will take\\nmuch more time to reach the bottom\\nRecall that Gradient Descent simply updates the weights  by directly subtracting the\\ngradient of the cost function J with regards to the weights  J multiplied by\\nthe learning rate  The equation is     J It does not care ab', 'regards to the weights  J multiplied by\\nthe learning rate  The equation is     J It does not care about what the\\nearlier gradients were If the local gradient is tiny it goes very slowly\\nMomentum optimization cares a great deal about what previous gradients were at\\neach iteration it subtracts the local gradient from the momentum vector  m multi\\nplied by the learning rate  and it updates the weights by simply adding this\\nmomentum vector see Equation 114  In other words the gradient is used for accel\\neration not for speed To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large the algorithm introduces a new hyperparameter\\n simply called the momentum  which must be set between 0 high friction and 1\\nno friction A typical momentum value is 09\\nEquation 114 Momentum algorithm\\n1  m mJ\\n2   m\\nY ou can easily verify that if the gradient remains constant the terminal velocity ie\\nthe maximum size of the weight updates is equal to that gradient multiplied by the\\nle', 'al velocity ie\\nthe maximum size of the weight updates is equal to that gradient multiplied by the\\nlearning rate  multiplied by 1\\n1  ignoring the sign For example if   09 then the\\nterminal velocity is equal to 10 times the gradient times the learning rate so Momen\\ntum optimization ends up going 10 times faster than Gradient Descent This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent\\nIn particular we saw in Chapter 4  that when the inputs have very different scales the \\ncost function will look like an elongated bowl see Figure 47  Gradient Descent goes\\ndown the steep slope quite fast but then it takes a very long time to go down the val\\nFaster Optimizers  34513 A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O1k2  Yurii\\nNesterov 1983\\nley In contrast Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom the optimum In deep neural networks that dont use\\nBatch Normalization th', 'until it reaches the bottom the optimum In deep neural networks that dont use\\nBatch Normalization the upper layers will often end up having inputs with very dif\\nferent scales so using Momentum optimization helps a lot It can also help roll past\\nlocal optima\\nDue to the momentum the optimizer may overshoot a bit then\\ncome back overshoot again and oscillate like this many times\\nbefore stabilizing at the minimum This is one of the reasons why it\\nis good to have a bit of friction in the system it gets rid of these\\noscillations and thus speeds up convergence\\nImplementing Momentum optimization in Keras is a nobrainer just use the SGD\\noptimizer and set its momentum  hyperparameter then lie back and profit\\noptimizer   kerasoptimizers SGDlr0001 momentum 09\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara\\nmeter to tune However the momentum value of 09 usually works well in practice\\nand almost always goes faster than regular Gradient Descent\\nNesterov Accelerated Grad', 'll in practice\\nand almost always goes faster than regular Gradient Descent\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization proposed by Yurii Nesterov in 1983 13\\nis almost always faster than vanilla Momentum optimization The idea of Nesterov\\nMomentum optimization  or Nesterov Accelerated Gradient  NAG is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc\\ntion of the momentum see Equation 115  The only difference from vanilla\\nMomentum optimization is that the gradient is measured at   m rather than at \\nEquation 115 Nesterov Accelerated Gradient algorithm\\n1  m mJm\\n2   m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction ie toward the optimum so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi\\nent at the original position as you can see in Figure 116  where 1 represents the\\ngradient of the c', '\\nent at the original position as you can see in Figure 116  where 1 represents the\\ngradient of the cost function measured at the starting point  and 2 represents the\\n346  Chapter 11 Training Deep Neural Networks14 Adaptive Subgradient Methods for Online Learning and Stochastic Optimization  J Duchi et al 2011gradient at the point located at   m As you can see the Nesterov update ends up\\nslightly closer to the optimum After a while these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization More\\nover note that when the momentum pushes the weights across a valley 1 continues\\nto push further across the valley while 2 pushes back toward the bottom of the val\\nley This helps reduce oscillations and thus converges faster\\nNAG will almost always speed up training compared to regular Momentum optimi\\nzation To use it simply set nesterovTrue  when creating the SGD optimizer\\noptimizer   kerasoptimizers SGDlr0001 momentum 09 nesterov True\\nFigure 116 ', 'eating the SGD optimizer\\noptimizer   kerasoptimizers SGDlr0001 momentum 09 nesterov True\\nFigure 116 Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again Gradient Descent starts by quickly going\\ndown the steepest slope then slowly goes down the bottom of the valley It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum\\nThe AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions see Equation 116 \\nEquation 116 AdaGrad algorithm\\n1  s sJJ\\n2   Js\\nFaster Optimizers  347The first step accumulates the square of the gradients into the vector s recall that the\\n symbol represents the elementwise multiplication This vectorized form is equiv\\nalent to computing si  si   J   i2 for each element si of the vector s in other\\nwords each si accumulates the squares of the partial derivative of the cost function\\nwith regards to parameter i If th', 'mulates the squares of the partial derivative of the cost function\\nwith regards to parameter i If the cost function is steep along the ith dimension then\\nsi will get larger and larger at each iteration\\nThe second step is almost identical to Gradient Descent but with one big difference\\nthe gradient vector is scaled down by a factor of  the  symbol represents the\\nelementwise division and  is a smoothing term to avoid division by zero typically\\nset to 1010 This vectorized form is equivalent to computing\\niiJ isi for all parameters i simultaneously\\nIn short this algorithm decays the learning rate but it does so faster for steep dimen\\nsions than for dimensions with gentler slopes This is called an adaptive learning rate  \\nIt helps point the resulting updates more directly toward the global optimum see\\nFigure 117  One additional benefit is that it requires much less tuning of the learn\\ning rate hyperparameter \\nFigure 117 AdaGrad versus Gradient Descent\\nAdaGrad often performs well for simple q', ' hyperparameter \\nFigure 117 AdaGrad versus Gradient Descent\\nAdaGrad often performs well for simple quadratic problems but unfortunately it\\noften stops too early when training neural networks The learning rate gets scaled\\ndown so much that the algorithm ends up stopping entirely before reaching the\\nglobal optimum So even though Keras has an Adagrad  optimizer you should not use\\nit to train deep neural networks it may be efficient for simpler tasks such as Linear\\nRegression though However understanding Adagrad is helpful to grasp the other\\nadaptive learning rate optimizers\\n348  Chapter 11 Training Deep Neural Networks15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey\\nHinton in his Coursera class on neural networks slides httpshomlinfo57  video httpshomlinfo58 \\nAmusingly since the authors did not write a paper to describe it researchers often cite slide 29 in lecture 6\\nin their papers\\n16 Adam A Method for Stochastic Optimization  D Kingma', 'en cite slide 29 in lecture 6\\nin their papers\\n16 Adam A Method for Stochastic Optimization  D Kingma J Ba 2015\\n17These are estimations of the mean and uncentered variance of the gradients The mean is often called the\\nfirst moment  while the variance is often called the second moment  hence the name of the algorithmRMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum the RMSProp  algorithm15 fixes this by accumulating only the gradi\\nents from the most recent iterations as opposed to all the gradients since the begin\\nning of training It does so by using exponential decay in the first step see Equation\\n117 \\nEquation 117 RMSProp algorithm\\n1  s s1 JJ\\n2   Js\\nThe decay rate  is typically set to 09 Y es it is once again a new hyperparameter but\\nthis default value often works well so you may not need to tune it at all\\nAs you might expect Keras has an RMSProp  optimizer\\noptimizer   kerasoptimizers RMSproplr0001 rho09\\nExcept on very simple problems', 'an RMSProp  optimizer\\noptimizer   kerasoptimizers RMSproplr0001 rho09\\nExcept on very simple problems this optimizer almost always performs much better\\nthan AdaGrad In fact it was the preferred optimization algorithm of many research\\ners until Adam optimization came around\\nAdam and Nadam Optimization\\nAdam 16 which stands for adaptive moment estimation  combines the ideas of Momen\\ntum optimization and RMSProp just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients see Equation\\n118 17\\nFaster Optimizers  349Equation 118 Adam algorithm\\n1  m 1m1 1J\\n2  s 2s1 2JJ\\n3  mm\\n1 1t\\n4  ss\\n1 2t\\n5   m s\\nt represents the iteration number starting at 1\\nIf you just look at steps 1 2 and 5 you will notice Adams close similarity to both\\nMomentum optimization and RMSProp The only difference is that step 1 computes\\nan exponentially decaying average rather than an expo', 'op The only difference is that step 1 computes\\nan exponentially decaying average rather than an exponentially decaying sum but\\nthese are actually equivalent except for a constant factor the decaying average is just\\n1  1 times the decaying sum Steps 3 and 4 are somewhat of a technical detail since\\nm and s are initialized at 0 they will be biased toward 0 at the beginning of training\\nso these two steps will help boost m and s at the beginning of training\\nThe momentum decay hyperparameter 1 is typically initialized to 09 while the scal\\ning decay hyperparameter 2 is often initialized to 0999 As earlier the smoothing\\nterm  is usually initialized to a tiny number such as 107 These are the default values\\nfor the Adam  class to be precise epsilon  defaults to None  which tells Keras to use\\nkerasbackendepsilon  which defaults to 107 you can change it using\\nkerasbackendsetepsilon \\noptimizer   kerasoptimizers Adamlr0001 beta109 beta20999\\nSince Adam is an adaptive learning rate algorithm like AdaG', 'soptimizers Adamlr0001 beta109 beta20999\\nSince Adam is an adaptive learning rate algorithm like AdaGrad and RMSProp it\\nrequires less tuning of the learning rate hyperparameter  Y ou can often use the\\ndefault value   0001 making Adam even easier to use than Gradient Descent\\nIf you are starting to feel overwhelmed by all these different techni\\nques and wondering how to choose the right ones for your task\\ndont worry some practical guidelines are provided at the end of\\nthis chapter\\nFinally two variants of Adam are worth mentioning\\n350  Chapter 11 Training Deep Neural Networks18Incorporating Nesterov Momentum into Adam  Timothy Dozat 2015\\n19The Marginal Value of Adaptive Gradient Methods in Machine Learning  A C Wilson et al 2017\\nAdamax introduced in the same paper as Adam notice that in step 2 of Equation\\n118  Adam accumulates the squares of the gradients in s with a greater weight\\nfor more recent weights In step 5 if we ignore  and steps 3 and 4 which are\\ntechnical details anyway Adam jus', 'recent weights In step 5 if we ignore  and steps 3 and 4 which are\\ntechnical details anyway Adam just scales down the parameter updates by the\\nsquare root of s In short Adam scales down the parameter updates by the 2\\nnorm of the timedecayed gradients recall that the 2 norm is the square root of\\nthe sum of squares Adamax just replaces the 2 norm with the  norm a fancy\\nway of saying the max Specifically it replaces step 2 in Equation 118  with\\n max 2J it drops step 4 and in step 5 it scales down the gradient\\nupdates by a factor of s which is just the max of the timedecayed gradients In\\npractice this can make Adamax more stable than Adam but this really depends\\non the dataset and in general Adam actually performs better So its just one\\nmore optimizer you can try if you experience problems with Adam on some task\\nNadam optimization18 is more important it is simply Adam optimization plus\\nthe Nesterov trick so it will often converge slightly faster than Adam In his\\nreport Timothy Dozat compar', 'esterov trick so it will often converge slightly faster than Adam In his\\nreport Timothy Dozat compares many different optimizers on various tasks and\\nfinds that Nadam generally outperforms Adam but is sometimes outperformed\\nby RMSProp\\nAdaptive optimization methods including RMSProp Adam and\\nNadam optimization are often great converging fast to a good sol\\nution However a 2017 paper19 by Ashia C Wilson et al showed\\nthat they can lead to solutions that generalize poorly on some data\\nsets So when you are disappointed by your models performance\\ntry using plain Nesterov Accelerated Gradient instead your dataset\\nmay just be allergic to adaptive gradients Also check out the latest\\nresearch it is moving fast eg AdaBound\\nAll the optimization techniques discussed so far only rely on the firstorder  partial\\nderivatives  Jacobians  The optimization literature contains amazing algorithms\\nbased on the secondorder partial derivatives  the Hessians  which are the partial\\nderivatives of the Jacobians Un', 'secondorder partial derivatives  the Hessians  which are the partial\\nderivatives of the Jacobians Unfortunately these algorithms are very hard to apply\\nto deep neural networks because there are n2 Hessians per output where n is the\\nnumber of parameters as opposed to just n Jacobians per output Since DNNs typi\\ncally have tens of thousands of parameters the secondorder optimization algorithms\\nFaster Optimizers  35120PrimalDual Subgradient Methods for Convex Problems  Yurii Nesterov 2005\\n21 Ad Click Prediction a View from the Trenches  H McMahan et al 2013often dont even fit in memory and even when they do computing the Hessians is \\njust too slow\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models meaning that\\nmost parameters will be nonzero If you need a blazingly fast model at runtime or if\\nyou need it to take up less memory you may prefer to end up with a sparse model\\ninstead\\nOne trivial way to achieve this is to train the model as usual then get ', 'with a sparse model\\ninstead\\nOne trivial way to achieve this is to train the model as usual then get rid of the tiny\\nweights set them to 0 However this will typically not lead to a very sparse model\\nand it may degrade the models performance\\nA better option is to apply strong 1 regularization during training as it pushes the\\noptimizer to zero out as many weights as it can as discussed in Chapter 4  about Lasso\\nRegression\\nHowever in some cases these techniques may remain insufficient One last option is\\nto apply Dual Averaging  often called Follow The Regularized Leader  FTRL a techni\\nque proposed by Yurii Nesterov 20 When used with 1 regularization this technique\\noften leads to very sparse models Keras implements a variant of FTRL called FTRL\\nProximal21 in the FTRL  optimizer\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky If you set it way too high training may\\nactually diverge as we discussed in Chapter 4  If you set it too low training will\\neventually converge to th', ' diverge as we discussed in Chapter 4  If you set it too low training will\\neventually converge to the optimum but it will take a very long time If you set it\\nslightly too high it will make progress very quickly at first but it will end up dancing\\naround the optimum never really settling down If you have a limited computing\\nbudget you may have to interrupt training before it has converged properly yielding\\na suboptimal solution see Figure 118 \\n352  Chapter 11 Training Deep Neural NetworksFigure 118 Learning curves for various learning rates \\nAs we discussed in Chapter 10  one approach is to start with a large learning rate and\\ndivide it by 3 until the training algorithm stops diverging Y ou will not be too far\\nfrom the optimal learning rate which will learn quickly and converge to good solu\\ntion\\nHowever you can do better than a constant learning rate if you start with a high\\nlearning rate and then reduce it once it stops making fast progress you can reach a\\ngood solution faster than wit', ' and then reduce it once it stops making fast progress you can reach a\\ngood solution faster than with the optimal constant learning rate There are many dif\\nferent strategies to reduce the learning rate during training These strategies are called\\nlearning schedules  we briefly introduced this concept in Chapter 4  the most com\\nmon of which are\\nPower scheduling\\nSet the learning rate to a function of the iteration number t t  0  1  tkc\\nThe initial learning rate 0 the power c typically set to 1 and the steps s are\\nhyperparameters The learning rate drops at each step and after s steps it is down\\nto 0  2 After s more steps it is down to 0  3 Then down to 0  4 then 0  5\\nand so on As you can see this schedule first drops quickly then more and more\\nslowly Of course this requires tuning 0 s and possibly c\\nExponential scheduling\\nSet the learning rate to t  0 01ts The learning rate will gradually drop by a\\nfactor of 10 every s steps While power scheduling reduces the learning rate more\\nand more sl', 'op by a\\nfactor of 10 every s steps While power scheduling reduces the learning rate more\\nand more slowly exponential scheduling keeps slashing it by a factor of 10 every\\ns steps\\nPiecewise constant scheduling\\nUse a constant learning rate for a number of epochs eg 0  01 for 5 epochs\\nthen a smaller learning rate for another number of epochs eg 1  0001 for 50\\nepochs and so on Although this solution can work very well it requires fid\\nFaster Optimizers  35322 An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition  A Senior et al\\n2013dling around to figure out the right sequence of learning rates and how long to\\nuse each of them\\nPerformance scheduling\\nMeasure the validation error every N steps just like for early stopping and\\nreduce the learning rate by a factor of  when the error stops dropping\\nA 2013 paper22 by Andrew Senior et al compared the performance of some of the\\nmost popular learning schedules when training deep neural networks for speech rec\\nognition us', 'of the\\nmost popular learning schedules when training deep neural networks for speech rec\\nognition using Momentum optimization The authors concluded that in this setting\\nboth performance scheduling and exponential scheduling performed well They\\nfavored exponential scheduling because it was easy to tune and it converged slightly\\nfaster to the optimal solution they also mentioned that it was easier to implement\\nthan performance scheduling but in Keras both options are easy\\nImplementing power scheduling in Keras is the easiest option just set the decay\\nhyperparameter when creating an optimizer The decay  is the inverse of s the num\\nber of steps it takes to divide the learning rate by one more unit and Keras assumes\\nthat c is equal to 1 For example\\noptimizer   kerasoptimizers SGDlr001 decay1e4\\nExponential scheduling and piecewise scheduling are quite simple too Y ou first need\\nto define a function that takes the current epoch and returns the learning rate For\\nexample lets implement exponent', 'ction that takes the current epoch and returns the learning rate For\\nexample lets implement exponential scheduling\\ndef exponentialdecayfn epoch\\n    return 001  01epoch  20\\nIf you do not want to hardcode 0 and s you can create a function that returns a\\nconfigured function\\ndef exponentialdecay lr0 s\\n    def exponentialdecayfn epoch\\n        return lr0  01epoch  s\\n    return exponentialdecayfn\\nexponentialdecayfn   exponentialdecay lr0001 s20\\nNext just create a LearningRateScheduler  callback giving it the schedule function\\nand pass this callback to the fit  method\\nlrscheduler   kerascallbacks LearningRateScheduler exponentialdecayfn \\nhistory  modelfitXtrainscaled  ytrain  callbacks lrscheduler \\n354  Chapter 11 Training Deep Neural NetworksThe LearningRateScheduler  will update the optimizers learningrate  attribute at\\nthe beginning of each epoch Updating the learning rate just once per epoch is usually\\nenough but if you want it to be updated more often for example at every step you\\nneed to', ' is usually\\nenough but if you want it to be updated more often for example at every step you\\nneed to write your own callback see the notebook for an example This can make\\nsense if there are many steps per epoch\\nThe schedule function can optionally take the current learning rate as a second argu\\nment For example the following schedule function just multiplies the previous\\nlearning rate by 01120 which results in the same exponential decay except the decay\\nnow starts at the beginning of epoch 0 instead of 1 This implementation relies on\\nthe optimizers initial learning rate contrary to the previous implementation so\\nmake sure to set it appropriately\\ndef exponentialdecayfn epoch lr\\n    return lr  011  20\\nWhen you save a model the optimizer and its learning rate get saved along with it\\nThis means that with this new schedule function you could just load a trained model\\nand continue training where it left off no problem However things are not so simple\\nif your schedule function uses the epoch ', 're it left off no problem However things are not so simple\\nif your schedule function uses the epoch  argument indeed the epoch does not get\\nsaved and it gets reset to 0 every time you call the fit  method This could lead to a\\nvery large learning rate when you continue training a model where it left off which\\nwould likely damage your models weights One solution is to manually set the fit\\nmethods initialepoch  argument so the epoch  starts at the right value\\nFor piecewise constant scheduling you can use a schedule function like the following\\none as earlier you can define a more general function if you want see the notebook\\nfor an example then create a LearningRateScheduler  callback with this function\\nand pass it to the fit  method just like we did for exponential scheduling\\ndef piecewiseconstantfn epoch\\n    if epoch  5\\n        return 001\\n    elif epoch  15\\n        return 0005\\n    else\\n        return 0001\\nFor performance scheduling simply use the ReduceLROnPlateau  callback For exam\\nple ', '     return 0001\\nFor performance scheduling simply use the ReduceLROnPlateau  callback For exam\\nple if you pass the following callback to the fit  method it will multiply the learn\\ning rate by 05 whenever the best validation loss does not improve for 5 consecutive\\nepochs other options are available please check the documentation for more\\ndetails\\nlrscheduler   kerascallbacks ReduceLROnPlateau factor05 patience 5\\nLastly tfkeras offers an alternative way to implement learning rate scheduling just\\ndefine the learning rate using one of the schedules available in kerasoptimiz\\nFaster Optimizers  355ersschedules  then pass this learning rate to any optimizer This approach updates\\nthe learning rate at each step rather than at each epoch For example here is how to\\nimplement the same exponential schedule as earlier\\ns  20  lenXtrain  32  number of steps in 20 epochs batch size  32\\nlearningrate   kerasoptimizers schedules ExponentialDecay 001 s 01\\noptimizer   kerasoptimizers SGDlearningrate \\nThis i', 'asoptimizers schedules ExponentialDecay 001 s 01\\noptimizer   kerasoptimizers SGDlearningrate \\nThis is nice and simple plus when you save the model the learning rate and its\\nschedule including its state get saved as well However this approach is not part of\\nthe Keras API it is specific to tfkeras\\nTo sum up exponential decay or performance scheduling can considerably speed up\\nconvergence so give them a try\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk\\nJohn von Neumann cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo Deep neural networks typi\\ncally have tens of thousands of parameters sometimes even millions With so many\\nparameters the network has an incredible amount of freedom and can fit a huge vari\\nety of complex datasets But this great flexibility also means that it is prone to overfit\\nting the training set We need regularization\\nWe already implemented one', ' that it is prone to overfit\\nting the training set We need regularization\\nWe already implemented one of the best regularization techniques in Chapter 10 \\nearly stopping Moreover even though Batch Normalization was designed to solve\\nthe vanishingexploding gradients problems is also acts like a pretty good regularizer\\nIn this section we will present other popular regularization techniques for neural net\\nworks 1 and 2 regularization dropout and maxnorm regularization\\n1 and 2 Regularization\\nJust like you did in Chapter 4  for simple linear models you can use 1 and 2 regulari\\nzation to constrain a neural networks connection weights but typically not its bia\\nses Here is how to apply 2 regularization to a Keras layers connection weights\\nusing a regularization factor of 001\\nlayer  keraslayersDense100 activation elu\\n                           kernelinitializer henormal \\n                           kernelregularizer kerasregularizers l2001\\nThe l2  function returns a regularizer that will be calle', ' kernelregularizer kerasregularizers l2001\\nThe l2  function returns a regularizer that will be called to compute the regulariza\\ntion loss at each step during training This regularization loss is then added to the\\nfinal loss As you might expect you can just use kerasregularizersl1  if you\\n356  Chapter 11 Training Deep Neural Networks23Improving neural networks by preventing coadaptation of feature detectors  G Hinton et al 2012\\n24Dropout A Simple Way to Prevent Neural Networks from Overfitting  N Srivastava et al 2014want 1 regularization and if you want both 1 and 2 regularization use kerasregu\\nlarizersl1l2  specifying both regularization factors\\nSince you will typically want to apply the same regularizer to all layers in your net\\nwork as well as the same activation function and the same initialization strategy in all\\nhidden layers you may find yourself repeating the same arguments over and over\\nThis makes it ugly and errorprone To avoid this you can try refactoring your code\\nto use lo', 'and over\\nThis makes it ugly and errorprone To avoid this you can try refactoring your code\\nto use loops Another option is to use Pythons functoolspartial  function it lets\\nyou create a thin wrapper for any callable with some default argument values For\\nexample\\nfrom functools  import partial\\nRegularizedDense   partialkeraslayersDense\\n                           activation elu\\n                           kernelinitializer henormal \\n                           kernelregularizer kerasregularizers l2001\\nmodel  kerasmodelsSequential \\n    keraslayersFlatteninputshape 28 28\\n    RegularizedDense 300\\n    RegularizedDense 100\\n    RegularizedDense 10 activation softmax \\n                     kernelinitializer glorotuniform \\n\\nDropout\\nDropout  is one of the most popular regularization techniques for deep neural net\\nworks It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al and it has proven to be highly successful even the stateof\\ntheart neural networ', 'ish Srivastava et al and it has proven to be highly successful even the stateof\\ntheart neural networks got a 12 accuracy boost simply by adding dropout This\\nmay not sound like a lot but when a model already has 95 accuracy getting a 2\\naccuracy boost means dropping the error rate by almost 40 going from 5 error to\\nroughly 3\\nIt is a fairly simple algorithm at every training step every neuron including the\\ninput neurons but always excluding the output neurons has a probability p of being\\ntemporarily dropped out  meaning it will be entirely ignored during this training\\nstep but it may be active during the next step see Figure 119  The hyperparameter\\np is called the dropout rate  and it is typically set to 50 After training neurons dont\\nget dropped anymore And thats all except for a technical detail we will discuss\\nmomentarily\\nAvoiding Overfitting  Through Regularization  357Figure 119 Dropout regularization\\nIt is quite surprising at first that this rather brutal technique works at all Woul', 't regularization\\nIt is quite surprising at first that this rather brutal technique works at all Would a\\ncompany perform better if its employees were told to toss a coin every morning to\\ndecide whether or not to go to work Well who knows perhaps it would The com\\npany would obviously be forced to adapt its organization it could not rely on any sin\\ngle person to fill in the coffee machine or perform any other critical tasks so this\\nexpertise would have to be spread across several people Employees would have to\\nlearn to cooperate with many of their coworkers not just a handful of them The\\ncompany would become much more resilient If one person quit it wouldnt make\\nmuch of a difference Its unclear whether this idea would actually work for compa\\nnies but it certainly does for neural networks Neurons trained with dropout cannot\\ncoadapt with their neighboring neurons they have to be as useful as possible on\\ntheir own They also cannot rely excessively on just a few input neurons they must\\npay at', 'possible on\\ntheir own They also cannot rely excessively on just a few input neurons they must\\npay attention to each of their input neurons They end up being less sensitive to slight\\nchanges in the inputs In the end you get a more robust network that generalizes bet\\nter\\nAnother way to understand the power of dropout is to realize that a unique neural\\nnetwork is generated at each training step Since each neuron can be either present or\\nabsent there is a total of 2N possible networks where N is the total number of drop\\npable neurons This is such a huge number that it is virtually impossible for the same\\nneural network to be sampled twice Once you have run a 10000 training steps you\\nhave essentially trained 10000 different neural networks each with just one training\\ninstance These neural networks are obviously not independent since they share\\nmany of their weights but they are nevertheless all different The resulting neural\\nnetwork can be seen as an averaging ensemble of all these smaller ', 'll different The resulting neural\\nnetwork can be seen as an averaging ensemble of all these smaller neural networks\\nThere is one small but important technical detail Suppose p  50 in which case\\nduring testing a neuron will be connected to twice as many input neurons as it was\\non average during training To compensate for this fact we need to multiply each\\n358  Chapter 11 Training Deep Neural Networks25This is specific to tfkeras so you may prefer to use kerasbackendsetlearningphase1  before calling\\nthe fit  method and set it back to 0 right after\\nneurons input connection weights by 05 after training If we dont each neuron will\\nget a total input signal roughly twice as large as what the network was trained on and\\nit is unlikely to perform well More generally we need to multiply each input connec\\ntion weight by the keep probability  1  p after training Alternatively we can divide\\neach neurons output by the keep probability during training these alternatives are\\nnot perfectly equivalent bu', 'ns output by the keep probability during training these alternatives are\\nnot perfectly equivalent but they work equally well\\nTo implement dropout using Keras you can use the keraslayersDropout  layer\\nDuring training it randomly drops some inputs setting them to 0 and divides the\\nremaining inputs by the keep probability After training it does nothing at all it just\\npasses the inputs to the next layer For example the following code applies dropout\\nregularization before every Dense  layer using a dropout rate of 02\\nmodel  kerasmodelsSequential \\n    keraslayersFlatteninputshape 28 28\\n    keraslayersDropoutrate02\\n    keraslayersDense300 activation elu kernelinitializer henormal \\n    keraslayersDropoutrate02\\n    keraslayersDense100 activation elu kernelinitializer henormal \\n    keraslayersDropoutrate02\\n    keraslayersDense10 activation softmax \\n\\nSince dropout is only active during training the training loss is\\npenalized compared to the validation loss so comparing the two\\ncan be misleading I', ' training loss is\\npenalized compared to the validation loss so comparing the two\\ncan be misleading In particular a model may be overfitting the\\ntraining set and yet have similar training and validation losses So\\nmake sure to evaluate the training loss without dropout eg after\\ntraining Alternatively you can call the fit  method inside a\\nwith kerasbackendlearningphasescope1  block this will\\nforce dropout to be active during both training and validation25\\nIf you observe that the model is overfitting you can increase the dropout rate Con\\nversely you should try decreasing the dropout rate if the model underfits the training\\nset It can also help to increase the dropout rate for large layers and reduce it for\\nsmall ones Moreover many stateoftheart architectures only use dropout after the\\nlast hidden layer so you may want to try this if full dropout is too strong\\nDropout does tend to significantly slow down convergence but it usually results in a\\nmuch better model when tuned properly So it is ', 'ly slow down convergence but it usually results in a\\nmuch better model when tuned properly So it is generally well worth the extra time\\nand effort\\nAvoiding Overfitting  Through Regularization  35926Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning  Y  Gal and Z\\nGhahramani 2016\\n27Specifically they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process \\nIf you want to regularize a selfnormalizing network based on the\\nSELU activation function as discussed earlier you should use\\nAlphaDropout  this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs it was introduced in the same\\npaper as SELU as regular dropout would break selfnormalization\\nMonteCarlo MC Dropout\\nIn 2016 a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout\\nFirst the paper establishes a profound connection betwee', 'mani added more good reasons to\\nuse dropout\\nFirst the paper establishes a profound connection between dropout networks\\nie neural networks containing a dropout layer before every weight layer and\\napproximate Bayesian inference27 giving dropout a solid mathematical justifica\\ntion\\nSecond they introduce a powerful technique called MC Dropout  which can\\nboost the performance of any trained dropout model without having to retrain it\\nor even modify it at all\\nMoreover MC Dropout also provides a much better measure of the models\\nuncertainty\\nFinally it is also amazingly simple to implement If this all sounds like a one\\nweird trick advertisement then take a look at the following code It is the full\\nimplementation of MC Dropout  boosting the dropout model we trained earlier\\nwithout retraining it\\nwith kerasbackendlearningphasescope 1  force training mode  dropout on\\n    yprobas   npstackmodelpredictXtestscaled \\n                         for sample in range100\\nyproba  yprobas meanaxis0\\nWe first force', 'testscaled \\n                         for sample in range100\\nyproba  yprobas meanaxis0\\nWe first force training mode on using a learningphasescope1  context This\\nturns dropout on within the with  block Then we make 100 predictions over the test\\nset and we stack them Since dropout is on all predictions will be different Recall\\nthat predict  returns a matrix with one row per instance and one column per class\\nSince there are 10000 instances in the test set and 10 classes this is a matrix of shape\\n10000 10 We stack 100 such matrices so yprobas  is an array of shape 100 10000\\n10 Once we average over the first dimension  axis0  we get yproba  an array of\\nshape 10000 10 like we would get with a single prediction Thats all Averaging\\n360  Chapter 11 Training Deep Neural Networksover multiple predictions with dropout on gives us a Monte Carlo estimate that is\\ngenerally more reliable than the result of a single prediction with dropout off For\\nexample lets look at the models prediction for the first', 'of a single prediction with dropout off For\\nexample lets look at the models prediction for the first instance in the test set with\\ndropout off\\n nproundmodelpredictXtestscaled 1 2\\narray0   0   0   0   0   0   0   001 0   099\\n      dtypefloat32\\nThe model seems almost certain that this image belongs to class 9 ankle boot\\nShould you trust it Is there really so little room for doubt Compare this with the\\npredictions made when dropout is activated\\n nproundyprobas  1 2\\narray0   0   0   0   0   014 0   017 0   068\\n       0   0   0   0   0   016 0   02  0   064\\n       0   0   0   0   0   002 0   001 0   097\\n       \\nThis tells a very different story apparently when we activate dropout the model is\\nnot sure anymore It still seems to prefer class 9 but sometimes it hesitates with\\nclasses 5 sandal and 7 sneaker which makes sense given theyre all footwear Once\\nwe average over the first dimension we get the following MC dropout predictions\\n nproundyproba1 2\\narray0   0   0   0   0   022 0   016 0   06', 't the following MC dropout predictions\\n nproundyproba1 2\\narray0   0   0   0   0   022 0   016 0   062\\n      dtypefloat32\\nThe model still thinks this image belongs to class 9 but only with a 62 confidence\\nwhich seems much more reasonable than 99 Plus its useful to know exactly which\\nother classes it thinks are likely And you can also take a look at the standard devia\\ntion of the probability estimates \\n ystd  yprobas stdaxis0\\n nproundystd1 2\\narray0   0   0   0   0   028 0   021 002 032\\n      dtypefloat32\\nApparently theres quite a lot of variance in the probability estimates if you were\\nbuilding a risksensitive system eg a medical or financial system you should prob\\nably treat such an uncertain prediction with extreme caution Y ou definitely would\\nnot treat it like a 99 confident prediction Moreover the models accuracy got a\\nsmall boost from 868 to 869\\n accuracy   npsumypred  ytest  lenytest\\n accuracy\\n08694\\nAvoiding Overfitting  Through Regularization  361The number of Monte Carlo samples', 't\\n accuracy\\n08694\\nAvoiding Overfitting  Through Regularization  361The number of Monte Carlo samples you use 100 in this example\\nis a hyperparameter you can tweak The higher it is the more accu\\nrate the predictions and their uncertainty estimates will be How\\never it you double it inference time will also be doubled\\nMoreover above a certain number of samples you will notice little\\nimprovement So your job is to find the right tradeoff between\\nlatency and accuracy depending on your application\\nIf your model contains other layers that behave in a special way during training such\\nas Batch Normalization layers then you should not force training mode like we just\\ndid Instead you should replace the Dropout  layers with the following MCDropout\\nclass\\nclass MCDropout keraslayersDropout\\n    def callself inputs\\n        return supercallinputs training True\\nWe just sublass the Dropout  layer and override the call  method to force its train\\ning argument to True  see Chapter 12  Similarly you could def', 'de the call  method to force its train\\ning argument to True  see Chapter 12  Similarly you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout  instead If you are creating a model from\\nscratch its just a matter of using MCDropout  rather than Dropout  But if you have a\\nmodel that was already trained using Dropout  you need to create a new model iden\\ntical to the existing model except replacing the Dropout  layers with MCDropout  then\\ncopy the existing models weights to your new model\\nIn short MC Dropout is a fantastic technique that boosts dropout models and pro\\nvides better uncertainty estimates And of course since it is just regular dropout dur\\ning training it also acts like a regularizer\\nMaxNorm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmaxnorm regularization  for each neuron it constrains the weights w of the incom\\ning connections such that  w 2  r where r is the maxnorm hyperparameter\\nand   2 is the 2 norm\\nMax', 'om\\ning connections such that  w 2  r where r is the maxnorm hyperparameter\\nand   2 is the 2 norm\\nMaxnorm regularization does not add a regularization loss term to the overall loss\\nfunction Instead it is typically implemented by computing w2 after each training\\nstep and clipping w if needed  w wr\\nw2\\nReducing r increases the amount of regularization and helps reduce overfitting Max\\nnorm regularization can also help alleviate the vanishingexploding gradients prob\\nlems if you are not using Batch Normalization\\n362  Chapter 11 Training Deep Neural NetworksTo implement maxnorm regularization in Keras just set every hidden layers ker\\nnelconstraint  argument to a maxnorm  constraint with the appropriate max\\nvalue for example\\nkeraslayersDense100 activation elu kernelinitializer henormal \\n                   kernelconstraint kerasconstraints maxnorm 1\\nAfter each training iteration the models fit  method will call the object returned\\nby maxnorm  passing it the layers weights and getting clipped wei', 'thod will call the object returned\\nby maxnorm  passing it the layers weights and getting clipped weights in return\\nwhich then replace the layers weights As we will see in Chapter 12  you can define\\nyour own custom constraint function if you ever need to and use it as the ker\\nnelconstraint  Y ou can also constrain the bias terms by setting the biascon\\nstraint  argument\\nThe maxnorm  function has an axis  argument that defaults to 0 A Dense  layer usu\\nally has weights of shape number of inputs number of neurons so using axis0\\nmeans that the max norm constraint will apply independently to each neurons weight\\nvector If you want to use maxnorm with convolutional layers see Chapter 14 \\nmake sure to set the maxnorm  constraints axis  argument appropriately usually\\naxis0 1 2 \\nSummary and Practical Guidelines\\nIn this chapter we have covered a wide range of techniques and you may be wonder\\ning which ones you should use The configuration in Table 112  will work fine in\\nmost cases without requiring', 'h ones you should use The configuration in Table 112  will work fine in\\nmost cases without requiring much hyperparameter tuning\\nTable 112 Default DNN configuration\\nHyperparameter Default value\\nKernel initializer LeCun initialization\\nActivation function SELU\\nNormalization None selfnormalization\\nRegularization Early stopping\\nOptimizer Nadam\\nLearning rate schedule Performance scheduling\\nDont forget to standardize the input features Of course you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem\\nor use unsupervised pretraining if you have a lot of unlabeled data or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task\\nThe default configuration in Table 112  may need to be tweaked\\nSummary and Practical Guidelines  363If your model selfnormalizes\\nIf it overfits the training set then you should add alpha dropout and always\\nuse early stopping as well Do not use other regularization methods or else\\nthe', 'ha dropout and always\\nuse early stopping as well Do not use other regularization methods or else\\nthey would break selfnormalization\\nIf your model cannot selfnormalize eg it is a recurrent net or it contains skip\\nconnections\\nY ou can try using ELU or another activation function instead of SELU it\\nmay perform better Make sure to change the initialization method accord\\ningly eg He init for ELU or ReLU\\nIf it is a deep network you should use Batch Normalization after every hidden\\nlayer If it overfits the training set you can also try using maxnorm or 2 reg\\nularization\\nIf you need a sparse model you can use 1 regularization and optionally zero out\\nthe tiny weights after training If you need an even sparser model you can try\\nusing FTRL instead of Nadam optimization along with 1 regularization In any\\ncase this will break selfnormalization so you will need to switch to BN if your\\nmodel is deep\\nIf you need a lowlatency model one that performs lightningfast predictions\\nyou may need to use less la', ' you need a lowlatency model one that performs lightningfast predictions\\nyou may need to use less layers avoid Batch Normalization and possibly replace\\nthe SELU activation function with the leaky ReLU Having a sparse model will\\nalso help Y ou may also want to reduce the float precision from 32bits to 16bit\\nor even 8bits see \\nIf you are building a risksensitive application or inference latency is not very\\nimportant in your application you can use MC Dropout to boost performance\\nand get more reliable probability estimates along with uncertainty estimates\\nWith these guidelines you are now ready to train very deep nets I hope you are now\\nconvinced that you can go a very long way using just Keras However there may\\ncome a time when you need to have even more control for example to write a custom\\nloss function or to tweak the training algorithm For such cases you will need to use\\nTensorFlows lowerlevel API as we will see in the next chapter\\nExercises\\n1Is it okay to initialize all the weights ', 'werlevel API as we will see in the next chapter\\nExercises\\n1Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization\\n2Is it okay to initialize the bias terms to 0\\n3Name three advantages of the SELU activation function over ReLU\\n364  Chapter 11 Training Deep Neural Networks4In which cases would you want to use each of the following activation functions\\nSELU leaky ReLU and its variants ReLU tanh logistic and softmax\\n5What may happen if you set the momentum  hyperparameter too close to 1 eg\\n099999 when using an SGD optimizer\\n6Name three ways you can produce a sparse model\\n7Does dropout slow down training Does it slow down inference ie making\\npredictions on new instances What are about MC dropout\\n8Deep Learning\\naBuild a DNN with five hidden layers of 100 neurons each He initialization\\nand the ELU activation function\\nbUsing Adam optimization and early stopping try training it on MNIST but\\nonly on digits 0 to 4 as we will us', 'dam optimization and early stopping try training it on MNIST but\\nonly on digits 0 to 4 as we will use transfer learning for digits 5 to 9 in the\\nnext exercise Y ou will need a softmax output layer with five neurons and as\\nalways make sure to save checkpoints at regular intervals and save the final\\nmodel so you can reuse it later\\ncTune the hyperparameters using crossvalidation and see what precision you\\ncan achieve\\ndNow try adding Batch Normalization and compare the learning curves is it\\nconverging faster than before Does it produce a better model\\neIs the model overfitting the training set Try adding dropout to every layer\\nand try again Does it help\\n9Transfer learning\\naCreate a new DNN that reuses all the pretrained hidden layers of the previous\\nmodel freezes them and replaces the softmax output layer with a new one\\nbTrain this new DNN on digits 5 to 9 using only 100 images per digit and time\\nhow long it takes Despite this small number of examples can you achieve\\nhigh precision\\ncTry cac', 'time\\nhow long it takes Despite this small number of examples can you achieve\\nhigh precision\\ncTry caching the frozen layers and train the model again how much faster is it\\nnow\\ndTry again reusing just four hidden layers instead of five Can you achieve a\\nhigher precision\\neNow unfreeze the top two hidden layers and continue training can you get\\nthe model to perform even better\\n10Pretraining on an auxiliary task\\naIn this exercise you will build a DNN that compares two MNIST digit images\\nand predicts whether they represent the same digit or not Then you will reuse\\nthe lower layers of this network to train an MNIST classifier using very little\\nExercises  365training data Start by building two DNNs lets call them DNN A and B both\\nsimilar to the one you built earlier but without the output layer each DNN\\nshould have five hidden layers of 100 neurons each He initialization and ELU\\nactivation Next add one more hidden layer with 10 units on top of both\\nDNNs To do this you should use a keraslayersC', 'add one more hidden layer with 10 units on top of both\\nDNNs To do this you should use a keraslayersConcatenate  layer to con\\ncatenate the outputs of both DNNs for each instance then feed the result to\\nthe hidden layer Finally add an output layer with a single neuron using the\\nlogistic activation function\\nbSplit the MNIST training set in two sets split 1 should containing 55000\\nimages and split 2 should contain contain 5000 images Create a function\\nthat generates a training batch where each instance is a pair of MNIST images\\npicked from split 1 Half of the training instances should be pairs of images\\nthat belong to the same class while the other half should be images from dif\\nferent classes For each pair the training label should be 0 if the images are\\nfrom the same class or 1 if they are from different classes\\ncTrain the DNN on this training set For each image pair you can simultane\\nously feed the first image to DNN A and the second image to DNN B The\\nwhole network will gradually learn', 'y feed the first image to DNN A and the second image to DNN B The\\nwhole network will gradually learn to tell whether two images belong to the\\nsame class or not\\ndNow create a new DNN by reusing and freezing the hidden layers of DNN A\\nand adding a softmax output layer on top with 10 neurons Train this network\\non split 2 and see if you can achieve high performance despite having only\\n500 images per class\\nSolutions to these exercises are available in \\n366  Chapter 11 Training Deep Neural NetworksCHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 12 in the final\\nrelease of the book\\nSo far we have used only TensorFlows high level API tfkeras but it already got us\\npretty far we built various neural network architectures including regression and\\ncla', 'already got us\\npretty far we built various neural network architectures including regression and\\nclassification nets wide  deep nets and selfnormalizing nets using all sorts of tech\\nniques such as Batch Normalization dropout learning rate schedules and more In\\nfact 95 of the use cases you will encounter will not require anything else than\\ntfkeras and tfdata see Chapter 13  But now its time to dive deeper into TensorFlow\\nand take a look at its lowerlevel Python API  This will be useful when you need extra\\ncontrol to write custom loss functions custom metrics layers models initializers\\nregularizers weight constraints and more Y ou may even need to fully control the\\ntraining loop itself for example to apply special transformations or constraints to the\\ngradients beyond just clipping them or to use multiple optimizers for different\\nparts of the network We will cover all these cases in this chapter then we will also\\nlook at how you can boost your custom models and training algorithms using ', 'hapter then we will also\\nlook at how you can boost your custom models and training algorithms using Ten\\nsorFlows automatic graph generation feature But first lets take a quick tour of Ten\\nsorFlow\\n3671TensorFlow also includes another Deep Learning API called the Estimators API  but it is now recommended\\nto use tfkeras instead\\nTensorFlow 20 was released in March 2019 making TensorFlow\\nmuch easier to use The first edition of this book used TF 1 while\\nthis edition uses TF 2\\nA Quick Tour of TensorFlow\\nAs you know TensorFlow  is a powerful library for numerical computation particu\\nlarly well suited and finetuned for largescale Machine Learning but you could use\\nit for anything else that requires heavy computations It was developed by the Google\\nBrain team and it powers many of Googles largescale services such as Google Cloud\\nSpeech Google Photos and Google Search It was open sourced in November 2015\\nand it is now the most popular deep learning library in terms of citations in papers\\nadoption', 'r 2015\\nand it is now the most popular deep learning library in terms of citations in papers\\nadoption in companies stars on github etc countless projects use TensorFlow for\\nall sorts of Machine Learning tasks such as image classification natural language\\nprocessing NLP recommender systems time series forecasting and much more\\nSo what does TensorFlow actually offer Heres a summary\\nIts core is very similar to NumPy but with GPU support\\nIt also supports distributed computing across multiple devices and servers\\nIt includes a kind of justintime JIT compiler that allows it to optimize compu\\ntations for speed and memory usage it works by extracting the computation\\ngraph  from a Python function then optimizing it eg by pruning unused nodes\\nand finally running it efficiently eg by automatically running independent\\noperations in parallel\\nComputation graphs can be exported to a portable format so you can train a\\nTensorFlow model in one environment eg using Python on Linux and run it\\nin another eg ', 'u can train a\\nTensorFlow model in one environment eg using Python on Linux and run it\\nin another eg using Java on an Android device\\nIt implements autodiff see Chapter 10  and  and provides some excellent\\noptimizers such as RMSProp Nadam and FTRL see Chapter 11  so you can\\neasily minimize all sorts of loss functions\\nTensorFlow offers many more features built on top of these core features the\\nmost important is of course tfkeras1 but it also has data loading  preprocessing\\nops tfdata tfio etc image processing ops tfimage signal processing ops\\ntfsignal and more see Figure 121  for an overview of TensorFlows Python\\nAPI\\n368  Chapter 12 Custom Models and Training with TensorFlow2If you ever need to but you probably wont you can write your own operations using the C API\\n3If you are a researcher you may be eligible to use these TPUs for free see httpstensorfloworgtfrc  for more\\ndetails\\nFigure 121 TensorFlows Python API\\nWe will cover many of the packages and functions of the Tensor\\nFlow API but ', ' TensorFlows Python API\\nWe will cover many of the packages and functions of the Tensor\\nFlow API but its impossible to cover them all so you should really\\ntake some time to browse through the API you will find that it is\\nquite rich and well documented\\nAt the lowest level each TensorFlow operation is implemented using highly efficient\\nC code2 Many operations or ops for short have multiple implementations called\\nkernels  each kernel is dedicated to a specific device type such as CPUs GPUs or\\neven TPUs  Tensor Processing Units  As you may know GPUs can dramatically speed\\nup computations by splitting computations into many smaller chunks and running\\nthem in parallel across many GPU threads TPUs are even faster Y ou can purchase\\nyour own GPU devices for now TensorFlow only supports Nvidia cards with CUDA\\nCompute Capability 35 but TPUs are only available on Google Cloud Machine\\nLearning Engine  see 3\\nTensorFlows architecture is shown in Figure 122  most of the time your code will\\nuse the high', ' see 3\\nTensorFlows architecture is shown in Figure 122  most of the time your code will\\nuse the high level APIs especially tfkeras and tfdata but when you need more flexi\\nbility you will use the lower level Python API handling tensors directly Note that\\nAPIs for other languages are also available In any case TensorFlows execution\\nA Quick Tour of TensorFlow  369engine will take care of running the operations efficiently even across multiple devi\\nces and machines if you tell it to\\nFigure 122 TensorFlows architecture\\nTensorFlow runs not only on Windows Linux and MacOS but also on mobile devi\\nces using TensorFlow Lite  including both iOS and Android see  If you do not\\nwant to use the Python API there are also C Java Go and Swift APIs There is\\neven a Javascript implementation called TensorFlowjs  that makes it possible to run\\nyour models directly in your browser\\nTheres more to TensorFlow than just the library TensorFlow is at the center of an\\nextensive ecosystem of libraries First theres Te', ' just the library TensorFlow is at the center of an\\nextensive ecosystem of libraries First theres TensorBoard for visualization see\\nChapter 10  Next theres TensorFlow Extended TFX  which is a set of libraries built\\nby Google to productionize TensorFlow projects it includes tools for data validation\\npreprocessing model analysis and serving with TF Serving see  Google also\\nlaunched TensorFlow Hub  a way to easily download and reuse pretrained neural net\\nworks Y ou can also get many neural network architectures some of them pretrained\\nin TensorFlows model garden  Check out the TensorFlow Resources  or https\\ngithubcomjtoyawesometensorflow  for more TensorFlowbased projects Y ou will\\nfind hundreds of TensorFlow projects on GitHub so it is often easy to find existing\\ncode for whatever you are trying to do\\nMore and more ML papers are released along with their implemen\\ntation and sometimes even with pretrained models Check out\\nhttpspaperswithcodecom  to easily find them\\n370  Chapter 12 Custom ', 'with pretrained models Check out\\nhttpspaperswithcodecom  to easily find them\\n370  Chapter 12 Custom Models and Training with TensorFlowLast but not least TensorFlow has a dedicated team of passionate and helpful devel\\nopers and a large community contributing to improving it To ask technical ques\\ntions you should use httpstackoverflowcom  and tag your question with tensorflow\\nand python  Y ou can file bugs and feature requests through GitHub For general dis\\ncussions join the Google group \\nOkay its time to start coding\\nUsing TensorFlow like NumPy\\nTensorFlows API revolves around tensors  hence the name TensorFlow A tensor is\\nusually a multidimensional array exactly like a NumPy ndarray  but it can also hold\\na scalar a simple value such as 42 These tensors will be important when we create\\ncustom cost functions custom metrics custom layers and more so lets see how to\\ncreate and manipulate them\\nTensors and Operations\\nY ou can easily create a tensor using tfconstant  For example here is a ten', 'm\\nTensors and Operations\\nY ou can easily create a tensor using tfconstant  For example here is a tensor\\nrepresenting a matrix with two rows and three columns of floats\\n tfconstant 1 2 3 4 5 6  matrix\\ntfTensor id0 shape2 3 dtypefloat32 numpy\\narray1 2 3\\n       4 5 6 dtypefloat32\\n tfconstant 42  scalar\\ntfTensor id1 shape dtypeint32 numpy42\\nJust like an ndarray  a tfTensor  has a shape and a data type  dtype \\n t  tfconstant 1 2 3 4 5 6\\n tshape\\nTensorShape2 3\\n tdtype\\ntffloat32\\nIndexing works much like in NumPy\\n t 1\\ntfTensor id5 shape2 2 dtypefloat32 numpy\\narray2 3\\n       5 6 dtypefloat32\\n t 1 tfnewaxis\\ntfTensor id15 shape2 1 dtypefloat32 numpy\\narray2\\n       5 dtypefloat32\\nMost importantly all sorts of tensor operations are available\\n t  10\\ntfTensor id18 shape2 3 dtypefloat32 numpy\\nUsing TensorFlow like NumPy  371array11 12 13\\n       14 15 16 dtypefloat32\\n tfsquaret\\ntfTensor id20 shape2 3 dtypefloat32 numpy\\narray 1  4  9\\n       16 25 36 dtypefloat32\\n t  tftranspose t\\ntfTensor id24 shape2 2 d', 'efloat32 numpy\\narray 1  4  9\\n       16 25 36 dtypefloat32\\n t  tftranspose t\\ntfTensor id24 shape2 2 dtypefloat32 numpy\\narray14 32\\n       32 77 dtypefloat32\\nNote that writing t  10  is equivalent to calling tfaddt 10  indeed Python calls\\nthe magic method tadd10  which just calls tfaddt 10  Other operators\\nlike   etc are also supported The  operator was added in Python 35 for matrix\\nmultiplication it is equivalent to calling the tfmatmul  function\\nY ou will find all the basic math operations you need eg tfadd  tfmultiply \\ntfsquare  tfexp  tfsqrt  and more generally most operations that you\\ncan find in NumPy eg tfreshape  tfsqueeze  tftile  but sometimes\\nwith a different name eg tfreducemean  tfreducesum  tfreducemax \\ntfmathlog  are the equivalent of npmean  npsum  npmax  and nplog \\nWhen the name differs there is often a good reason for it for example in Tensor\\nFlow you must write tftransposet  you cannot just write tT like in NumPy The\\nreason is that it does not do exactly the same thing ', 'et  you cannot just write tT like in NumPy The\\nreason is that it does not do exactly the same thing in TensorFlow a new tensor is\\ncreated with its own copy of the transposed data while in NumPy tT is just a trans\\nposed view on the same data Similarly the tfreducesum  operation is named this\\nway because its GPU kernel ie GPU implementation uses a reduce algorithm that\\ndoes not guarantee the order in which the elements are added because 32bit floats\\nhave limited precision this means that the result may change ever so slightly every\\ntime you call this operation The same is true of tfreducemean  but of course\\ntfreducemax  is deterministic\\n372  Chapter 12 Custom Models and Training with TensorFlow4A notable exception is tfmathlog  which is commonly used but there is no tflog  alias as it might be\\nconfused with logging\\nMany functions and classes have aliases For example tfadd\\nand tfmathadd  are the same function This allows TensorFlow\\nto have concise names for the most common operations4 whi', 'e the same function This allows TensorFlow\\nto have concise names for the most common operations4 while\\npreserving well organized packages\\nKeras LowLevel API\\nThe Keras API actually has its own lowlevel API located in kerasbackend  It\\nincludes functions like square  exp  sqrt  and so on In tfkeras these func\\ntions generally just call the corresponding TensorFlow operations If you want to\\nwrite code that will be portable to other Keras implementations you should use these\\nKeras functions However they only cover a subset of all functions available in Ten\\nsorFlow so in this book we will use the TensorFlow operations directly Here is as\\nsimple example using kerasbackend  which is commonly named K for short\\n from tensorflow  import keras\\n K  kerasbackend\\n KsquareKtranspose t  10\\ntfTensor id39 shape3 2 dtypefloat32 numpy\\narray11 26\\n       14 35\\n       19 46 dtypefloat32\\nTensors and NumPy\\nTensors play nice with NumPy you can create a tensor from a NumPy array and vice\\nversa and you can even app', ' play nice with NumPy you can create a tensor from a NumPy array and vice\\nversa and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors\\n a  nparray2 4 5\\n tfconstant a\\ntfTensor id111 shape3 dtypefloat64 numpyarray2 4 5\\n tnumpy  or nparrayt\\narray1 2 3\\n       4 5 6 dtypefloat32\\n tfsquarea\\ntfTensor id116 shape3 dtypefloat64 numpyarray4 16 25\\n npsquaret\\narray 1  4  9\\n       16 25 36 dtypefloat32\\nUsing TensorFlow like NumPy  373Notice that NumPy uses 64bit precision by default while Tensor\\nFlow uses 32bit This is because 32bit precision is generally more\\nthan enough for neural networks plus it runs faster and uses less\\nRAM So when you create a tensor from a NumPy array make sure\\nto set dtypetffloat32 \\nType Conversions\\nType conversions can significantly hurt performance and they can easily go unno\\nticed when they are done automatically To avoid this TensorFlow does not perform\\nany type conversions automatically it just raises an exception if you try to exe', 'ow does not perform\\nany type conversions automatically it just raises an exception if you try to execute an\\noperation on tensors with incompatible types For example you cannot add a float\\ntensor and an integer tensor and you cannot even add a 32bit float and a 64bit float\\n tfconstant 2  tfconstant 40\\nTracebackInvalidArgumentErrorexpected to be a float\\n tfconstant 2  tfconstant 40 dtypetffloat64\\nTracebackInvalidArgumentErrorexpected to be a double\\nThis may be a bit annoying at first but remember that its for a good cause And of\\ncourse you can use tfcast  when you really need to convert types\\n t2  tfconstant 40 dtypetffloat64\\n tfconstant 20  tfcastt2 tffloat32\\ntfTensor id136 shape dtypefloat32 numpy420\\nVariables\\nSo far we have used constant tensors as their name suggests you cannot modify\\nthem However the weights in a neural network need to be tweaked by backpropaga\\ntion and other parameters may also need to change over time eg a momentum\\noptimizer keeps track of past gradients What we n', 'rs may also need to change over time eg a momentum\\noptimizer keeps track of past gradients What we need is a tfVariable \\n v  tfVariable 1 2 3 4 5 6\\n v\\ntfVariable Variable0 shape2 3 dtypefloat32 numpy\\narray1 2 3\\n       4 5 6 dtypefloat32\\nA tfVariable  acts much like a constant tensor you can perform the same opera\\ntions with it it plays nicely with NumPy as well and it is just as picky with types But\\nit can also be modified in place using the assign  method or assignadd  or\\nassignsub  which increment or decrement the variable by the given value Y ou\\ncan also modify individual cells or slices using the cells or slices assign\\nmethod direct item assignment will not work or using the scatterupdate  or\\nscatterndupdate  methods\\nvassign2  v             2 4 6 8 10 12\\nv0 1assign42          2 42 6 8 10 12\\n374  Chapter 12 Custom Models and Training with TensorFlowv 2assign0 1    2 42 0 8 10 1\\nvscatterndupdate indices0 0 1 2 updates100 200\\n                            100 42 0 8 10 200\\nIn practice y', 'erndupdate indices0 0 1 2 updates100 200\\n                            100 42 0 8 10 200\\nIn practice you will rarely have to create variables manually since\\nKeras provides an addweight  method that will take care of it for\\nyou as we will see Moreover model parameters will generally be\\nupdated directly by the optimizers so you will rarely need to\\nupdate variables manually\\nOther Data Structures\\nTensorFlow supports several other data structures including the following please see\\nthe notebook or  for more details\\nSparse tensors  tfSparseTensor  efficiently represent tensors containing mostly\\n0s The tfsparse  package contains operations for sparse tensors\\nTensor arrays  tfTensorArray  are lists of tensors They have a fixed size by\\ndefault but can optionally be made dynamic All tensors they contain must have\\nthe same shape and data type\\nRagged tensors  tfRaggedTensor  represent static lists of lists of tensors where\\nevery tensor has the same shape and data type The tfragged  package contains\\no', 'ists of tensors where\\nevery tensor has the same shape and data type The tfragged  package contains\\noperations for ragged tensors\\nString tensors  are regular tensors of type tfstring  These actually represent byte\\nstrings not Unicode strings so if you create a string tensor using a Unicode\\nstring eg a regular Python 3 string like caf  then it will get encoded to\\nUTF8 automatically eg bcafxc3xa9  Alternatively you can represent\\nUnicode strings using tensors of type tfint32  where each item represents a\\nUnicode codepoint eg 99 97 102 233  The tfstrings  package with\\nan s contains ops for byte strings and Unicode strings and to convert one into\\nthe other\\nSets are just represented as regular tensors or sparse tensors containing one or\\nmore sets and you can manipulate them using operations from the tfsets\\npackage\\nQueues  including First In First Out FIFO queues  FIFOQueue  queues that can\\nprioritize some items  PriorityQueue  queues that shuffle their items  Random\\nShuffleQueue  and queues t', 'ritize some items  PriorityQueue  queues that shuffle their items  Random\\nShuffleQueue  and queues that can batch items of different shapes by padding\\nPaddingFIFOQueue  These classes are all in the tfqueue  package\\nWith tensors operations variables and various data structures at your disposal you\\nare now ready to customize your models and training algorithms\\nUsing TensorFlow like NumPy  375Customizing Models and Training Algorithms\\nLets start by creating a custom loss function which is a simple and common use case\\nCustom Loss Functions\\nSuppose you want to train a regression model but your training set is a bit noisy Of\\ncourse you start by trying to clean up your dataset by removing or fixing the outliers\\nbut it turns out to be insufficient the dataset is still noisy Which loss function should\\nyou use The mean squared error might penalize large errors too much so your\\nmodel will end up being imprecise The mean absolute error would not penalize out\\nliers as much but training might take a', 'ing imprecise The mean absolute error would not penalize out\\nliers as much but training might take a while to converge and the trained model\\nmight not be very precise This is probably a good time to use the Huber loss intro\\nduced in Chapter 10  instead of the good old MSE The Huber loss is not currently\\npart of the official Keras API but it is available in tfkeras just use an instance of the\\nkeraslossesHuber  class But lets pretend its not there implementing it is easy as\\npie Just create a function that takes the labels and predictions as arguments and use\\nTensorFlow operations to compute every instances loss\\ndef huberfn ytrue ypred\\n    error  ytrue  ypred\\n    issmallerror   tfabserror  1\\n    squaredloss   tfsquareerror  2\\n    linearloss    tfabserror  05\\n    return tfwhereissmallerror  squaredloss  linearloss \\nFor better performance you should use a vectorized implementa\\ntion as in this example Moreover if you want to benefit from Ten\\nsorFlows graph features you should use only Tensor', ' example Moreover if you want to benefit from Ten\\nsorFlows graph features you should use only TensorFlow\\noperations\\nIt is also preferable to return a tensor containing one loss per instance rather than\\nreturning the mean loss This way Keras can apply class weights or sample weights\\nwhen requested see Chapter 10 \\nNext you can just use this loss when you compile the Keras model then train your\\nmodel\\nmodelcompilelosshuberfn  optimizer nadam\\nmodelfitXtrain ytrain \\nAnd thats it For each batch during training Keras will call the huberfn  function\\nto compute the loss and use it to perform a Gradient Descent step Moreover it will\\nkeep track of the total loss since the beginning of the epoch and it will display the\\nmean loss\\n376  Chapter 12 Custom Models and Training with TensorFlowBut what happens to this custom loss when we save the model\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine as Keras just\\nsaves the name o', '\\nSaving a model containing a custom loss function actually works fine as Keras just\\nsaves the name of the function However whenever you load it you need to provide a\\ndictionary that maps the function name to the actual function More generally when\\nyou load a model containing custom objects you need to map the names to the\\nobjects\\nmodel  kerasmodelsloadmodel mymodelwithacustomlossh5 \\n                                customobjects huberfn  huberfn \\nWith the current implementation any error between 1 and 1 is considered small \\nBut what if we want a different threshold One solution is to create a function that\\ncreates a configured loss function\\ndef createhuber threshold 10\\n    def huberfn ytrue ypred\\n        error  ytrue  ypred\\n        issmallerror   tfabserror  threshold\\n        squaredloss   tfsquareerror  2\\n        linearloss    threshold   tfabserror  threshold 2  2\\n        return tfwhereissmallerror  squaredloss  linearloss \\n    return huberfn\\nmodelcompilelosscreatehuber 20 optimizer n', 'issmallerror  squaredloss  linearloss \\n    return huberfn\\nmodelcompilelosscreatehuber 20 optimizer nadam\\nUnfortunately when you save the model the threshold  will not be saved This means\\nthat you will have to specify the threshold  value when loading the model note that\\nthe name to use is huberfn  which is the name of the function we gave Keras not\\nthe name of the function that created it\\nmodel  kerasmodelsloadmodel mymodelwithacustomlossthreshold2h5 \\n                                customobjects huberfn  createhuber 20\\nY ou can solve this by creating a subclass of the keraslossesLoss  class and imple\\nment its getconfig  method\\nclass HuberLoss keraslossesLoss\\n    def init self threshold 10 kwargs\\n        selfthreshold   threshold\\n        superinit kwargs\\n    def callself ytrue ypred\\n        error  ytrue  ypred\\n        issmallerror   tfabserror  selfthreshold\\n        squaredloss   tfsquareerror  2\\n        linearloss    selfthreshold   tfabserror  selfthreshold 2  2\\n        return tfwher', 'eerror  2\\n        linearloss    selfthreshold   tfabserror  selfthreshold 2  2\\n        return tfwhereissmallerror  squaredloss  linearloss \\n    def getconfig self\\n        baseconfig   supergetconfig \\n        return baseconfig  threshold  selfthreshold \\nCustomizing Models and Training Algorithms  3775It would not be a good idea to use a weighted mean if we did then two instances with the same weight but in\\ndifferent batches would have a different impact on training depending on the total weight of each batch\\nThe Keras API only specifies how to use subclassing to define lay\\ners models callbacks and regularizers If you build other compo\\nnents such as losses metrics initializers or constraints using\\nsubclassing they may not be portable to other Keras implementa\\ntions\\nLets walk through this code\\nThe constructor accepts kwargs  and passes them to the parent constructor\\nwhich handles standard hyperparameters the name  of the loss and the reduction\\nalgorithm to use to aggregate the individual ', 'yperparameters the name  of the loss and the reduction\\nalgorithm to use to aggregate the individual instance losses By default it is\\nsumoverbatchsize  which means that the loss will be the sum of the\\ninstance losses possibly weighted by the sample weights if any and then divide\\nthe result by the batch size not by the sum of weights so this is not the weighted\\nmean5 Other possible values are sum  and None \\nThe call  method takes the labels and predictions computes all the instance\\nlosses and returns them\\nThe getconfig  method returns a dictionary mapping each hyperparameter\\nname to its value It first calls the parent classs getconfig  method then adds\\nthe new hyperparameters to this dictionary note that the convenient x  syn\\ntax was added in Python 35\\nY ou can then use any instance of this class when you compile the model\\nmodelcompilelossHuberLoss 2 optimizer nadam\\nWhen you save the model the threshold will be saved along with it and when you\\nload the model you just need to map the clas', ' the threshold will be saved along with it and when you\\nload the model you just need to map the class name to the class itself\\nmodel  kerasmodelsloadmodel mymodelwithacustomlossclassh5 \\n                                customobjects HuberLoss  HuberLoss \\nWhen you save a model Keras calls the loss instances getconfig  method and\\nsaves the config as JSON in the HDF5 file When you load the model it calls the\\nfromconfig  class method on the HuberLoss  class this method is implemented by\\nthe base class  Loss  and just creates an instance of the class passing config  to the\\nconstructor\\nThats it for losses It was not too hard was it Well its just as simple for custom acti\\nvation functions initializers regularizers and constraints Lets look at these now\\n378  Chapter 12 Custom Models and Training with TensorFlowCustom Activation Functions Initializers Regularizers and\\nConstraints\\nMost Keras functionalities such as losses regularizers constraints initializers met\\nrics activation functions layers ', 'onalities such as losses regularizers constraints initializers met\\nrics activation functions layers and even full models can be customized in very much\\nthe same way Most of the time you will just need to write a simple function with the\\nappropriate inputs and outputs For example here are examples of a custom activa\\ntion function equivalent to kerasactivationssoftplus  or tfnnsoftplus  a\\ncustom Glorot initializer equivalent to kerasinitializersglorotnormal  a cus\\ntom 1 regularizer equivalent to kerasregularizersl1001  and a custom con\\nstraint that ensures weights are all positive equivalent to\\nkerasconstraintsnonneg  or tfnnrelu \\ndef mysoftplus z  return value is just tfnnsoftplusz\\n    return tfmathlogtfexpz  10\\ndef myglorotinitializer shape dtypetffloat32\\n    stddev  tfsqrt2  shape0  shape1\\n    return tfrandomnormalshape stddevstddev dtypedtype\\ndef myl1regularizer weights\\n    return tfreducesum tfabs001  weights\\ndef mypositiveweights weights  return value is just tfnnreluweights\\n    re', 'ducesum tfabs001  weights\\ndef mypositiveweights weights  return value is just tfnnreluweights\\n    return tfwhereweights  0 tfzeroslike weights weights\\nAs you can see the arguments depend on the type of custom function These custom\\nfunctions can then be used normally for example\\nlayer  keraslayersDense30 activation mysoftplus \\n                           kernelinitializer myglorotinitializer \\n                           kernelregularizer myl1regularizer \\n                           kernelconstraint mypositiveweights \\nThe activation function will be applied to the output of this Dense  layer and its result\\nwill be passed on to the next layer The layers weights will be initialized using the\\nvalue returned by the initializer At each training step the weights will be passed to the\\nregularization function to compute the regularization loss which will be added to the\\nmain loss to get the final loss used for training Finally the constraint function will be\\ncalled after each training step and the ', 's used for training Finally the constraint function will be\\ncalled after each training step and the layers weights will be replaced by the con\\nstrained weights\\nIf a function has some hyperparameters that need to be saved along with the model\\nthen you will want to subclass the appropriate class such as kerasregulariz\\nersRegularizer  kerasconstraintsConstraint  kerasinitializersInitial\\nizer  or keraslayersLayer  for any layer including activation functions For\\nexample much like we did for the custom loss here is a simple class for 1 regulariza\\nCustomizing Models and Training Algorithms  3796However the Huber loss is seldom used as a metric the MAE or MSE are preferredtion that saves its factor  hyperparameter this time we do not need to call the parent\\nconstructor or the getconfig  method as they are not defined by the parent class\\nclass MyL1Regularizer kerasregularizers Regularizer \\n    def init self factor\\n        selffactor  factor\\n    def call self weights\\n        return tfreducesum ', 'ef init self factor\\n        selffactor  factor\\n    def call self weights\\n        return tfreducesum tfabsselffactor  weights\\n    def getconfig self\\n        return factor  selffactor\\nNote that you must implement the call  method for losses layers including activa\\ntion functions and models or the call  method for regularizers initializers\\nand constraints For metrics things are a bit different as we will see now\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing losses are used by Gradient\\nDescent to train  a model so they must be differentiable at least where they are evalu\\nated and their gradients should not be 0 everywhere Plus its okay if they are not\\neasily interpretable by humans eg crossentropy In contrast metrics are used to\\nevaluate  a model they must be more easily interpretable and they can be non\\ndifferentiable or have 0 gradients everywhere eg accuracy\\nThat said in most cases defining a custom metric function is exactly the same as\\ndefining a custom loss fu', 'aid in most cases defining a custom metric function is exactly the same as\\ndefining a custom loss function In fact we could even use the Huber loss function we\\ncreated earlier as a metric6 it would work just fine and persistence would also work\\nthe same way in this case only saving the name of the function huberfn \\nmodelcompilelossmse optimizer nadam metricscreatehuber 20\\nFor each batch during training Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch Most of the time this is exactly what you\\nwant But not always Consider a binary classifiers precision for example As we saw\\nin Chapter 3  precision is the number of true positives divided by the number of posi\\ntive predictions including both true positives and false positives Suppose the model\\nmade 5 positive predictions in the first batch 4 of which were correct thats 80 pre\\ncision Then suppose the model made 3 positive predictions in the second batch but\\nthey were all incorrect thats 0 precision ', 'model made 3 positive predictions in the second batch but\\nthey were all incorrect thats 0 precision for the second batch If you just compute\\nthe mean of these two precisions you get 40 But wait a second this is not the mod\\nels precision over these two batches Indeed there were a total of 4 true positives 4 \\n0 out of 8 positive predictions 5  3 so the overall precision is 50 not 40 What\\nwe need is an object that can keep track of the number of true positives and the num\\n380  Chapter 12 Custom Models and Training with TensorFlowber of false positives and compute their ratio when requested This is precisely what\\nthe kerasmetricsPrecision  class does\\n precision   kerasmetricsPrecision \\n precision 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1\\ntfTensor id581729 shape dtypefloat32 numpy08\\n precision 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0\\ntfTensor id581780 shape dtypefloat32 numpy05\\nIn this example we created a Precision  object then we used it like a function pass\\ning it the labels and predictions for the first b', 'ision  object then we used it like a function pass\\ning it the labels and predictions for the first batch then for the second batch note\\nthat we could also have passed sample weights We used the same number of true\\nand false positives as in the example we just discussed After the first batch it returns\\nthe precision of 80 then after the second batch it returns 50 which is the overall\\nprecision so far not the second batchs precision This is called a streaming metric  or\\nstateful metric  as it is gradually updated batch after batch\\nAt any point we can call the result  method to get the current value of the metric\\nWe can also look at its variables tracking the number of true and false positives\\nusing the variables  attribute and reset these variables using the resetstates\\nmethod\\n presult\\ntfTensor id581794 shape dtypefloat32 numpy05\\n pvariables\\ntfVariable truepositives0  numpyarray4 dtypefloat32\\n tfVariable falsepositives0  numpyarray4 dtypefloat32\\n presetstates   both variables get reset t', 't32\\n tfVariable falsepositives0  numpyarray4 dtypefloat32\\n presetstates   both variables get reset to 00\\nIf you need to create such a streaming metric you can just create a subclass of the\\nkerasmetricsMetric  class Here is a simple example that keeps track of the total\\nHuber loss and the number of instances seen so far When asked for the result it\\nreturns the ratio which is simply the mean Huber loss\\nclass HuberMetric kerasmetricsMetric\\n    def init self threshold 10 kwargs\\n        superinit kwargs  handles base args eg dtype\\n        selfthreshold   threshold\\n        selfhuberfn   createhuber threshold \\n        selftotal  selfaddweight total initializer zeros\\n        selfcount  selfaddweight count initializer zeros\\n    def updatestate self ytrue ypred sampleweight None\\n        metric  selfhuberfn ytrue ypred\\n        selftotalassignadd tfreducesum metric\\n        selfcountassignadd tfcasttfsizeytrue tffloat32\\n    def resultself\\n        return selftotal  selfcount\\n    def getconfig self\\n ', 'fsizeytrue tffloat32\\n    def resultself\\n        return selftotal  selfcount\\n    def getconfig self\\n        baseconfig   supergetconfig \\n        return baseconfig  threshold  selfthreshold \\nCustomizing Models and Training Algorithms  3817This class is for illustration purposes only A simpler and better implementation would just subclass the\\nkerasmetricsMean  class see the notebook for an example\\nLets walk through this code7\\nThe constructor uses the addweight  method to create the variables needed to\\nkeep track of the metrics state over multiple batches in this case the sum of all\\nHuber losses  total  and the number of instances seen so far  count  Y ou could\\njust create variables manually if you preferred Keras tracks any tfVariable  that\\nis set as an attribute and more generally any trackable object such as layers or\\nmodels\\nThe updatestate  method is called when you use an instance of this class as a\\nfunction as we did with the Precision  object It updates the variables given the\\nlabel', 'is class as a\\nfunction as we did with the Precision  object It updates the variables given the\\nlabels and predictions for one batch and sample weights but in this case we just\\nignore them\\nThe result  method computes and returns the final result in this case just the\\nmean Huber metric over all instances When you use the metric as a function the\\nupdatestate  method gets called first then the result  method is called\\nand its output is returned\\nWe also implement the getconfig  method to ensure the threshold  gets\\nsaved along with the model\\nThe default implementation of the resetstates  method just resets all vari\\nables to 00 but you can override it if needed\\nKeras will take care of variable persistence seamlessly no action is\\nrequired\\nWhen you define a metric using a simple function Keras automatically calls it for\\neach batch and it keeps track of the mean during each epoch just like we did man\\nually So the only benefit of our HuberMetric  class is that the threshold  will be saved\\nBut of ', 'man\\nually So the only benefit of our HuberMetric  class is that the threshold  will be saved\\nBut of course some metrics like precision cannot simply be averaged over batches\\nin thoses cases theres no other option than to implement a streaming metric\\nNow that we have built a streaming metric building a custom layer will seem like a\\nwalk in the park\\n382  Chapter 12 Custom Models and Training with TensorFlowCustom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation In this case you will\\nneed to create a custom layer Or sometimes you may simply want to build a very\\nrepetitive architecture containing identical blocks of layers repeated many times and\\nit would be convenient to treat each block of layers as a single layer For example if\\nthe model is a sequence of layers A B C A B C A B C then you might want to\\ndefine a custom layer D containing layers A B C and your model would then simply\\nbe D', ' might want to\\ndefine a custom layer D containing layers A B C and your model would then simply\\nbe D D D Lets see how to build custom layers\\nFirst some layers have no weights such as keraslayersFlatten  or keraslay\\nersReLU  If you want to create a custom layer without any weights the simplest\\noption is to write a function and wrap it in a keraslayersLambda  layer For exam\\nple the following layer will apply the exponential function to its inputs\\nexponentiallayer   keraslayersLambdalambda x tfexpx\\nThis custom layer can then be used like any other layer using the sequential API the\\nfunctional API or the subclassing API Y ou can also use it as an activation function\\nor you could just use activationtfexp  or activationkerasactivationsexpo\\nnential  or simply activationexponential  The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales eg 0001 10 1000\\nAs you probably guessed by now to build a custom stateful laye', 'very\\ndifferent scales eg 0001 10 1000\\nAs you probably guessed by now to build a custom stateful layer ie a layer with\\nweights you need to create a subclass of the keraslayersLayer  class For exam\\nple the following class implements a simplified version of the Dense  layer\\nclass MyDensekeraslayersLayer\\n    def init self units activation None kwargs\\n        superinit kwargs\\n        selfunits  units\\n        selfactivation   kerasactivations getactivation \\n    def buildself batchinputshape \\n        selfkernel  selfaddweight \\n            namekernel  shapebatchinputshape 1 selfunits\\n            initializer glorotnormal \\n        selfbias  selfaddweight \\n            namebias shapeselfunits initializer zeros\\n        superbuildbatchinputshape   must be at the end\\n    def callself X\\n        return selfactivation X  selfkernel  selfbias\\n    def computeoutputshape self batchinputshape \\n        return tfTensorShape batchinputshape aslist1  selfunits\\nCustomizing Models and Training Algorithms  3838Thi', 'tfTensorShape batchinputshape aslist1  selfunits\\nCustomizing Models and Training Algorithms  3838This function is specific to tfkeras Y ou could use kerasactivationsActivation  instead\\n9The Keras API calls this argument inputshape  but since it also includes the batch dimension I prefer to call\\nit batchinputshape  Same for computeoutputshape     def getconfig self\\n        baseconfig   supergetconfig \\n        return baseconfig  units selfunits\\n                activation  kerasactivations serialize selfactivation \\nLets walk through this code\\nThe constructor takes all the hyperparameters as arguments in this example just\\nunits  and activation  and importantly it also takes a kwargs  argument It\\ncalls the parent constructor passing it the kwargs  this takes care of standard\\narguments such as inputshape  trainable  name  and so on Then it saves the\\nhyperparameters as attributes converting the activation  argument to the\\nappropriate activation function using the kerasactivationsget  function', ' activation  argument to the\\nappropriate activation function using the kerasactivationsget  function it\\naccepts functions standard strings like relu  or selu  or simply None 8\\nThe build  methods role is to create the layers variables by calling the\\naddweight  method for each weight The build  method is called the first\\ntime the layer is used At that point Keras will know the shape of this layers\\ninputs and it will pass it to the build  method9 which is often necessary to cre\\nate some of the weights For example we need to know the number of neurons in\\nthe previous layer in order to create the connection weights matrix ie the ker\\nnel  this corresponds to the size of the last dimension of the inputs At the end\\nof the build  method and only at the end you must call the parents build\\nmethod this tells Keras that the layer is built it just sets selfbuilt  True \\nThe call  method actually performs the desired operations In this case we\\ncompute the matrix multiplication of the inputs X and the ', 'ms the desired operations In this case we\\ncompute the matrix multiplication of the inputs X and the layers kernel we add\\nthe bias vector we apply the activation function to the result and this gives us the\\noutput of the layer\\nThe computeoutputshape  method simply returns the shape of this layers\\noutputs In this case it is the same shape as the inputs except the last dimension\\nis replaced with the number of neurons in the layer Note that in tfkeras shapes\\nare instances of the tfTensorShape  class which you can convert to Python lists\\nusing aslist \\nThe getconfig  method is just like earlier Note that we save the activation\\nfunctions full configuration by calling kerasactivationsserialize \\nY ou can now use a MyDense  layer just like any other layer\\n384  Chapter 12 Custom Models and Training with TensorFlowY ou can generally omit the computeoutputshape  method as\\ntfkeras automatically infers the output shape except when the\\nlayer is dynamic as we will see shortly In other Keras implemen\\nta', ' the output shape except when the\\nlayer is dynamic as we will see shortly In other Keras implemen\\ntations this method is either required or by default it assumes the\\noutput shape is the same as the input shape\\nTo create a layer with multiple inputs eg Concatenate  the argument to the call\\nmethod should be a tuple containing all the inputs and similarly the argument to the\\ncomputeoutputshape  method should be a tuple containing each inputs batch\\nshape To create a layer with multiple outputs the call  method should return the\\nlist of outputs and the computeoutputshape  should return the list of batch out\\nput shapes one per output For example the following toy layer takes two inputs\\nand returns three outputs\\nclass MyMultiLayer keraslayersLayer\\n    def callself X\\n        X1 X2  X\\n        return X1  X2 X1  X2 X1  X2\\n    def computeoutputshape self batchinputshape \\n        b1 b2  batchinputshape\\n        return b1 b1 b1  should probably handle broadcasting rules\\nThis layer may now be used lik', 'pe\\n        return b1 b1 b1  should probably handle broadcasting rules\\nThis layer may now be used like any other layer but of course only using the func\\ntional and subclassing APIs not the sequential API which only accepts layers with\\none input and one output\\nIf your layer needs to have a different behavior during training and during testing\\neg if it uses Dropout  or BatchNormalization  layers then you must add a train\\ning argument to the call  method and use this argument to decide what to do For\\nexample lets create a layer that adds Gaussian noise during training for regulariza\\ntion but does nothing during testing Keras actually has a layer that does the same\\nthing keraslayersGaussianNoise \\nclass MyGaussianNoise keraslayersLayer\\n    def init self stddev kwargs\\n        superinit kwargs\\n        selfstddev  stddev\\n    def callself X training None\\n        if training \\n            noise  tfrandomnormaltfshapeX stddevselfstddev\\n            return X  noise\\n        else\\n            return X\\n ', 'andomnormaltfshapeX stddevselfstddev\\n            return X  noise\\n        else\\n            return X\\n    def computeoutputshape self batchinputshape \\n        return batchinputshape\\nCustomizing Models and Training Algorithms  38510The name subclassing API usually refers only to the creation of custom models by subclassing although\\nmany other things can be created by subclassing as we saw in this chapterWith that you can now build any custom layer you need Now lets create custom\\nmodels\\nCustom Models\\nWe already looked at custom model classes in Chapter 10  when we discussed the sub\\nclassing API10 It is actually quite straightforward just subclass the kerasmod\\nelsModel  class create layers and variables in the constructor and implement the\\ncall  method to do whatever you want the model to do For example suppose you\\nwant to build the model represented in Figure 123 \\nFigure 123 Custom Model Example\\nThe inputs go through a first dense layer then through a residual block  composed of\\ntwo dense l', 'ple\\nThe inputs go through a first dense layer then through a residual block  composed of\\ntwo dense layers and an addition operation as we will see in Chapter 14  a residual\\nblock adds its inputs to its outputs then through this same residual block 3 more\\ntimes then through a second residual block and the final result goes through a dense\\noutput layer Note that this model does not make much sense its just an example to\\nillustrate the fact that you can easily build any kind of model you want even contain\\n386  Chapter 12 Custom Models and Training with TensorFlowing loops and skip connections To implement this model it is best to first create a\\nResidualBlock  layer since we are going to create a couple identical blocks and we\\nmight want to reuse it in another model\\nclass ResidualBlock keraslayersLayer\\n    def init self nlayers  nneurons  kwargs\\n        superinit kwargs\\n        selfhidden  keraslayersDensenneurons  activation elu\\n                                          kernelinitializer ', 'eraslayersDensenneurons  activation elu\\n                                          kernelinitializer henormal \\n                       for  in rangenlayers \\n    def callself inputs\\n        Z  inputs\\n        for layer in selfhidden\\n            Z  layerZ\\n        return inputs  Z\\nThis layer is a bit special since it contains other layers This is handled transparently\\nby Keras it automatically detects that the hidden  attribute contains trackable objects\\nlayers in this case so their variables are automatically added to this layers list of\\nvariables The rest of this class is selfexplanatory Next lets use the subclassing API\\nto define the model itself\\nclass ResidualRegressor kerasmodelsModel\\n    def init self outputdim  kwargs\\n        superinit kwargs\\n        selfhidden1  keraslayersDense30 activation elu\\n                                          kernelinitializer henormal \\n        selfblock1  ResidualBlock 2 30\\n        selfblock2  ResidualBlock 2 30\\n        selfout  keraslayersDenseoutputdim ', 'esidualBlock 2 30\\n        selfblock2  ResidualBlock 2 30\\n        selfout  keraslayersDenseoutputdim \\n    def callself inputs\\n        Z  selfhidden1inputs\\n        for  in range1  3\\n            Z  selfblock1Z\\n        Z  selfblock2Z\\n        return selfoutZ\\nWe create the layers in the constructor and use them in the call  method This\\nmodel can then be used like any other model compile it fit it evaluate it and use it to\\nmake predictions If you also want to be able to save the model using the save\\nmethod and load it using the kerasmodelsloadmodel  function you must\\nimplement the getconfig  method as we did earlier in both the ResidualBlock\\nclass and the ResidualRegressor  class Alternatively you can just save and load the\\nweights using the saveweights  and loadweights  methods\\nThe Model  class is actually a subclass of the Layer  class so models can be defined and\\nused exactly like layers But a model also has some extra functionalities including of\\ncourse its compile  fit  evaluate  and pre', ' a model also has some extra functionalities including of\\ncourse its compile  fit  evaluate  and predict  methods and a few var\\nCustomizing Models and Training Algorithms  387iants such as trainonbatch  or fitgenerator  plus the getlayers\\nmethod which can return any of the models layers by name or by index and the\\nsave  method and support for kerasmodelsloadmodel  and kerasmod\\nelsclonemodel  So if models provide more functionalities than layers why not\\njust define every layer as a model Well technically you could but it is probably\\ncleaner to distinguish the internal components of your model layers or reusable\\nblocks of layers from the model itself The former should subclass the Layer  class\\nwhile the latter should subclass the Model  class\\nWith that you can quite naturally and concisely build almost any model that you find\\nin a paper either using the sequential API the functional API the subclassing API or\\neven a mix of these  Almost any model Y es there are still a couple things that', 'e subclassing API or\\neven a mix of these  Almost any model Y es there are still a couple things that we\\nneed to look at first how to define losses or metrics based on model internals and\\nsecond how to build a custom training loop\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions and optionally sample weights However you will occasionally want to\\ndefine losses based on other parts of your model such as the weights or activations of\\nits hidden layers This may be useful for regularization purposes or to monitor some\\ninternal aspect of your model\\nTo define a custom loss based on model internals just compute it based on any part\\nof the model you want then pass the result to the addloss  method For example\\nthe following custom model represents a standard MLP regressor with 5 hidden lay\\ners except it also implements a reconstruction loss  see  we add an extra Dense\\nlayer on top of the last hidden layer', 'o implements a reconstruction loss  see  we add an extra Dense\\nlayer on top of the last hidden layer and its role is to try to reconstruct the inputs of\\nthe model Since the reconstruction must have the same shape as the models inputs\\nwe need to create this Dense  layer in the build  method to have access to the shape\\nof the inputs In the call  method we compute both the regular output of the MLP \\nplus the output of the reconstruction layer We then compute the mean squared dif\\nference between the reconstructions and the inputs and we add this value times\\n005 to the models list of losses by calling addloss  During training Keras will\\nadd this loss to the main loss which is why we scaled down the reconstruction loss\\nto ensure the main loss dominates As a result the model will be forced to preserve\\nas much information as possible through the hidden layers even information that is\\nnot directly useful for the regression task itself In practice this loss sometimes\\nimproves generalization it i', 'y useful for the regression task itself In practice this loss sometimes\\nimproves generalization it is a regularization loss\\nclass ReconstructingRegressor kerasmodelsModel\\n    def init self outputdim  kwargs\\n        superinit kwargs\\n        selfhidden  keraslayersDense30 activation selu\\n                                          kernelinitializer lecunnormal \\n388  Chapter 12 Custom Models and Training with TensorFlow                       for  in range5\\n        selfout  keraslayersDenseoutputdim \\n    def buildself batchinputshape \\n        ninputs   batchinputshape 1\\n        selfreconstruct   keraslayersDenseninputs \\n        superbuildbatchinputshape \\n    def callself inputs\\n        Z  inputs\\n        for layer in selfhidden\\n            Z  layerZ\\n        reconstruction   selfreconstruct Z\\n        reconloss   tfreducemean tfsquarereconstruction   inputs\\n        selfaddloss 005  reconloss \\n        return selfoutZ\\nSimilarly you can add a custom metric based on model internals by computing it ', '     return selfoutZ\\nSimilarly you can add a custom metric based on model internals by computing it in\\nany way you want as long at the result is the output of a metric object For example\\nyou can create a kerasmetricsMean  object in the constructor then call it in the\\ncall  method passing it the reconloss  and finally add it to the model by calling\\nthe models addmetric  method This way when you train the model Keras will\\ndisplay both the mean loss over each epoch the loss is the sum of the main loss plus\\n005 times the reconstruction loss and the mean reconstruction error over each\\nepoch Both will go down during training\\nEpoch 15\\n1161011610   loss 43092  reconstructionerror 17360\\nEpoch 25\\n1161011610   loss 11232  reconstructionerror 08964\\n\\nIn over 99 of the cases everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build even with complex architectures los\\nses metrics and so on However in some rare cases you may need to customize the\\ntraining loo', 'ures los\\nses metrics and so on However in some rare cases you may need to customize the\\ntraining loop itself However before we get there we need to look at how to compute\\ngradients automatically in TensorFlow\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff see Chapter 10  and  to compute gradients\\nautomatically lets consider a simple toy function\\ndef fw1 w2\\n    return 3  w1  2  2  w1  w2\\nIf you know calculus you can analytically find that the partial derivative of this func\\ntion with regards to w1 is 6  w1   2  w2  Y ou can also find that its partial derivative\\nwith regards to w2 is 2  w1  For example at the point w1 w2   5 3  these par\\nCustomizing Models and Training Algorithms  389tial derivatives are equal to 36 and 10 respectively so the gradient vector at this point\\nis 36 10 But if this were a neural network the function would be much more com\\nplex typically with tens of thousands of parameters and finding the partial deriva\\ntives analytically by hand would be', 'h tens of thousands of parameters and finding the partial deriva\\ntives analytically by hand would be an almost impossible task One solution could be\\nto compute an approximation of each partial derivative by measuring how much the\\nfunctions output changes when you tweak the corresponding parameter\\n w1 w2  5 3\\n eps  1e6\\n fw1  eps w2  fw1 w2  eps\\n36000003007075065\\n fw1 w2  eps  fw1 w2  eps\\n10000000003174137\\nLooks about right This works rather well and it is trivial to implement but it is just\\nan approximation and importantly you need to call f at least once per parameter\\nnot twice since we could compute fw1 w2  just once This makes this approach\\nintractable for large neural networks So instead we should use autodiff see Chap\\nter 10  and  TensorFlow makes this pretty simple\\nw1 w2  tfVariable 5 tfVariable 3\\nwith tfGradientTape  as tape\\n    z  fw1 w2\\ngradients   tapegradient z w1 w2\\nWe first define two variables w1 and w2 then we create a tfGradientTape  context\\nthat will automatically recor', 'efine two variables w1 and w2 then we create a tfGradientTape  context\\nthat will automatically record every operation that involves a variable and finally we\\nask this tape to compute the gradients of the result z with regards to both variables\\nw1 w2  Lets take a look at the gradients that TensorFlow computed\\n gradients\\ntfTensor id828234 shape dtypefloat32 numpy360\\n tfTensor id828229 shape dtypefloat32 numpy100\\nPerfect Not only is the result accurate the precision is only limited by the floating\\npoint errors but the gradient  method only goes through the recorded computa\\ntions once in reverse order no matter how many variables there are so it is incredi\\nbly efficient Its like magic\\nOnly put the strict minimum inside the tfGradientTape  block\\nto save memory Alternatively you can pause recording by creating\\na with tapestoprecording  block inside the tfGradient\\nTape  block\\nThe tape is automatically erased immediately after you call its gradient  method so\\nyou will get an exception if you t', 'tically erased immediately after you call its gradient  method so\\nyou will get an exception if you try to call gradient  twice\\n390  Chapter 12 Custom Models and Training with TensorFlowwith tfGradientTape  as tape\\n    z  fw1 w2\\ndzdw1  tapegradient z w1   tensor 360\\ndzdw2  tapegradient z w2  RuntimeError\\nIf you need to call gradient  more than once you must make the tape persistent\\nand delete it when you are done with it to free resources\\nwith tfGradientTape persistent True as tape\\n    z  fw1 w2\\ndzdw1  tapegradient z w1   tensor 360\\ndzdw2  tapegradient z w2   tensor 100 works fine now\\ndel tape\\nBy default the tape will only track operations involving variables so if you try to\\ncompute the gradient of z with regards to anything else than a variable the result will\\nbe None \\nc1 c2  tfconstant 5 tfconstant 3\\nwith tfGradientTape  as tape\\n    z  fc1 c2\\ngradients   tapegradient z c1 c2  returns None None\\nHowever you can force the tape to watch any tensors you like to record every opera\\ntion tha', 'one None\\nHowever you can force the tape to watch any tensors you like to record every opera\\ntion that involves them Y ou can then compute gradients with regards to these ten\\nsors as if they were variables\\nwith tfGradientTape  as tape\\n    tapewatchc1\\n    tapewatchc2\\n    z  fc1 c2\\ngradients   tapegradient z c1 c2  returns tensor 36 tensor 10\\nThis can be useful in some cases for example if you want to implement a regulariza\\ntion loss that penalizes activations that vary a lot when the inputs vary little the loss\\nwill be based on the gradient of the activations with regards to the inputs Since the\\ninputs are not variables you would need to tell the tape to watch them\\nIf you compute the gradient of a list of tensors eg z1 z2 z3  with regards to\\nsome variables eg w1 w2  TensorFlow actually efficiently computes the sum of\\nthe gradients of these tensors ie gradientz1 w1 w2  plus gradientz2\\nw1 w2  plus gradientz3 w1 w2  Due to the way reversemode autodiff\\nworks it is not possible to compute the', '  plus gradientz3 w1 w2  Due to the way reversemode autodiff\\nworks it is not possible to compute the individual gradients  z1 z2 and z3 without\\nactually calling gradient  multiple times once for z1 once for z2 and once for z3\\nwhich requires making the tape persistent and deleting it afterwards\\nCustomizing Models and Training Algorithms  391Moreover it is actually possible to compute second order partial derivatives the Hes\\nsians ie the partial derivatives of the partial derivatives To do this we need to\\nrecord the operations that are performed when computing the firstorder partial\\nderivatives the Jacobians this requires a second tape Here is how it works\\nwith tfGradientTape persistent True as hessiantape \\n    with tfGradientTape  as jacobiantape \\n        z  fw1 w2\\n    jacobians   jacobiantape gradient z w1 w2\\nhessians   hessiantape gradient jacobian  w1 w2\\n            for jacobian  in jacobians \\ndel hessiantape\\nThe inner tape is used to compute the Jacobians as we did earlier The outer', 'cobians \\ndel hessiantape\\nThe inner tape is used to compute the Jacobians as we did earlier The outer tape is\\nused to compute the partial derivatives of each Jacobian Since we need to call gradi\\nent  once for each Jacobian or else we would get the sum of the partial derivatives\\nover all the Jabobians as explained earlier we need the outer tape to be persistent so\\nwe delete it at the end The Jacobians are obviously the same as earlier 36 and 5 but\\nnow we also have the Hessians\\n hessians   dzdw1dw1 dzdw1dw2 dzdw2dw1 dzdw2dw2\\ntfTensor id830578 shape dtypefloat32 numpy60\\n  tfTensor id830595 shape dtypefloat32 numpy20\\n tfTensor id830600 shape dtypefloat32 numpy20 None\\nLets verify these Hessians The first two are the partial derivatives of 6  w1   2  w2\\nwhich is as we saw earlier the partial derivative of f with regards to w1 with\\nregards to w1 and w2 The result is correct 6 for w1 and 2 for w2 The next two are the\\npartial derivatives of 2  w1  the partial derivative of f with regards to w2 w', ' The next two are the\\npartial derivatives of 2  w1  the partial derivative of f with regards to w2 with\\nregards to w1 and w2 which are 2 for w1 and 0 for w2 Note that TensorFlow returns\\nNone  instead of 0 since w2 does not appear at all in 2  w1  TensorFlow also returns\\nNone  when you use an operation whose gradients are not defined eg tfargmax \\nIn some rare cases you may want to stop gradients from backpropagating through\\nsome part of your neural network To do this you must use the tfstopgradient\\nfunction it just returns its inputs during the forward pass like tfidentity  but\\nit does not let gradients through during backpropagation it acts like a constant For\\nexample\\ndef fw1 w2\\n    return 3  w1  2  tfstopgradient 2  w1  w2\\nwith tfGradientTape  as tape\\n    z  fw1 w2  same result as without stopgradient\\ngradients   tapegradient z w1 w2   returns tensor 30 None\\n392  Chapter 12 Custom Models and Training with TensorFlowFinally you may occasionally run into some numerical issues when compu', 's and Training with TensorFlowFinally you may occasionally run into some numerical issues when computing gradi\\nents For example if you compute the gradients of the mysoftplus  function for\\nlarge inputs the result will be NaN\\n x  tfVariable 100\\n with tfGradientTape  as tape\\n     z  mysoftplus x\\n\\n tapegradient z x\\ntfTensor  numpyarraynan dtypefloat32\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties due to floating point precision errors autodiff ends up com\\nputing infinity divided by infinity which returns NaN Fortunately we can analyti\\ncally find that the derivative of the softplus function is just 1  1  1  expx which\\nis numerically stable Next we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the mysoftplus  function by decorating it with\\ntfcustomgradient  and making it return both its normal output and the function\\nthat computes the derivatives note that it will receive as input the gradients ', 'put and the function\\nthat computes the derivatives note that it will receive as input the gradients that were\\nbackpropagated so far down to the softplus function and according to the chain rule\\nwe should multiply them with this functions gradients\\ntfcustomgradient\\ndef mybettersoftplus z\\n    exp  tfexpz\\n    def mysoftplusgradients grad\\n        return grad  1  1  exp\\n    return tfmathlogexp  1 mysoftplusgradients\\nNow when we compute the gradients of the mybettersoftplus  function we get\\nthe proper result even for large input values however the main output still explodes\\nbecause of the exponential one workaround is to use tfwhere  to just return the\\ninputs when they are large\\nCongratulations Y ou can now compute the gradients of any function provided it is\\ndifferentiable at the point where you compute it you can even compute Hessians\\nblock backpropagation when needed and even write your own gradient functions\\nThis is probably more flexibility than you will ever need even if you build your', ' gradient functions\\nThis is probably more flexibility than you will ever need even if you build your own\\ncustom training loops as we will see now\\nCustom Training Loops\\nIn some rare cases the fit  method may not be flexible enough for what you need\\nto do For example the Wide and Deep paper we discussed in Chapter 10  actually\\nuses two different optimizers one for the wide path and the other for the deep path\\nSince the fit  method only uses one optimizer the one that we specify when\\nCustomizing Models and Training Algorithms  393compiling the model implementing this paper requires writing your own custom\\nloop\\nY ou may also like to write your own custom training loops simply to feel more confi\\ndent that it does precisely what you intent it to do perhaps you are unsure about\\nsome details of the fit  method It can sometimes feel safer to make everything\\nexplicit However remember that writing a custom training loop will make your code\\nlonger more error prone and harder to maintain\\nUnless you', 'a custom training loop will make your code\\nlonger more error prone and harder to maintain\\nUnless you really need the extra flexibility you should prefer using\\nthe fit  method rather than implementing your own training\\nloop especially if you work in a team\\nFirst lets build a simple model No need to compile it since we will handle the train\\ning loop manually\\nl2reg  kerasregularizers l2005\\nmodel  kerasmodelsSequential \\n    keraslayersDense30 activation elu kernelinitializer henormal \\n                       kernelregularizer l2reg\\n    keraslayersDense1 kernelregularizer l2reg\\n\\nNext lets create a tiny function that will randomly sample a batch of instances from\\nthe training set in Chapter 13  we will discuss the Data API which offers a much bet\\nter alternative\\ndef randombatch X y batchsize 32\\n    idx  nprandomrandintlenX sizebatchsize \\n    return Xidx yidx\\nLets also define a function that will display the training status including the number\\nof steps the total number of steps the mean loss ', 'l display the training status including the number\\nof steps the total number of steps the mean loss since the start of the epoch ie we\\nwill use the Mean  metric to compute it and other metrics\\ndef printstatusbar iteration  total loss metricsNone\\n    metrics    join 4f formatmname mresult\\n                         for m in loss  metrics or \\n    end   if iteration   total else n\\n    printr   formatiteration  total  metrics\\n          endend\\nThis code is selfexplanatory unless you are unfamiliar with Python string format\\nting 4f  will format a float with 4 digits after the decimal point Moreover using\\nr carriage return along with end  ensures that the status bar always gets printed\\non the same line In the notebook the printstatusbar  function also includes a\\nprogress bar but you could use the handy tqdm library instead\\n394  Chapter 12 Custom Models and Training with TensorFlowWith that lets get down to business First we need to define some hyperparameters\\nchoose the optimizer the loss funct', 'et down to business First we need to define some hyperparameters\\nchoose the optimizer the loss function and the metrics just the MAE in this exam\\nple\\nnepochs   5\\nbatchsize   32\\nnsteps  lenXtrain  batchsize\\noptimizer   kerasoptimizers Nadamlr001\\nlossfn  keraslossesmeansquarederror\\nmeanloss   kerasmetricsMean\\nmetrics  kerasmetricsMeanAbsoluteError \\nAnd now we are ready to build the custom loop\\nfor epoch in range1 nepochs   1\\n    printEpoch  formatepoch nepochs \\n    for step in range1 nsteps  1\\n        Xbatch ybatch  randombatch Xtrainscaled  ytrain\\n        with tfGradientTape  as tape\\n            ypred  modelXbatch training True\\n            mainloss   tfreducemean lossfnybatch ypred\\n            loss  tfaddnmainloss   modellosses\\n        gradients   tapegradient loss modeltrainablevariables \\n        optimizer applygradients zipgradients  modeltrainablevariables \\n        meanloss loss\\n        for metric in metrics\\n            metricybatch ypred\\n        printstatusbar step  batchsize  lenyt', '  for metric in metrics\\n            metricybatch ypred\\n        printstatusbar step  batchsize  lenytrain meanloss  metrics\\n    printstatusbar lenytrain lenytrain meanloss  metrics\\n    for metric in meanloss   metrics\\n        metricresetstates \\nTheres a lot going on in this code so lets walk through it\\nWe create two nested loops one for the epochs the other for the batches within\\nan epoch\\nThen we sample a random batch from the training set\\nInside the tfGradientTape  block we make a prediction for one batch using\\nthe model as a function and we compute the loss it is equal to the main loss\\nplus the other losses in this model there is one regularization loss per layer\\nSince the meansquarederror  function returns one loss per instance we\\ncompute the mean over the batch using tfreducemean  if you wanted to\\napply different weights to each instance this is where you would do it The regu\\nlarization losses are already reduced to a single scalar each so we just need to\\nsum them using tfaddn  whic', 'on losses are already reduced to a single scalar each so we just need to\\nsum them using tfaddn  which sums multiple tensors of the same shape\\nand data type\\nCustomizing Models and Training Algorithms  39511The truth is we did not process every single instance in the training set because we sampled instances ran\\ndomly so some were processed more than once while others were not processed at all In practice thats fine\\nMoreover if the training set size is not a multiple of the batch size we will miss a few instances\\n12Alternatively check out Klearningphase  Ksetlearningphase  and Klearningphasescope \\n13With the exception of optimizers as very few people ever customize these see the notebook for an exampleNext we ask the tape  to compute the gradient of the loss with regards to each\\ntrainable variable  not all variables and we apply them to the optimizer to per\\nform a Gradient Descent step\\nNext we update the mean loss and the metrics over the current epoch and we\\ndisplay the status bar\\nAt th', 't we update the mean loss and the metrics over the current epoch and we\\ndisplay the status bar\\nAt the end of each epoch we display the status bar again to make it look com\\nplete11 and to print a line feed and we reset the states of the mean loss and the\\nmetrics\\nIf you set the optimizers clipnorm  or clipvalue  hyperparameters it will take care of\\nthis for you If you want to apply any other transformation to the gradients simply do\\nso before calling the applygradients  method\\nIf you add weight constraints to your model eg by setting kernelconstraint  or\\nbiasconstraint  when creating a layer you should update the training loop to\\napply these constraints just after applygradients \\nfor variable  in modelvariables \\n    if variable constraint  is not None\\n        variable assignvariable constraint variable \\nMost importantly this training loop does not handle layers that behave differently\\nduring training and testing eg BatchNormalization  or Dropout  To handle these\\nyou need to call the mode', 'ng training and testing eg BatchNormalization  or Dropout  To handle these\\nyou need to call the model with trainingTrue  and make sure it propagates this to\\nevery layer that needs it12\\nAs you can see there are quite a lot of things you need to get right it is easy to make a\\nmistake But on the bright side you get full control so its your call\\nNow that you know how to customize any part of your models13 and training algo\\nrithms lets see how you can use TensorFlows automatic graph generation feature it\\ncan speed up your custom code considerably and it will also make it portable to any\\nplatform supported by TensorFlow see \\nTensorFlow Functions and Graphs\\nIn TensorFlow 1 graphs were unavoidable as were the complexities that came with\\nthem they were a central part of TensorFlows API In TensorFlow 2 they are still\\n396  Chapter 12 Custom Models and Training with TensorFlowthere but not as central and much much simpler to use To demonstrate this lets\\nstart with a trivial function that just comp', 'l and much much simpler to use To demonstrate this lets\\nstart with a trivial function that just computes the cube of its input\\ndef cubex\\n    return x  3\\nWe can obviously call this function with a Python value such as an int or a float or\\nwe can call it with a tensor\\n cube2\\n8\\n cubetfconstant 20\\ntfTensor id18634148 shape dtypefloat32 numpy80\\nNow lets use tffunction  to convert this Python function to a TensorFlow Func\\ntion\\n tfcube  tffunction cube\\n tfcube\\ntensorflowpythoneagerdeffunctionFunction at 0x1546fc080\\nThis TF Function can then be used exactly like the original Python function and it\\nwill return the same result but as tensors\\n tfcube2\\ntfTensor id18634201 shape dtypeint32 numpy8\\n tfcubetfconstant 20\\ntfTensor id18634211 shape dtypefloat32 numpy80\\nUnder the hood tffunction  analyzed the computations performed by the cube\\nfunction and generated an equivalent computation graph As you can see it was\\nrather painless we will see how this works shortly Alternatively we could have used\\ntff', 'n see it was\\nrather painless we will see how this works shortly Alternatively we could have used\\ntffunction  as a decorator this is actually more common\\ntffunction\\ndef tfcubex\\n    return x  3\\nThe original Python function is still available via the TF Functions pythonfunction\\nattribute in case you ever need it\\n tfcubepythonfunction 2\\n8\\nTensorFlow optimizes the computation graph pruning unused nodes simplifying\\nexpressions eg 1  2 would get replaced with 3 and more Once the optimized\\ngraph is ready the TF Function efficiently executes the operations in the graph in the\\nappropriate order and in parallel when it can As a result a TF Function will usually\\nrun much faster than the original Python function especially if it performs complex\\nTensorFlow Functions and Graphs  39714However in this trivial example the computation graph is so small that there is nothing at all to optimize so\\ntfcube  actually runs much slower than cube \\ncomputations14 Most of the time you will not really need to know', 'ctually runs much slower than cube \\ncomputations14 Most of the time you will not really need to know more than that\\nwhen you want to boost a Python function just transform it into a TF Function\\nThats all\\nMoreover when you write a custom loss function a custom metric a custom layer or\\nany other custom function and you use it in a Keras model as we did throughout\\nthis chapter Keras automatically converts your function into a TF Function no need\\nto use tffunction  So most of the time all this magic is 100 transparent\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamicTrue  when creating a custom layer\\nor a custom model Alternatively you can set runeagerlyTrue\\nwhen calling the models compile  method\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes and it caches it for subsequent calls For example if you call tfcubetfcon\\nstant10  a graph will be generated for int32 tensors of shape  Then if you call\\ntfcubetfconstan', 'fcon\\nstant10  a graph will be generated for int32 tensors of shape  Then if you call\\ntfcubetfconstant20  the same graph will be reused But if you then call\\ntfcubetfconstant10 20  a new graph will be generated for int32 tensors\\nof shape 2 This is how TF Functions handle polymorphism ie varying argument\\ntypes and shapes However this is only true for tensor arguments if you pass numer\\nical Python values to a TF Function a new graph will be generated for every distinct\\nvalue for example calling tfcube10  and tfcube20  will generate two graphs\\nIf you call a TF Function many times with different numerical\\nPython values then many graphs will be generated slowing down\\nyour program and using up a lot of RAM Python values should be\\nreserved for arguments that will have few unique values such as\\nhyperparameters like the number of neurons per layer This allows\\nTensorFlow to better optimize each variant of your model\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs Well first it starts ', 'ant of your model\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs Well first it starts by analyzing the Python\\nfunctions source code to capture all the control flow statements such as for loops\\nand while  loops if statements as well as break  continue  and return  statements\\nThis first step is called autograph  The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state\\nments it offers magic methods like add  or mul  to capture operators like\\n398  Chapter 12 Custom Models and Training with TensorFlow and  but there are no while  or if  magic methods After analyzing\\nthe functions code autograph outputs an upgraded version of that function in which\\nall the control flow statements are replaced by the appropriate TensorFlow opera\\ntions such as tfwhileloop  for loops and tfcond  for if statements For\\nexample in Figure 124  autograph analyzes the source code of the sumsquares\\nPython function and it generates th', 'Figure 124  autograph analyzes the source code of the sumsquares\\nPython function and it generates the tfsumsquares  function In this function\\nthe for loop is replaced by the definition of the loopbody  function containing\\nthe body of the original for loop followed by a call to the forstmt  function This\\ncall will build the appropriate tfwhileloop  operation in the computation graph\\nFigure 124 How TensorFlow generates graphs using autograph and tracing\\nNext TensorFlow calls this upgraded function but instead of passing the actual\\nargument it passes a symbolic tensor  meaning a tensor without any actual value only\\na name a data type and a shape For example if you call sumsquarestfcon\\nstant10  then the tfsumsquares  function will actually be called with a sym\\nbolic tensor of type int32 and shape  The function will run in graph mode  meaning\\nthat each TensorFlow operation will just add a node in the graph to represent itself\\nand its output tensors as opposed to the regular mode called eage', 'e in the graph to represent itself\\nand its output tensors as opposed to the regular mode called eager execution  or\\neager mode  In graph mode TF operations do not perform any actual computations\\nThis should feel familiar if you know TensorFlow 1 as graph mode was the default\\nmode In Figure 124  you can see the tfsumsquares  function being called with\\na symbolic tensor as argument in this case an int32 tensor of shape  and the final\\ngraph generated during tracing The ellipses represent operations and the arrows\\nrepresent tensors both the generated function and the graph are simplified\\nTensorFlow Functions and Graphs  399To view the generated functions source code you can call tfauto\\ngraphtocodesumsquarespythonfunction  The code is not\\nmeant to be pretty but it can sometimes help for debugging\\nTF Function Rules\\nMost of the time converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial just decorate it with tffunction  or let Keras take care\\nof it for ', 'ns\\ninto a TF Function is trivial just decorate it with tffunction  or let Keras take care\\nof it for you However there are a few rules to respect\\nIf you call any external library including NumPy or even the standard library\\nthis call will run only during tracing it will not be part of the graph Indeed a\\nTensorFlow graph can only include TensorFlow constructs tensors operations\\nvariables datasets and so on So make sure you use tfreducesum  instead of\\nnpsum  and tfsort  instead of the builtin sorted  function and so on\\nunless you really want the code to run only during tracing\\nFor example if you define a TF function fx  that just returns npran\\ndomrand  a random number will only be generated when the function is\\ntraced so ftfconstant2  and ftfconstant3  will return the\\nsame random number but ftfconstant2 3  will return a different\\none If you replace nprandomrand  with tfrandomuniform  then a\\nnew random number will be generated upon every call since the operation\\nwill be part of the graph\\nI', '\\nnew random number will be generated upon every call since the operation\\nwill be part of the graph\\nIf your nonTensorFlow code has sideeffects such as logging something or\\nupdating a Python counter then you should not expect that sideeffect to\\noccur every time you call the TF Function as it will only occur when the func\\ntion is traced\\nY ou can wrap arbitrary Python code in a tfpyfunction  operation but\\nthis will hinder performance as TensorFlow will not be able to do any graph\\noptimization on this code and it will also reduce portability as the graph will\\nonly run on platforms where Python is available and the right libraries\\ninstalled\\nY ou can call other Python functions or TF Functions but they should follow the\\nsame rules as TensorFlow will also capture their operations in the computation\\ngraph Note that these other functions do not need to be decorated with\\ntffunction \\nIf the function creates a TensorFlow variable or any other stateful TensorFlow\\nobject such as a dataset or a queue ', ' creates a TensorFlow variable or any other stateful TensorFlow\\nobject such as a dataset or a queue it must do so upon the very first call and\\nonly then or else you will get an exception It is usually preferable to create vari\\nables outside of the TF Function eg in the build  method of a custom layer\\n400  Chapter 12 Custom Models and Training with TensorFlowThe source code of your Python function should be available to TensorFlow If\\nthe source code is unavailable for example if you define your function in the\\nPython shell which does not give access to the source code or if you deploy only\\nthe compiled Python files pyc  to production then the graph generation pro\\ncess will fail or have limited functionality\\nTensorFlow will only capture for loops that iterate over a tensor or a Dataset  So\\nmake sure you use for i in tfrange10  rather than for i in range10  or\\nelse the loop will not be captured in the graph Instead it will run during tracing\\nThis may be what you want if the for loop is me', 'ured in the graph Instead it will run during tracing\\nThis may be what you want if the for loop is meant to build the graph for exam\\nple to create each layer in a neural network\\nAnd as always for performance reasons you should prefer a vectorized imple\\nmentation whenever you can rather than using loops\\nIts time to sum up In this chapter we started with a brief overview of TensorFlow\\nthen we looked at TensorFlows lowlevel API including tensors operations variables\\nand special data structures We then used these tools to customize almost every com\\nponent in tfkeras Finally we looked at how TF Functions can boost performance\\nhow graphs are generated using autograph and tracing and what rules to follow when\\nyou write TF Functions if you would like to open the black box a bit further for\\nexample to explore the generated graphs you will find further technical details\\nin \\nIn the next chapter we will look at how to efficiently load and preprocess data with\\nTensorFlow\\nTensorFlow Functions and Gra', 'ill look at how to efficiently load and preprocess data with\\nTensorFlow\\nTensorFlow Functions and Graphs  401CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 13 in the final\\nrelease of the book\\nSo far we have used only datasets that fit in memory but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries but TensorFlow makes it easy thanks to the Data API  you just create a data\\nset object tell it where to get the data then transform it in any way you want and\\nTensorFlow takes care of all the implementation details such as multithreading\\nqueuing batching prefetching and so on\\nOff the s', 'l the implementation details such as multithreading\\nqueuing batching prefetching and so on\\nOff the shelf the Data API can read from text files such as CSV files binary files\\nwith fixedsize records and binary files that use TensorFlows TFRecord format\\nwhich supports records of varying sizes TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers an open source binary format The Data API also\\nhas support for reading from SQL databases Moreover many Open Source exten\\nsions are available to read from all sorts of data sources such as Googles BigQuery\\nservice\\nHowever reading huge datasets efficiently is not the only difficulty the data also\\nneeds to be preprocessed Indeed it is not always composed strictly of convenient\\nnumerical fields sometimes there will be text features categorical features and so on\\nTo handle this TensorFlow provides the Features API  it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network For\\n403', 'ily convert these\\nfeatures to numerical features that can be consumed by your neural network For\\n403example categorical features with a large number of categories such as cities or\\nwords can be encoded using embeddings  as we will see an embedding is a trainable\\ndense vector that represents a category\\nBoth the Data API and the Features API work seamlessly with\\ntfkeras\\nIn this chapter we will cover the Data API the TFRecord format and the Features\\nAPI in detail We will also take a quick look at a few related projects from Tensor\\nFlows ecosystem\\nTF Transform  tfTransform  makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set before\\ntraining to speed it up and then exported to a TF Function and incorporated\\ninto your trained model so that once it is deployed in production it can take\\ncare of preprocessing new instances on the fly\\nTF Datasets TFDS provides a convenient function to download many common\\ndatasets of all kinds i', ' fly\\nTF Datasets TFDS provides a convenient function to download many common\\ndatasets of all kinds including large ones like ImageNet and it provides conve\\nnient dataset objects to manipulate them using the Data API\\nSo lets get started\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset  as you might suspect\\nthis represents a sequence of data items Usually you will use datasets that gradually\\nread data from disk but for simplicity lets just create a dataset entirely in RAM using\\ntfdataDatasetfromtensorslices \\n X  tfrange10   any data tensor\\n dataset  tfdataDatasetfromtensorslices X\\n dataset\\nTensorSliceDataset shapes  types tfint32\\nThe fromtensorslices  function takes a tensor and creates a tfdataDataset\\nwhose elements are all the slices of X along the first dimension so this dataset con\\ntains 10 items tensors 0 1 2  9 In this case we would have obtained the same\\ndataset if we had used tfdataDatasetrange10 \\nY ou can simply iterate over a datasets items like this\\n fo', 'set if we had used tfdataDatasetrange10 \\nY ou can simply iterate over a datasets items like this\\n for item in dataset\\n     printitem\\n404  Chapter 13 Loading and Preprocessing Data with TensorFlow\\ntfTensor0 shape dtypeint32\\ntfTensor1 shape dtypeint32\\ntfTensor2 shape dtypeint32\\n\\ntfTensor9 shape dtypeint32\\nChaining Transformations\\nOnce you have a dataset you can apply all sorts of transformations to it by calling its\\ntransformation methods Each method returns a new dataset so you can chain trans\\nformations like this this chain is illustrated in Figure 131 \\n dataset  datasetrepeat3batch7\\n for item in dataset\\n     printitem\\n\\ntfTensor0 1 2 3 4 5 6 shape7 dtypeint32\\ntfTensor7 8 9 0 1 2 3 shape7 dtypeint32\\ntfTensor4 5 6 7 8 9 0 shape7 dtypeint32\\ntfTensor1 2 3 4 5 6 7 shape7 dtypeint32\\ntfTensor8 9 shape2 dtypeint32\\nFigure 131 Chaining Dataset Transformations\\nIn this example we first call the repeat  method on the original dataset and it\\nreturns a new dataset that will repeat the items of the or', 'at  method on the original dataset and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times Of\\ncourse this will not copy the whole data in memory 3 times In fact if you call this\\nmethod with no arguments the new dataset will repeat the source dataset forever\\nThen we call the batch  method on this new dataset and again this creates a new\\ndataset This one will group the items of the previous dataset in batches of 7 items\\nFinally we iterate over the items of this final dataset As you can see the batch\\nmethod had to output a final batch of size 2 instead of 7 but you can call it with\\ndropremainderTrue  if you want it to drop this final batch so that all batches have\\nthe exact same size\\nThe Data API  405The dataset methods do not modify datasets they create new ones\\nso make sure to keep a reference to these new datasets eg data\\nset    or else nothing will happen\\nY ou can also apply any transformation you want to the items by calling the map\\nmethod For example ', '\\nY ou can also apply any transformation you want to the items by calling the map\\nmethod For example this creates a new dataset with all items doubled\\n dataset  datasetmaplambda x x  2  Items 024681012\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata Sometimes this will include computations that can be quite intensive such as\\nreshaping or rotating an image so you will usually want to spawn multiple threads to\\nspeed things up its as simple as setting the numparallelcalls  argument\\nWhile the map  applies a transformation to each item the apply  method applies a\\ntransformation to the dataset as a whole For example the following code unbatches\\nthe dataset by applying the unbatch  function to the dataset this function is cur\\nrently experimental but it will most likely move to the core API in a future release\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers\\n dataset  datasetapplytfdataexperimental unbatch  Ite', 'nteger tensor instead of a batch of 7\\nintegers\\n dataset  datasetapplytfdataexperimental unbatch  Items 024\\nIt is also possible to simply filter the dataset using the filter  method\\n dataset  datasetfilterlambda x x  10  Items 0 2 4 6 8 0 2 4 6\\nY ou will often want to look at just a few items from a dataset Y ou can use the take\\nmethod for that\\n for item in datasettake3\\n     printitem\\n\\ntfTensor0 shape dtypeint64\\ntfTensor2 shape dtypeint64\\ntfTensor4 shape dtypeint64\\nShuffling  the Data\\nAs you know Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed see Chapter 4  A simple way to ensure this\\nis to shuffle the instances For this you can just use the shuffle  method It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset then whenever it is asked for an item it will pull one out randomly\\nfrom the buffer and replace it with a fresh one from the source dataset until it has\\niterate', 'andomly\\nfrom the buffer and replace it with a fresh one from the source dataset until it has\\niterated entirely through the source dataset At this point it continues to pull out\\nitems randomly from the buffer until it is empty Y ou must specify the buffer size and\\n406  Chapter 13 Loading and Preprocessing Data with TensorFlow1Imagine a sorted deck of cards on your left suppose you just take the top 3 cards and shuffle them then pick\\none randomly and put it to your right keeping the other 2 in your hands Take another card on your left\\nshuffle the 3 cards in your hands and pick one of them randomly and put it on your right When you are\\ndone going through all the cards like this you will have a deck of cards on your right do you think it will be\\nperfectly shuffled\\nit is important to make it large enough or else shuffling will not be very efficient1\\nHowever obviously do not exceed the amount of RAM you have and even if you\\nhave plenty of it theres no need to go well beyond the datasets size', 'of RAM you have and even if you\\nhave plenty of it theres no need to go well beyond the datasets size Y ou can provide\\na random seed if you want the same random order every time you run your program\\n dataset  tfdataDatasetrange10repeat3  0 to 9 three times\\n dataset  datasetshufflebuffersize 5 seed42batch7\\n for item in dataset\\n     printitem\\n\\ntfTensor0 2 3 6 7 9 4 shape7 dtypeint64\\ntfTensor5 0 1 1 8 6 5 shape7 dtypeint64\\ntfTensor4 8 7 1 2 3 0 shape7 dtypeint64\\ntfTensor5 4 2 7 8 9 9 shape7 dtypeint64\\ntfTensor3 6 shape2 dtypeint64\\nIf you call repeat  on a shuffled dataset by default it will generate\\na new order at every iteration This is generally a good idea but if\\nyou prefer to reuse the same order at each iteration eg for tests\\nor debugging you can set reshuffleeachiterationFalse \\nFor a large dataset that does not fit in memory this simple shufflingbuffer approach\\nmay not be sufficient since the buffer will be small compared to the dataset One sol\\nution is to shuffle the source data its', 'nce the buffer will be small compared to the dataset One sol\\nution is to shuffle the source data itself for example on Linux you can shuffle text\\nfiles using the shuf  command This will definitely improve shuffling a lot However\\neven if the source data is shuffled you will usually want to shuffle it some more or\\nelse the same order will be repeated at each epoch and the model may end up being\\nbiased eg due to some spurious patterns present by chance in the source datas\\norder To shuffle the instances some more a common approach is to split the source\\ndata into multiple files then read them in a random order during training However\\ninstances located in the same file will still end up close to each other To avoid this\\nyou can pick multiple files randomly and read them simultaneously interleaving\\ntheir lines Then on top of that you can add a shuffling buffer using the shuffle\\nmethod If all this sounds like a lot of work dont worry the Data API actually makes\\nall this possible in just a few', 'his sounds like a lot of work dont worry the Data API actually makes\\nall this possible in just a few lines of code Lets see how to do this\\nThe Data API  407Interleaving Lines From Multiple Files\\nFirst lets suppose that you loaded the California housing dataset you shuffled it\\nunless it was already shuffled you split it into a training set a validation set and a\\ntest set then you split each set into many CSV files that each look like this each row\\ncontains 8 input features plus the target median house value\\nMedIncHouseAgeAveRoomsAveBedrmsPopulAveOccupLatLongMedianHouseValue\\n35214150304991106514470160593763122431442\\n5327550649000991034640344333369117391687\\n31290754231591513280225083844122981621\\n\\nLets also suppose trainfilepaths  contains the list of file paths and you also have\\nvalidfilepaths  and testfilepaths \\n trainfilepaths\\ndatasetshousingmytrain00csv datasetshousingmytrain01csv\\nNow lets create a dataset containing only these file paths\\nfilepathdataset   tfdataDatasetlistfiles trainf', 'ts create a dataset containing only these file paths\\nfilepathdataset   tfdataDatasetlistfiles trainfilepaths  seed42\\nBy default the listfiles  function returns a dataset that shuffles the file paths In\\ngeneral this is a good thing but you can set shuffleFalse  if you do not want that\\nfor some reason\\nNext we can call the interleave  method to read from 5 files at a time and inter\\nleave their lines skipping the first line of each file which is the header row using the\\nskip  method\\nnreaders   5\\ndataset  filepathdataset interleave \\n    lambda filepath  tfdataTextLineDataset filepath skip1\\n    cyclelength nreaders \\nThe interleave  method will create a dataset that will pull 5 file paths from the\\nfilepathdataset  and for each one it will call the function we gave it a lambda in\\nthis example to create a new dataset in this case a TextLineDataset  It will then\\ncycle through these 5 datasets reading one line at a time from each until all datasets\\nare out of items Then it will get the next 5 fil', 'ing one line at a time from each until all datasets\\nare out of items Then it will get the next 5 file paths from the filepathdataset  and\\ninterleave them the same way and so on until it runs out of file paths\\nFor interleaving to work best it is preferable to have files of identi\\ncal length or else the end of the longest files will not be interleaved\\n408  Chapter 13 Loading and Preprocessing Data with TensorFlowBy default interleave  does not use parallelism it just reads one line at a time\\nfrom each file sequentially However if you want it to actually read files in parallel\\nyou can set the numparallelcalls  argument to the number of threads you want\\nY ou can even set it to tfdataexperimentalAUTOTUNE  to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU however this is\\nan experimental feature for now Lets look at what the dataset contains now\\n for line in datasettake5\\n     printlinenumpy\\n\\nb420834405323209171846023370374712222782\\nb41812520570130996', 'ine in datasettake5\\n     printlinenumpy\\n\\nb420834405323209171846023370374712222782\\nb4181252057013099656920240273373118313215\\nb3687544045244099304570319583404118151625\\nb334563704514009084458032253366712172526\\nb35214150304991106514470160593763122431442\\nThese are the first rows ignoring the header row of 5 CSV files chosen randomly\\nLooks good But as you can see these are just byte strings we need to parse them\\nand also scale the data\\nPreprocessing the Data\\nLets implement a small function that will perform this preprocessing\\nXmean Xstd    mean and scale of each feature in the training set\\nninputs   8\\ndef preprocess line\\n  defs  0  ninputs   tfconstant  dtypetffloat32\\n  fields  tfiodecodecsv line recorddefaults defs\\n  x  tfstackfields1\\n  y  tfstackfields1\\n  return x  Xmean  Xstd y\\nLets walk through this code\\nFirst we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set Xmean  and Xstd  are just 1D tensors or NumPy\\narrays containing 8 floats one', 'ure in the training set Xmean  and Xstd  are just 1D tensors or NumPy\\narrays containing 8 floats one per input feature\\nThe preprocess  function takes one CSV line and starts by parsing it For this\\nit uses the tfiodecodecsv  function which takes two arguments the first is\\nthe line to parse and the second is an array containing the default value for each\\ncolumn in the CSV file This tells TensorFlow not only the default value for each\\ncolumn but also the number of columns and the type of each column In this\\nexample we tell it that all feature columns are floats and missing values should\\ndefault to 0 but we provide an empty array of type tffloat32  as the default\\nvalue for the last column the target this tells TensorFlow that this column con\\nThe Data API  409tains floats but that there is no default value so it will raise an exception if it\\nencounters a missing value\\nThe decodecsv  function returns a list of scalar tensors one per column but\\nwe need to return 1D tensor arrays So we call tf', 'returns a list of scalar tensors one per column but\\nwe need to return 1D tensor arrays So we call tfstack  on all tensors except\\nfor the last one the target this will stack these tensors into a 1D array We then\\ndo the same for the target value this makes it a 1D tensor array with a single\\nvalue rather than a scalar tensor\\nFinally we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations and we return a tuple containing the\\nscaled features and the target\\nLets test this preprocessing function\\n preprocess b420834405323209171846023370374712222782 \\ntfTensor id6227 shape8 dtypefloat32 numpy\\n array 016579159  1216324   005204564 039215982 05277444 \\n        02633488   08543046  13072058  dtypefloat32\\n tfTensor  numpyarray2782 dtypefloat32\\nWe can now apply this preprocessing function to the dataset\\nPutting Everything Together\\nTo make the code reusable lets put together everything we have discussed so far into\\na small helper function it wil', 'e reusable lets put together everything we have discussed so far into\\na small helper function it will create and return a dataset that will efficiently load Cal\\nifornia housing data from multiple CSV files then shuffle it preprocess it and batch it\\nsee Figure 132 \\ndef csvreaderdataset filepaths  repeatNone nreaders 5\\n                       nreadthreads None shufflebuffersize 10000\\n                       nparsethreads 5 batchsize 32\\n    dataset  tfdataDatasetlistfiles filepaths repeatrepeat\\n    dataset  datasetinterleave \\n        lambda filepath  tfdataTextLineDataset filepath skip1\\n        cyclelength nreaders  numparallelcalls nreadthreads \\n    dataset  datasetshuffleshufflebuffersize \\n    dataset  datasetmappreprocess  numparallelcalls nparsethreads \\n    dataset  datasetbatchbatchsize \\n    return datasetprefetch 1\\n410  Chapter 13 Loading and Preprocessing Data with TensorFlow2In general just prefetching one batch is fine but in some cases you may need to prefetch a few more Alterna\\nt', 'l just prefetching one batch is fine but in some cases you may need to prefetch a few more Alterna\\ntively you can let TensorFlow decide automatically by passing tfdataexperimentalAUTOTUNE  this is an\\nexperimental feature for now\\nFigure 132 Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code except the very last line  prefetch1 \\nwhich is actually quite important for performance\\nPrefetching\\nBy calling prefetch1  at the end we are creating a dataset that will do its best to\\nalways be one batch ahead2 In other words while our training algorithm is working\\non one batch the dataset will already be working in parallel on getting the next batch\\nready This can improve performance dramatically as is illustrated on Figure 133  If\\nwe also ensure that loading and preprocessing are multithreaded by setting numpar\\nallelcalls  when calling interleave  and map  we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter th', '  we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU this way the GPU will be almost 100 utilized except for\\nthe data transfer time from the CPU to the GPU and training will run much faster\\nThe Data API  411Figure 133 Speedup Training Thanks  to Prefetching and Multithreading\\nIf you plan to purchase a GPU card its processing power and its\\nmemory size are of course very important in particular a large\\nRAM is crucial for computer vision but its memory bandwidth  is\\njust as important as the processing power to get good performance\\nthis is the number of gigabytes of data it can get in or out of its\\nRAM per second\\nWith that you can now build efficient input pipelines to load and preprocess data\\nfrom multiple text files We have discussed the most common dataset methods but\\nthere are a few more you may want to look at concatenate  zip  window \\nreduce  cache  shard  flatmap  and paddedbatch  There are also a c', 'look at concatenate  zip  window \\nreduce  cache  shard  flatmap  and paddedbatch  There are also a cou\\nple more class methods fromgenerator  and fromtensors  which create a new\\ndataset from a Python generator or a list of tensors respectively Please check the API\\ndocumentation for more details Also note that there are experimental features avail\\nable in tfdataexperimental  many of which will most likely make it to the core\\nAPI in future releases eg check out the CsvDataset  class and the SqlDataset\\nclasses\\n412  Chapter 13 Loading and Preprocessing Data with TensorFlow3Support for datasets is specific to tfkeras it will not work on other implementations of the Keras API\\n4The number of steps per epoch is optional if the dataset just goes through the data once but if you do not\\nspecify it the progress bar will not be displayed during the first epoch\\n5Note that for now the dataset must be created within the TF Function This may be fixed by the time you read\\nthese lines see TensorFlow issue', 'eated within the TF Function This may be fixed by the time you read\\nthese lines see TensorFlow issue 25414Using the Dataset With tfkeras\\nNow we can use the csvreaderdataset  function to create a dataset for the train\\ning set ensuring it repeats the data forever the validation set and the test set\\ntrainset   csvreaderdataset trainfilepaths  repeatNone\\nvalidset   csvreaderdataset validfilepaths \\ntestset   csvreaderdataset testfilepaths \\nAnd now we can simply build and train a Keras model using these datasets3 All we\\nneed to do is to call the fit  method with the datasets instead of Xtrain  and\\nytrain  and specify the number of steps per epoch for each set4\\nmodel  kerasmodelsSequential \\nmodelcompile\\nmodelfittrainset  stepsperepoch lenXtrain  batchsize  epochs10\\n          validationdata validset \\n          validationsteps lenXvalid  batchsize \\nSimilarly we can pass a dataset to the evaluate  and predict  methods and again\\nspecify the number of steps per epoch\\nmodelevaluate testset  stepsle', ' and predict  methods and again\\nspecify the number of steps per epoch\\nmodelevaluate testset  stepslenXtest  batchsize \\nmodelpredictnewset stepslenXnew  batchsize \\nUnlike the other sets the newset  will usually not contain labels if it does Keras will\\njust ignore them Note that in all these cases you can still use NumPy arrays instead\\nof datasets if you want but of course they need to have been loaded and preprocessed\\nfirst\\nIf you want to build your own custom training loop as in Chapter 12  you can just\\niterate over the training set very naturally\\nfor Xbatch ybatch in trainset \\n      perform one gradient descent step\\nIn fact it is even possible to create a tffunction see Chapter 12  that performs the\\nwhole training loop5\\ntffunction\\ndef trainmodel optimizer  lossfn nepochs  \\n    trainset   csvreaderdataset trainfilepaths  repeatnepochs  \\n    for Xbatch ybatch in trainset \\n        with tfGradientTape  as tape\\nThe Data API  413            ypred  modelXbatch\\n            mainloss   tfreduce', 'radientTape  as tape\\nThe Data API  413            ypred  modelXbatch\\n            mainloss   tfreducemean lossfnybatch ypred\\n            loss  tfaddnmainloss   modellosses\\n        grads  tapegradient loss modeltrainablevariables \\n        optimizer applygradients zipgrads modeltrainablevariables \\nCongratulations you now know how to build powerful input pipelines using the Data\\nAPI However so far we have used CSV files which are common simple and conve\\nnient but they are not really efficient and they do not support large or complex data\\nstructures very well such as images or audio So lets use TFRecords instead\\nIf you are happy with CSV files or whatever other format you are\\nusing you do not have  to use TFRecords As the saying goes if it\\naint broke dont fix it TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data\\nThe TFRecord Format\\nThe TFRecord format is TensorFlows preferred format for storing large amounts of\\ndata and reading it efficiently It is a ve', 'TensorFlows preferred format for storing large amounts of\\ndata and reading it efficiently It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes each record just has a length a CRC\\nchecksum to check that the length was not corrupted then the actual data and finally\\na CRC checksum for the data Y ou can easily create a TFRecord file using the\\ntfioTFRecordWriter  class\\nwith tfioTFRecordWriter mydatatfrecord  as f\\n    fwritebThis is the first record \\n    fwritebAnd this is the second record \\nAnd you can then use a tfdataTFRecordDataset  to read one or more TFRecord\\nfiles\\nfilepaths   mydatatfrecord \\ndataset  tfdataTFRecordDataset filepaths \\nfor item in dataset\\n    printitem\\nThis will output\\ntfTensorbThis is the first record shape dtypestring\\ntfTensorbAnd this is the second record shape dtypestring\\nBy default a TFRecordDataset  will read files one by one but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num', ' one but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting numparallelreads  Alternatively you could\\nobtain the same result by using listfiles  and interleave\\nas we did earlier to read multiple CSV files\\n414  Chapter 13 Loading and Preprocessing Data with TensorFlow6Since protobuf objects are meant to be serialized and transmitted they are called messages Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files especially if they need to\\nbe loaded via a network connection Y ou can create a compressed TFRecord file by\\nsetting the options  argument\\noptions  tfioTFRecordOptions compressiontype GZIP\\nwith tfioTFRecordWriter mycompressedtfrecord  options as f\\n  \\nWhen reading a compressed TFRecord file you need to specify the compression type\\ndataset  tfdataTFRecordDataset mycompressedtfrecord \\n                                  compressiontype GZIP\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any bin', 'ompressiontype GZIP\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want TFRecord files usually\\ncontain serialized Protocol Buffers also called protobufs  This is a portable extensi\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008 and they are now widely used in particular in gRPC  Googles remote proce\\ndure call system Protocol Buffers are defined using a simple language that looks like\\nthis\\nsyntax  proto3 \\nmessage Person \\n  string name  1\\n  int32 id  2\\n  repeated  string email  3\\n\\nThis definition says we are using the protobuf format version 3 and it specifies that\\neach Person  object6 may optionally have a name  of type string  an id of type int32 \\nand zero or more email  fields each of type string  The numbers 1 2 and 3 are the\\nfield identifiers they will be used in each records binary representation Once you\\nhave a definition in a proto  file you can compile it This requires protoc  the proto\\nbu', 'on Once you\\nhave a definition in a proto  file you can compile it This requires protoc  the proto\\nbuf compiler to generate access classes in Python or some other language Note that\\nthe protobuf definitions we will use have already been compiled for you and their\\nPython classes are part of TensorFlow so you will not need to use protoc  All you\\nneed to know is how to use protobuf access classes in Python To illustrate the basics\\nlets look at a simple example that uses the access classes generated for the Person\\nprotobuf the code is explained in the comments\\n from personpb2  import Person   import the generated access class\\n person  PersonnameAl id123 emailabcom    create a Person\\n printperson   display the Person\\nThe TFRecord Format  4157This chapter contains the bare minimum you need to know about protobufs to use TFRecords To learn more\\nabout protobufs please visit httpshomlinfoprotobuf name Al\\nid 123\\nemail abcom\\n personname   read a field\\nAl\\n personname  Alice   modify a field\\n person', 'name Al\\nid 123\\nemail abcom\\n personname   read a field\\nAl\\n personname  Alice   modify a field\\n personemail0   repeated fields can be accessed like arrays\\nabcom\\n personemailappendcdcom    add an email address\\n s  personSerializeToString    serialize the object to a byte string\\n s\\nbnx05Alicex10x1ax07abcomx1ax07cdcom\\n person2  Person   create a new Person\\n person2ParseFromString s   parse the byte string 27 bytes long\\n27\\n person  person2   now they are equal\\nTrue\\nIn short we import the Person  class generated by protoc  we create an instance and\\nwe play with it visualizing it reading and writing some fields then we serialize it\\nusing the SerializeToString  method This is the binary data that is ready to be\\nsaved or transmitted over the network When reading or receiving this binary data\\nwe can parse it using the ParseFromString  method and we get a copy of the object\\nthat was serialized7\\nWe could save the serialized Person  object to a TFRecord file then we could load and\\nparse it everythin', 'ould save the serialized Person  object to a TFRecord file then we could load and\\nparse it everything would work fine However SerializeToString  and ParseFrom\\nString  are not TensorFlow operations and neither are the other operations in this\\ncode so they cannot be included in a TensorFlow Function except by wrapping\\nthem in a tfpyfunction  operation which would make the code slower and less\\nportable as we saw in Chapter 12  Fortunately TensorFlow does include special pro\\ntobuf definitions for which it provides parsing operations\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example  protobuf which\\nrepresents one instance in a dataset It contains a list of named features where each\\nfeature can either be a list of byte strings a list of floats or a list of integers Here is the\\nprotobuf definition\\nsyntax  proto3 \\nmessage BytesList   repeated  bytes value  1 \\nmessage FloatList   repeated  float value  1 packed  true \\nmessage Int64List   repeated  int64 val', ' \\nmessage FloatList   repeated  float value  1 packed  true \\nmessage Int64List   repeated  int64 value  1 packed  true \\n416  Chapter 13 Loading and Preprocessing Data with TensorFlow8Why was Example  even defined since it contains no more than a Features  object Well TensorFlow may one\\nday decide to add more fields to it As long as the new Example  definition still contains the features  field\\nwith the same id it will be backward compatible This extensibility is one of the great features of protobufsmessage Feature \\n    oneof kind \\n        BytesList  byteslist   1\\n        FloatList  floatlist   2\\n        Int64List  int64list   3\\n    \\n\\nmessage Features   mapstring Feature feature  1 \\nmessage Example  Features  features   1 \\nThe definitions of BytesList  FloatList  and Int64List  are straightforward enough\\npacked  true  is used for repeated numerical fields for a more efficient encod\\ning A Feature  either contains a BytesList  a FloatList  or an Int64List  A Fea\\ntures  with an s contains', ' Feature  either contains a BytesList  a FloatList  or an Int64List  A Fea\\ntures  with an s contains a dictionary that maps a feature name to the\\ncorresponding feature value And finally an Example  just contains a Features  object8\\nHere is how you could create a tftrainExample  representing the same person as\\nearlier and write it to TFRecord file\\nfrom tensorflowtrain  import BytesList  FloatList  Int64List\\nfrom tensorflowtrain  import Feature Features  Example\\npersonexample   Example\\n    features Features \\n        feature\\n            name Featurebyteslist BytesList valuebAlice\\n            id Featureint64list Int64List value123\\n            emails  Featurebyteslist BytesList valuebabcom \\n                                                          b cdcom \\n        \\nThe code is a bit verbose and repetitive but its rather straightforward and you could\\neasily wrap it inside a small helper function Now that we have an Example  protobuf\\nwe can serialize it by calling its SerializeToString  metho', 'on Now that we have an Example  protobuf\\nwe can serialize it by calling its SerializeToString  method then write the result\\ning data to a TFRecord file\\nwith tfioTFRecordWriter mycontactstfrecord  as f\\n    fwritepersonexample SerializeToString \\nNormally you would write much more than just one example Typically you would\\ncreate a conversion script that reads from your current format say CSV files creates\\nan Example  protobuf for each instance serializes them and saves them to several\\nTFRecord files ideally shuffling them in the process This requires a bit of work so\\nonce again make sure it is really necessary perhaps your pipeline works fine with\\nCSV files\\nThe TFRecord Format  417Now that we have a nice TFRecord file containing a serialized Example  lets try to\\nload it\\nLoading and Parsing Examples\\nTo load the serialized Example  protobufs we will use a tfdataTFRecordDataset\\nonce again and we will parse each Example  using tfioparsesingleexample \\nThis is a TensorFlow operation so it can b', 'we will parse each Example  using tfioparsesingleexample \\nThis is a TensorFlow operation so it can be included in a TF Function It requires at\\nleast two arguments a string scalar tensor containing the serialized data and a\\ndescription of each feature The description is a dictionary that maps each feature\\nname to either a tfioFixedLenFeature  descriptor indicating the features shape\\ntype and default value or a tfioVarLenFeature  descriptor indicating only the type\\nif the length may vary such as for the emails  feature For example\\nfeaturedescription   \\n    name tfioFixedLenFeature  tfstring defaultvalue \\n    id tfioFixedLenFeature  tfint64 defaultvalue 0\\n    emails  tfioVarLenFeature tfstring\\n\\nfor serializedexample  in tfdataTFRecordDataset mycontactstfrecord \\n    parsedexample   tfioparsesingleexample serializedexample \\n                                                featuredescription \\nThe fixed length features are parsed as regular tensors but the variable length fea\\ntures are parsed ', 'he fixed length features are parsed as regular tensors but the variable length fea\\ntures are parsed as sparse tensors Y ou can convert a sparse tensor to a dense tensor\\nusing tfsparsetodense  but in this case it is simpler to just access its values\\n tfsparsetodense parsedexample emails  defaultvalue b\\ntfTensor  dtypestring numpyarraybabcom bcdcom \\n parsedexample emails values\\ntfTensor  dtypestring numpyarraybabcom bcdcom \\nA BytesList  can contain any binary data you want including any serialized object\\nFor example you can use tfioencodejpeg  to encode an image using the JPEG\\nformat and put this binary data in a BytesList  Later when your code reads the\\nTFRecord it will start by parsing the Example  then you will need to call\\ntfiodecodejpeg  to parse the data and get the original image or you can use\\ntfiodecodeimage  which can decode any BMP  GIF JPEG or PNG image Y ou\\ncan also store any tensor you want in a BytesList  by serializing the tensor using\\ntfioserializetensor  then putting th', 'tensor you want in a BytesList  by serializing the tensor using\\ntfioserializetensor  then putting the resulting byte string in a BytesList\\nfeature Later when you parse the TFRecord you can parse this data using\\ntfioparsetensor \\nInstead of parsing examples one by one using tfioparsesingleexample  you\\nmay want to parse them batch by batch using tfioparseexample \\n418  Chapter 13 Loading and Preprocessing Data with TensorFlowdataset  tfdataTFRecordDataset mycontactstfrecord batch10\\nfor serializedexamples  in dataset\\n    parsedexamples   tfioparseexample serializedexamples \\n                                          featuredescription \\nAs you can see the Example  proto will probably be sufficient for most use cases\\nHowever it may be a bit cumbersome to use when you are dealing with lists of lists\\nFor example suppose you want to classify text documents Each document may be\\nrepresented as a list of sentences where each sentence is represented as a list of\\nwords And perhaps each document also h', ' of sentences where each sentence is represented as a list of\\nwords And perhaps each document also has a list of comments where each com\\nment is also represented as a list of words Moreover there may be some contextual\\ndata as well such as the documents author title and publication date TensorFlows\\nSequenceExample  protobuf is designed for such use cases\\nHandling Lists of Lists Using the SequenceExample  Protobuf\\nHere is the definition of the SequenceExample  protobuf\\nmessage FeatureList   repeated  Feature feature  1 \\nmessage FeatureLists   mapstring FeatureList  featurelist   1 \\nmessage SequenceExample  \\n    Features  context  1\\n    FeatureLists  featurelists   2\\n\\nA SequenceExample  contains a Features  object for the contextual data and a Fea\\ntureLists  object which contains one or more named FeatureList  objects eg a\\nFeatureList  named content  and another named comments  Each FeatureList\\njust contains a list of Feature  objects each of which may be a list of byte strings a list\\nof', 'eList\\njust contains a list of Feature  objects each of which may be a list of byte strings a list\\nof 64bit integers or a list of floats in this example each Feature  would represent a\\nsentence or a comment perhaps in the form of a list of word identifiers Building a\\nSequenceExample  serializing it and parsing it is very similar to building serializing\\nand parsing an Example  but you must use tfioparsesinglesequenceexam\\nple  to parse a single SequenceExample  or tfioparsesequenceexample  to\\nparse a batch and both functions return a tuple containing the context features as a\\ndictionary and the feature lists also as a dictionary If the feature lists contain\\nsequences of varying sizes as in the example above you may want to convert them\\nto ragged tensors using tfRaggedTensorfromsparse  see the notebook for the\\nfull code\\nparsedcontext  parsedfeaturelists   tfioparsesinglesequenceexample \\n    serializedsequenceexample  contextfeaturedescriptions \\n    sequencefeaturedescriptions \\nparsedconten', 'serializedsequenceexample  contextfeaturedescriptions \\n    sequencefeaturedescriptions \\nparsedcontent   tfRaggedTensor fromsparse parsedfeaturelists content \\nNow that you know how to efficiently store load and parse data the next step is to\\nprepare it so that it can be fed to a neural network This means converting all features\\nThe TFRecord Format  419into numerical features ideally not too sparse scaling them and more In particular\\nif your data contains categorical features or text features they need to be converted to\\nnumbers For this the Features API  can help\\nThe Features API\\nPreprocessing your data can be performed in many ways it can be done ahead of\\ntime when preparing your data files using any tool you like Or you can preprocess\\nyour data on the fly when loading it with the Data API eg using the datasets map\\nmethod as we saw earlier Or you can include a preprocessing layer directly in your\\nmodel Whichever solution you prefer the Features API can help you it is a set of\\nfunctions', 'y in your\\nmodel Whichever solution you prefer the Features API can help you it is a set of\\nfunctions available in the tffeaturecolumn  package which let you define how\\neach feature or group of features in your data should be preprocessed therefore you\\ncan think of this API as the analog of ScikitLearns ColumnTransformer  class We\\nwill start by looking at the different types of columns available and then we will look\\nat how to use them\\nLets go back to the variant of the California housing dataset that we used in Chap\\nter 2  since it includes a categorical feature and missing data Here is a simple numeri\\ncal column named housingmedianage \\nhousingmedianage   tffeaturecolumn numericcolumn housingmedianage \\nNumeric columns let you specify a normalization function using the normalizerfn\\nargument For example lets tweak the housingmedianage  column to define how\\nit should be scaled Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training s', 's requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set\\nagemean  agestd  Xmean1 Xstd1   The median age is column in 1\\nhousingmedianage   tffeaturecolumn numericcolumn \\n    housingmedianage  normalizerfn lambda x x  agemean   agestd\\nIn some cases it might improve performance to bucketize some numerical features\\neffectively transforming a numerical feature into a categorical feature For example\\nlets create a bucketized column based on the medianincome  column with 5 buckets\\nless than 15 15000 then 15 to 3 3 to 45 45 to 6 and above 6 notice that when\\nyou specify 4 boundaries there are actually 5 buckets\\nmedianincome   tffeaturecolumn numericcolumn medianincome \\nbucketizedincome   tffeaturecolumn bucketizedcolumn \\n    medianincome  boundaries 15 3 45 6\\nIf the medianincome  feature is equal to say 32 then the bucketizedincome  feature\\nwill automatically be equal to 2 ie the index of the corresponding income bucket\\nChoosing the right boundaries ', 'tically be equal to 2 ie the index of the corresponding income bucket\\nChoosing the right boundaries can be somewhat of an art but one approach is to just\\nuse percentiles of the data eg the 10th percentile the 20th percentile and so on If\\na feature is multimodal  meaning it has separate peaks in its distribution you may\\n420  Chapter 13 Loading and Preprocessing Data with TensorFlowwant to define a bucket for each mode placing the boundaries in between the peaks\\nWhether you use the percentiles or the modes you need to analyze the distribution of\\nyour data ahead of time just like we had to measure the mean and standard deviation\\nahead of time to normalize the housingmedianage  column\\nCategorical Features\\nFor categorical features such as oceanproximity  there are several options If it is\\nalready represented as a category ID ie an integer from 0 to the max ID then you\\ncan use the categoricalcolumnwithidentity  function specifying the max\\nID If not and you know the list of all possible categ', 'olumnwithidentity  function specifying the max\\nID If not and you know the list of all possible categories then you can use categori\\ncalcolumnwithvocabularylist \\noceanproxvocab   1H OCEAN  INLAND  ISLAND  NEAR BAY  NEAR OCEAN \\noceanproximity   tffeaturecolumn categoricalcolumnwithvocabularylist \\n    oceanproximity  oceanproxvocab \\nIf you prefer to have TensorFlow load the vocabulary from a file you can call catego\\nricalcolumnwithvocabularyfile  instead As you might expect these two\\nfunctions will simply map each category to its index in the vocabulary eg NEAR\\nBAY  will be mapped to 3 and unknown categories will be mapped to 1\\nFor categorical columns with a large vocabulary eg for zipcodes cities words\\nproducts users etc it may not be convenient to get the full list of possible cate\\ngories or perhaps categories may be added or removed so frequently that using cate\\ngory indices would be too unreliable In this case you may prefer to use a\\ncategoricalcolumnwithhashbucket  If we had a city  ', ' unreliable In this case you may prefer to use a\\ncategoricalcolumnwithhashbucket  If we had a city  feature in the dataset\\nwe could encode it like this\\ncityhash   tffeaturecolumn categoricalcolumnwithhashbucket \\n    city hashbucketsize 1000\\nThis feature will compute a hash for each category ie for each city modulo the\\nnumber of hash buckets  hashbucketsize  Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions ie different categories ending up\\nin the same bucket but the higher you set it the more RAM will be used by the\\nembedding table as we will see shortly\\nCrossed Categorical Features\\nIf you suspect that two or more categorical features are more meaningful when used\\njointly then you can create a crossed column  For example suppose people are partic\\nularly fond of old houses inland and new houses near the ocean then it might help to\\nThe Features API  4219Since the housingmedianage  feature was normalized the boundaries are for normalized agescreate a buc', 'ince the housingmedianage  feature was normalized the boundaries are for normalized agescreate a bucketized column for the housingmedianage  feature9 and cross it with\\nthe oceanproximity  column The crossed column will compute a hash of every age\\n ocean proximity combination it comes across modulo the hashbucketsize  and\\nthis will give it the cross category ID Y ou may then choose to use only this crossed\\ncolumn in your model or also include the individual columns\\nbucketizedage   tffeaturecolumn bucketizedcolumn \\n    housingmedianage  boundaries 1 05 0 05 1  age was scaled\\nageandoceanproximity   tffeaturecolumn crossedcolumn \\n    bucketizedage  oceanproximity  hashbucketsize 100\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature you start by bucketizing the latitude and longitude for\\nexample into 20 buckets each then you cross these bucketized features into a loca\\ntion  column This will create a 2020 grid over California and', ' these bucketized features into a loca\\ntion  column This will create a 2020 grid over California and each cell in the grid\\nwill correspond to one category\\nlatitude   tffeaturecolumn numericcolumn latitude \\nlongitude   tffeaturecolumn numericcolumn longitude \\nbucketizedlatitude   tffeaturecolumn bucketizedcolumn \\n    latitude  boundaries listnplinspace 32 42 20  1\\nbucketizedlongitude   tffeaturecolumn bucketizedcolumn \\n    longitude  boundaries listnplinspace 125 114 20  1\\nlocation   tffeaturecolumn crossedcolumn \\n    bucketizedlatitude  bucketizedlongitude  hashbucketsize 1000\\nEncoding Categorical Features Using OneHot Vectors\\nNo matter which option you choose to build a categorical feature categorical col\\numns bucketized columns or crossed columns it must be encoded before you can\\nfeed it to a neural network There are two options to encode a categorical feature\\nonehot vectors or embeddings  For the first option simply use the indicatorcol\\numn  function\\noceanproximityonehot   tffeature', 'ngs  For the first option simply use the indicatorcol\\numn  function\\noceanproximityonehot   tffeaturecolumn indicatorcolumn oceanproximity \\nA onehot vector encoding has the size of the vocabulary length which is fine if there\\nare just a few possible categories but if the vocabulary is large you will end up with\\ntoo many inputs fed to your neural network it will have too many weights to learn\\nand it will probably not perform very well In particular this will typically be the case\\nwhen you use hash buckets In this case you should probably encode them using\\nembeddings  instead\\n422  Chapter 13 Loading and Preprocessing Data with TensorFlowAs a rule of thumb but your mileage may vary if the number of\\ncategories is lower than 10 then onehot encoding is generally the\\nway to go If the number of categories is greater than 50 which is\\noften the case when you use hash buckets then embeddings are\\nusually preferable In between 10 and 50 categories you may want\\nto experiment with both options and see', 'ally preferable In between 10 and 50 categories you may want\\nto experiment with both options and see which one works best for\\nyour use case Also embeddings typically require more training\\ndata unless you can reuse pretrained embeddings\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category By default\\nembeddings are initialized randomly so for example the NEAR BAY  category could\\nbe represented initially by a random vector such as 0131 0890  while the NEAR\\nOCEAN  category may be represented by another random vector such as 0631\\n0791  in this example we are using 2D embeddings but the number of dimensions\\nis a hyperparameter you can tweak Since these embeddings are trainable they will\\ngradually improve during training and as they represent fairly similar categories\\nGradient Descent will certainly end up pushing them closer together while it will\\ntend to move them away from the INLAND  categorys embedding see Figure 134 \\nIndeed t', 'r while it will\\ntend to move them away from the INLAND  categorys embedding see Figure 134 \\nIndeed the better the representation the easier it will be for the neural network to\\nmake accurate predictions so training tends to make embeddings useful representa\\ntions of the categories This is called representation learning  we will see other types of\\nrepresentation learning in \\nThe Features API  42310Distributed Representations of Words and Phrases and their Compositionality  T Mikolov et al 2013\\nFigure 134 Embeddings Will Gradually Improve During Training\\nWord Embeddings\\nNot only will embeddings generally be useful representations for the task at hand but\\nquite often these same embeddings can be reused successfully for other tasks as well\\nThe most common example of this is word embeddings  ie embeddings of individual\\nwords when you are working on a natural language processing task you are often\\nbetter off reusing pretrained word embeddings than training your own The idea of\\nusing vectors ', 'ften\\nbetter off reusing pretrained word embeddings than training your own The idea of\\nusing vectors to represent words dates back to the 1960s and many sophisticated\\ntechniques have been used to generate useful vectors including using neural net\\nworks but things really took off in 2013 when Tom Mikolov and other Google\\nresearchers published a paper10 describing how to learn word embeddings using deep\\nneural networks much faster than previous attempts This allowed them to learn\\nembeddings on a very large corpus of text they trained a deep neural network to pre\\ndict the words near any given word This allowed them to obtain astounding word\\nembeddings For example synonyms had very close embeddings and semantically\\nrelated words such as France Spain Italy and so on ended up clustered together But\\nits not just about proximity word embeddings were also organized along meaningful\\naxes in the embedding space Here is a famous example if you compute King  Man\\n Woman adding and subtracting the emb', 'edding space Here is a famous example if you compute King  Man\\n Woman adding and subtracting the embedding vectors of these words then the\\nresult will be very close to the embedding of the word Queen see Figure 135  In\\nother words the word embeddings encode the concept of gender Similarly you can\\ncompute Madrid  Spain  France and of course the result is close to Paris which\\nseems to show that the notion of capital city was also encoded in the embeddings\\n424  Chapter 13 Loading and Preprocessing Data with TensorFlowFigure 135 Word Embeddings\\nLets go back to the Features API Here is how you could encode the oceanproxim\\nity categories as 2D embeddings\\noceanproximityembed   tffeaturecolumn embeddingcolumn oceanproximity \\n                                                           dimension 2\\nEach of the five oceanproximity  categories will now be represented as a 2D vector\\nThese vectors are stored in an embedding matrix  with one row per category and one\\ncolumn per embedding dimension so in', 'tored in an embedding matrix  with one row per category and one\\ncolumn per embedding dimension so in this example it is a 52 matrix When an\\nembedding column is given a category index as input say 3 which corresponds to\\nthe category NEAR BAY  it just performs a lookup in the embedding matrix and\\nreturns the corresponding row say 0331 0190  Unfortunately the embedding\\nmatrix can be quite large especially when you have a large vocabulary if this is the\\ncase the model can only learn good representations for the categories for which it has\\nsufficient training data To reduce the size of the embedding matrix you can of\\ncourse try lowering the dimension  hyperparameter but if you reduce this parameter\\ntoo much the representations may not be as good Another option is to reduce the\\nvocabulary size eg if you are dealing with text you can try dropping the rare words\\nfrom the vocabulary and replace them all with a token like unknown  or UNK \\nIf you are using hash buckets you can also try reducing t', 'hem all with a token like unknown  or UNK \\nIf you are using hash buckets you can also try reducing the hashbucketsize  but\\nnot too much or else you will get collisions\\nThe Features API  425If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle and if you do not have enough train\\ning data to learn them then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data\\nAfter that you can reuse the trained embeddings for your main\\ntask\\nUsing Feature Columns for Parsing\\nLets suppose you have created feature columns for each of your input features as well\\nas for the target What can you do with them Well for one you can pass them to the\\nmakeparseexamplespec  function to generate feature descriptions so you dont\\nhave to do it manually as we did earlier\\ncolumns  bucketizedage   medianhousevalue   all features  target\\nfeaturedescriptions   tffeaturecolumn makeparseexamplespec columns\\nY ou dont always have to cre', 'rget\\nfeaturedescriptions   tffeaturecolumn makeparseexamplespec columns\\nY ou dont always have to create a separate feature column for each\\nand every feature For example instead of having 2 numerical fea\\nture columns you could choose to have a single 2D column just\\nset shape2  when calling numericalcolumn \\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions and separates the target column from the input features\\ndef parseexamples serializedexamples \\n    examples   tfioparseexample serializedexamples  featuredescriptions \\n    targets  examples popmedianhousevalue   separate the targets\\n    return examples  targets\\nNext you can create a TFRecordDataset  that will read batches of serialized examples\\nassuming the TFRecord file contains serialized Example  protobufs with the appropri\\nate features\\nbatchsize   32\\ndataset  tfdataTFRecordDataset mydatawithfeaturestfrecords \\ndataset  datasetrepeatshuffle10000batchbatchsize mapparseexamples \\nUsing Featu', 'ithfeaturestfrecords \\ndataset  datasetrepeatshuffle10000batchbatchsize mapparseexamples \\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model to convert all your input\\nfeatures into a single dense vector which the neural network can then process For\\nthis all you need to do is add a keraslayersDenseFeatures  layer as the first layer\\nin your model passing it the list of feature columns excluding the target column\\ncolumnswithouttarget   columns1\\nmodel  kerasmodelsSequential \\n    keraslayersDenseFeatures featurecolumns columnswithouttarget \\n426  Chapter 13 Loading and Preprocessing Data with TensorFlow    keraslayersDense1\\n\\nmodelcompilelossmse optimizer sgd metricsaccuracy \\nstepsperepoch   lenXtrain  batchsize\\nhistory  modelfitdataset stepsperepoch stepsperepoch  epochs5\\nThe DenseFeatures  layer will take care of converting every input feature to a dense\\nrepresentation and it will also apply any extra transformation we specified such as\\nscaling the h', 'se\\nrepresentation and it will also apply any extra transformation we specified such as\\nscaling the housingmedianage  using the normalizerfn  function we provided Y ou\\ncan take a closer look at what the DenseFeatures  layer does by calling it directly\\n somecolumns   oceanproximityembed  bucketizedincome \\n densefeatures   keraslayersDenseFeatures somecolumns \\n densefeatures \\n     oceanproximity  NEAR OCEAN  INLAND  INLAND \\n     medianincome  3 72 1\\n \\n\\ntfTensor id559790 shape3 7 dtypefloat32 numpy\\narray 0  0  1  0  0 036277947  030109018\\n        0  0  0  0  1  022548223  033142096\\n        1  0  0  0  0  022548223  033142096 dtypefloat32\\nIn this example we create a DenseFeatures  layer with just two columns and we call\\nit with some data in the form of a dictionary of features In this case since the bucke\\ntizedincome  column relies on the medianincome  column the dictionary must\\ninclude the medianincome  key and similarly since the oceanproximityembed\\ncolumn is based on the oceanproximity  ', 'edianincome  key and similarly since the oceanproximityembed\\ncolumn is based on the oceanproximity  column the dictionary must include the\\noceanproximity  key Columns are handled in alphabetical order so first we look\\nat the bucketized income column its name is the same as the medianincome  column\\nname plus bucketized  The incomes 3 72 and 1 get mapped respectively to cat\\negory 2 for incomes between 15 and 3 category 0 for incomes below 15 and cat\\negory 4 for incomes greater than 6 Then these category IDs get onehot encoded\\ncategory 2 gets encoded as 0 0 1 0 0  and so on note that bucketized\\ncolumns get onehot encoded by default no need to call indicatorcolumn  Now\\non to the oceanproximityembed  column The NEAR OCEAN  and INLAND  cate\\ngories just get mapped to their respective embeddings which were initialized ran\\ndomly The resulting tensor is the concatenation of the onehot vectors and the\\nembeddings\\nNow you can feed all kinds of features to a neural network including numerical fea\\ntu', 'the\\nembeddings\\nNow you can feed all kinds of features to a neural network including numerical fea\\ntures categorical features and even text by splitting the text into words then using\\nword embedding However performing all the preprocessing on the fly can slow\\ndown training Lets see how this can be improved\\nThe Features API  427TF Transform\\nIf preprocessing is computationally expensive then handling it before training rather\\nthan on the fly may give you a significant speedup the data will be preprocessed just\\nonce per instance before  training rather than once per instance and per epoch during\\ntraining Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data even distributed across multiple servers so why not use it to\\npreprocess all the training data This works great and indeed can speed up training\\nbut there is one problem once your model is trained suppose you want to deploy it\\nto a mobile app you will need to write some code in your app to tak', 'ed suppose you want to deploy it\\nto a mobile app you will need to write some code in your app to take care of prepro\\ncessing the data before it is fed to the model And suppose you also want to deploy\\nthe model to TensorFlowjs so it runs in a web browser Once again you will need to\\nwrite some preprocessing code This can become a maintenance nightmare when\\never you want to change the preprocessing logic you will need to update your Apache\\nBeam code your mobile app code and your Javascript code It is not only time con\\nsuming but also error prone you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser This trainingserving skew  will lead to bugs or degraded perfor\\nmance\\nOne improvement would be to take the trained model trained on data that was pre\\nprocessed by your Apache Beam code and before deploying it to your app or the\\nbrowser add an extra input layer to take care of preprocessing on th', 'deploying it to your app or the\\nbrowser add an extra input layer to take care of preprocessing on the fly either by\\nwriting a custom layer or by using a DenseFeatures  layer Thats definitely better\\nsince now you just have two versions of your preprocessing code the Apache Beam\\ncode and the preprocessing layers code\\nBut what if you could define your preprocessing operations just once This is what\\nTF Transform was designed for It is part of TensorFlow Extended  TFX an endto\\nend platform for productionizing TensorFlow models First to use a TFX component\\nsuch as TF Transform you must install it it does not come bundled with TensorFlow\\nY ou define your preprocessing function just once in Python by using TF Transform\\nfunctions for scaling bucketizing crossing features and more Y ou can also use any\\nTensorFlow operation you need Here is what this preprocessing function might look\\nlike if we just had two features\\nimport tensorflowtransform  as tft\\ndef preprocess inputs   inputs is a batch of i', 't had two features\\nimport tensorflowtransform  as tft\\ndef preprocess inputs   inputs is a batch of input features\\n    medianage   inputshousingmedianage \\n    oceanproximity   inputsoceanproximity \\n    standardizedage   tftscaletozscore medianage   tftmeanmedianage \\n    oceanproximityid   tftcomputeandapplyvocabulary oceanproximity \\n    return \\n        standardizedmedianage  standardizedage \\n428  Chapter 13 Loading and Preprocessing Data with TensorFlow11At the time of writing TFDS requires you to download a few files manually for ImageNet for legal reasons\\nbut this will hopefully get resolved soon\\n        oceanproximityid  oceanproximityid\\n    \\nNext TF Transform lets you apply this preprocess  function to the whole training\\nset using Apache Beam it provides an AnalyzeAndTransformDataset  class that you\\ncan use for this purpose in your Apache Beam pipeline In the process it will also\\ncompute all the necessary statistics over the whole training set in this example the\\nmean and standard d', 'ute all the necessary statistics over the whole training set in this example the\\nmean and standard deviation of the housingmedianage  feature and the vocabulary\\nfor the oceanproximity  feature The components that compute these statistics are\\ncalled analyzers \\nImportantly TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy This TF Function contains all the necessary\\nstatistics computed by Apache Beam the mean standard deviation and vocabulary\\nsimply included as constants\\nAt the time of this writing TF Transform only supports Tensor\\nFlow 1 Moreover Apache Beam only has partial support for\\nPython 3 That said both these limitations will likely be fixed by\\nthe time your read this\\nWith the Data API TFRecords the Features API and TF Transform you can build\\nhighly scalable input pipelines for training and also benefit from fast and portable\\ndata preprocessing in production\\nBut what if you just wanted to use a standard dataset Well in th', 'le\\ndata preprocessing in production\\nBut what if you just wanted to use a standard dataset Well in that case things are\\nmuch simpler just use TFDS\\nThe TensorFlow Datasets TFDS Project\\nThe TensorFlow Datasets  project makes it trivial to download common datasets from\\nsmall ones like MNIST or Fashion MNIST to huge datasets like ImageNet11 you will\\nneed quite a bit of disk space The list includes image datasets text datasets includ\\ning translation datasets audio and video datasets and more Y ou can visit https\\nhomlinfotfds  to view the full list along with a description of each dataset\\nTFDS is not bundled with TensorFlow so you need to install the tensorflow\\ndatasets  library eg using pip Then all you need to do is call the tfdsload\\nfunction and it will download the data you want unless it was already downloaded\\nearlier and return the data as a dictionary of Datasets  typically one for training\\nThe TensorFlow Datasets TFDS Project  429and one for testing but this depends on the dataset you', 'ing\\nThe TensorFlow Datasets TFDS Project  429and one for testing but this depends on the dataset you choose For example lets\\ndownload MNIST\\nimport tensorflowdatasets  as tfds\\ndataset  tfdsloadnamemnist\\nmnisttrain  mnisttest   datasettrain datasettest\\nY ou can then apply any transformation you want typically repeating batching and\\nprefetching and youre ready to train your model Here is a simple example\\nmnisttrain   mnisttrain repeat5batch32prefetch 1\\nfor item in mnisttrain \\n    images  itemimage\\n    labels  itemlabel\\n    \\nIn general load  returns a shuffled training set so theres no need\\nto shuffle it some more\\nNote that each item in the dataset is a dictionary containing both the features and the\\nlabels But Keras expects each item to be a tuple containing 2 elements again the fea\\ntures and the labels Y ou could transform the dataset using the map  method like\\nthis\\nmnisttrain   mnisttrain repeat5batch32\\nmnisttrain   mnisttrain maplambda items itemsimage itemslabel\\nmnisttrain   mnisttrai', ' repeat5batch32\\nmnisttrain   mnisttrain maplambda items itemsimage itemslabel\\nmnisttrain   mnisttrain prefetch 1\\nOr you can just ask the load  function to do this for you by setting assuper\\nvisedTrue  obviously this works only for labeled datasets Y ou can also specify the\\nbatch size if you want Then the dataset can be passed directly to your tfkeras model\\ndataset  tfdsloadnamemnist batchsize 32 assupervised True\\nmnisttrain   datasettrainrepeatprefetch 1\\nmodel  kerasmodelsSequential \\nmodelcompilelosssparsecategoricalcrossentropy  optimizer sgd\\nmodelfitmnisttrain  stepsperepoch 60000  32 epochs5\\nThis was quite a technical chapter and you may feel that it is a bit far from the\\nabstract beauty of neural networks but the fact is deep learning often involves large\\namounts of data and knowing how to load parse and preprocess it efficiently is a\\ncrucial skill to have In the next chapter we will look at Convolutional Neural Net\\nworks which are among the most successful neural net architectures', ' look at Convolutional Neural Net\\nworks which are among the most successful neural net architectures for image pro\\ncessing and many other applications\\n430  Chapter 13 Loading and Preprocessing Data with TensorFlowCHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles The following will be Chapter 14 in the final\\nrelease of the book\\nAlthough IBMs Deep Blue supercomputer beat the chess world champion Garry Kas\\nparov back in 1996 it wasnt until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words Why are these tasks so effortless to us humans The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness within\\nspecialized visu', 't that perception largely takes place outside the realm of our consciousness within\\nspecialized visual auditory and other sensory modules in our brains By the time\\nsensory information reaches our consciousness it is already adorned with highlevel\\nfeatures for example when you look at a picture of a cute puppy you cannot choose\\nnot to see the puppy or not to notice its cuteness Nor can you explain how you rec\\nognize a cute puppy its just obvious to you Thus we cannot trust our subjective\\nexperience perception is not trivial at all and to understand it we must look at how\\nthe sensory modules work\\nConvolutional neural networks CNNs emerged from the study of the brains visual\\ncortex and they have been used in image recognition since the 1980s In the last few\\nyears thanks to the increase in computational power the amount of available training\\ndata and the tricks presented in Chapter 11  for training deep nets CNNs have man\\naged to achieve superhuman performance on some complex visual tasks ', 'raining deep nets CNNs have man\\naged to achieve superhuman performance on some complex visual tasks They power\\nimage search services selfdriving cars automatic video classification systems and\\nmore Moreover CNNs are not restricted to visual perception they are also successful\\n4311Single Unit Activity in Striate Cortex of Unrestrained Cats  D Hubel and T Wiesel 1958\\n2Receptive Fields of Single Neurones in the Cats Striate Cortex  D Hubel and T Wiesel 1959\\n3Receptive Fields and Functional Architecture of Monkey Striate Cortex  D Hubel and T Wiesel 1968at many other tasks such as voice recognition  or natural language processing  NLP\\nhowever we will focus on visual applications for now\\nIn this chapter we will present where CNNs came from what their building blocks\\nlook like and how to implement them using TensorFlow and Keras Then we will dis\\ncuss some of the best CNN architectures and discuss other visual tasks including\\nobject detection  classifying multiple objects in an image and plac', 'uss other visual tasks including\\nobject detection  classifying multiple objects in an image and placing bounding boxes\\naround them and semantic segmentation  classifying each pixel according to the class\\nof the object it belongs to\\nThe Architecture of the Visual Cortex\\nDavid H Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 and a few years later on monkeys3 giving crucial insights on the\\nstructure of the visual cortex the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work In particular they showed that many neurons in\\nthe visual cortex have a small local receptive field meaning they react only to visual\\nstimuli located in a limited region of the visual field see Figure 141  in which the\\nlocal receptive fields of five neurons are represented by dashed circles The receptive\\nfields of different neurons may overlap and together they tile the whole visual field\\nMoreover the authors showed that some neurons react only to im', 'ther they tile the whole visual field\\nMoreover the authors showed that some neurons react only to images of horizontal\\nlines while others react only to lines with different orientations two neurons may\\nhave the same receptive field but react to different line orientations They also\\nnoticed that some neurons have larger receptive fields and they react to more com\\nplex patterns that are combinations of the lowerlevel patterns These observations\\nled to the idea that the higherlevel neurons are based on the outputs of neighboring\\nlowerlevel neurons in Figure 141  notice that each neuron is connected only to a\\nfew neurons from the previous layer This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field\\n432  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks4Neocognitron A Selforganizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\\nby Shift in Position  K Fukushima 1980\\n5GradientBased Learning Applied', 'attern Recognition Unaffected\\nby Shift in Position  K Fukushima 1980\\n5GradientBased Learning Applied to Document Recognition  Y  LeCun et al 1998\\nFigure 141 Local receptive fields  in the visual cortex\\nThese studies of the visual cortex inspired the neocognitron introduced in 1980 4\\nwhich gradually evolved into what we now call convolutional neural networks  An\\nimportant milestone was a 1998 paper5 by Y ann LeCun Lon Bottou Y oshua Bengio\\nand Patrick Haffner which introduced the famous LeNet5  architecture widely used\\nto recognize handwritten check numbers This architecture has some building blocks\\nthat you already know such as fully connected layers and sigmoid activation func\\ntions but it also introduces two new building blocks convolutional layers  and pooling\\nlayers  Lets look at them now\\nWhy not simply use a regular deep neural network with fully con\\nnected layers for image recognition tasks Unfortunately although\\nthis works fine for small images eg MNIST it breaks down for\\nlarger', 'ion tasks Unfortunately although\\nthis works fine for small images eg MNIST it breaks down for\\nlarger images because of the huge number of parameters it\\nrequires For example a 100  100 image has 10000 pixels and if\\nthe first layer has just 1000 neurons which already severely\\nrestricts the amount of information transmitted to the next layer\\nthis means a total of 10 million connections And thats just the first\\nlayer CNNs solve this problem using partially connected layers and\\nweight sharing\\nThe Architecture of the Visual Cortex  4336A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication It has deep connections with the Fourier transform and the Laplace transform\\nand is heavily used in signal processing Convolutional layers actually use crosscorrelations which are very\\nsimilar to convolutions see httpshomlinfo76  for more details\\nConvolutional Layer\\nThe most important building block of a CNN is the convolu', 'fo76  for more details\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer 6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\nlike they were in previous chapters but only to pixels in their receptive fields see\\nFigure 142  In turn each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer This architecture\\nallows the network to concentrate on small lowlevel features in the first hidden layer\\nthen assemble them into larger higherlevel features in the next hidden layer and so\\non This hierarchical structure is common in realworld images which is one of the\\nreasons why CNNs work so well for image recognition\\nFigure 142 CNN layers with rectangular local receptive fields\\nUntil now all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons and we had to flatten input\\nimages to 1D before feeding them to the n', 'osed of a long line of neurons and we had to flatten input\\nimages to 1D before feeding them to the neural network Now each\\nlayer is represented in 2D which makes it easier to match neurons\\nwith their corresponding inputs\\nA neuron located in row i column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i  fh  1 columns j to j  fw  1\\nwhere fh and fw are the height and width of the receptive field see Figure 143  In\\norder for a layer to have the same height and width as the previous layer it is com\\n434  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksmon to add zeros around the inputs as shown in the diagram This is called zero pad\\nding\\nFigure 143 Connections between layers and zero padding\\nIt is also possible to connect a large input layer to a much smaller layer by spacing out\\nthe receptive fields as shown in Figure 144  The shift from one receptive field to the\\nnext is called the stride  In the diagram a 5  7 in', 'e 144  The shift from one receptive field to the\\nnext is called the stride  In the diagram a 5  7 input layer plus zero padding is con\\nnected to a 3  4 layer using 3  3 receptive fields and a stride of 2 in this example\\nthe stride is the same in both directions but it does not have to be so A neuron loca\\nted in row i column j in the upper layer is connected to the outputs of the neurons in\\nthe previous layer located in rows i  sh to i  sh  fh  1 columns j  sw to j  sw  fw \\n1 where sh and sw are the vertical and horizontal strides\\nConvolutional Layer  435Figure 144 Reducing dimensionality using a stride of 2\\nFilters\\nA neurons weights can be represented as a small image the size of the receptive field\\nFor example Figure 145  shows two possible sets of weights called filters  or convolu\\ntion kernels  The first one is represented as a black square with a vertical white line in\\nthe middle it is a 7  7 matrix full of 0s except for the central column which is full of\\n1s neurons using these we', 'is a 7  7 matrix full of 0s except for the central column which is full of\\n1s neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line since all inputs will get multiplied by 0 except for the\\nones located in the central vertical line The second filter is a black square with a\\nhorizontal white line in the middle Once again neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line\\nNow if all neurons in a layer use the same vertical line filter and the same bias term\\nand you feed the network the input image shown in Figure 145  bottom image the\\nlayer will output the topleft image Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred Similarly the upperright image is what you get if all neu\\nrons use the same horizontal line filter notice that the horizontal white lines get\\nenhanced while the rest is blurred out Thus a layer full of neurons using the same\\n', ' white lines get\\nenhanced while the rest is blurred out Thus a layer full of neurons using the same\\nfilter outputs a feature map  which highlights the areas in an image that activate the\\nfilter the most Of course you do not have to define the filters manually instead dur\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task and the layers above will learn to combine them into more complex patterns\\n436  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 145 Applying two different  filters  to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now for simplicity I have represented the output of each convolutional layer as\\na thin 2D layer but in reality a convolutional layer has multiple filters you decide\\nhow many and it outputs one feature map per filter so it is more accurately repre\\nsented in 3D see Figure 146  To do so it has one neuron per pixel in each feature\\nmap and all neurons within a given feature map s', 'o do so it has one neuron per pixel in each feature\\nmap and all neurons within a given feature map share the same parameters ie the\\nsame weights and bias term However neurons in different feature maps use differ\\nent parameters A neurons receptive field is the same as described earlier but it\\nextends across all the previous layers feature maps In short a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs making it capable of\\ndetecting multiple features anywhere in its inputs\\nThe fact that all neurons in a feature map share the same parame\\nters dramatically reduces the number of parameters in the model\\nMoreover once the CNN has learned to recognize a pattern in one\\nlocation it can recognize it in any other location In contrast once\\na regular DNN has learned to recognize a pattern in one location it\\ncan recognize it only in that particular location\\nMoreover input images are also composed of multiple sublayers one per color chan\\nnel There are typically thre', 'input images are also composed of multiple sublayers one per color chan\\nnel There are typically three red green and blue RGB Grayscale images have just\\nConvolutional Layer  437one channel but some images may have much morefor example satellite images\\nthat capture extra light frequencies such as infrared\\nFigure 146 Convolution layers with multiple feature maps and images with three color\\nchannels\\nSpecifically a neuron located in row i column j of the feature map k in a given convo\\nlutional layer l is connected to the outputs of the neurons in the previous layer l  1\\nlocated in rows i  sh to i  sh  fh  1 and columns j  sw to j  sw  fw  1 across all\\nfeature maps in layer l  1 Note that all neurons located in the same row i and col\\numn j but in different feature maps are connected to the outputs of the exact same\\nneurons in the previous layer\\nEquation 141  summarizes the preceding explanations in one big mathematical equa\\ntion it shows how to compute the output of a given neuron in a convo', 'ns in one big mathematical equa\\ntion it shows how to compute the output of a given neuron in a convolutional layer\\n438  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksIt is a bit ugly due to all the different indices but all it does is calculate the weighted\\nsum of all the inputs plus the bias term\\nEquation 141 Computing the output of a neuron in a convolutional layer\\nzijkbk\\nu 0fh 1\\n\\nv 0fw 1\\n\\nk 0fn 1\\nxijkwuvkkwithiishu\\njjswv\\nzi j k is the output of the neuron located in row i column j in feature map k of the\\nconvolutional layer layer l\\nAs explained earlier sh and sw are the vertical and horizontal strides fh and fw are\\nthe height and width of the receptive field and fn is the number of feature maps\\nin the previous layer layer l  1\\nxi j k is the output of the neuron located in layer l  1 row i column j feature\\nmap k or channel k if the previous layer is the input layer\\nbk is the bias term for feature map k in layer l Y ou can think of it as a knob that\\ntweaks the ove', '\\nbk is the bias term for feature map k in layer l Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k\\nwu v k k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u column v relative to the neurons receptive field\\nand feature map k\\nTensorFlow Implementation\\nIn TensorFlow each input image is typically represented as a 3D tensor of shape\\nheight width channels  A minibatch is represented as a 4D tensor of shape\\nminibatch size height width channels  The weights of a convolutional\\nlayer are represented as a 4D tensor of shape  fh fw fn fn The bias terms of a convo\\nlutional layer are simply represented as a 1D tensor of shape  fn\\nLets look at a simple example The following code loads two sample images using\\nScikitLearns loadsampleimages  which loads two color images one of a Chi\\nnese temple and the other of a flower The pixel intensities for each color channel\\nis represented as a byte from 0 to 255 so we ', ' a flower The pixel intensities for each color channel\\nis represented as a byte from 0 to 255 so we scale these features simply by dividing by\\n255 to get floats ranging from 0 to 1 Then we create two 7  7 filters one with a\\nvertical white line in the middle and the other with a horizontal white line in the\\nmiddle and we apply them to both images using the tfnnconv2d  function\\nwhich is part of TensorFlows lowlevel Deep Learning API In this example we use\\nzero padding  paddingSAME  and a stride of 2 Finally we plot one of the resulting\\nfeature maps similar to the topright image in Figure 145 \\nConvolutional Layer  439from sklearndatasets  import loadsampleimage\\n Load sample images\\nchina  loadsampleimage chinajpg   255\\nflower  loadsampleimage flowerjpg   255\\nimages  nparraychina flower\\nbatchsize  height width channels   imagesshape\\n Create 2 filters\\nfilters  npzerosshape7 7 channels  2 dtypenpfloat32\\nfilters 3  0  1   vertical line\\nfilters3   1  1   horizontal line\\noutputs  tfnnconv2dimage', 'npfloat32\\nfilters 3  0  1   vertical line\\nfilters3   1  1   horizontal line\\noutputs  tfnnconv2dimages filters strides1 paddingSAME\\npltimshowoutputs0   1 cmapgray  plot 1st images 2nd feature map\\npltshow\\nMost of this code is selfexplanatory but the tfnnconv2d  line deserves a bit of\\nexplanation\\nimages  is the input minibatch a 4D tensor as explained earlier\\nfilters  is the set of filters to apply also a 4D tensor as explained earlier\\nstrides  is equal to 1 but it could also be a 1D array with 4 elements where the\\ntwo central elements are the vertical and horizontal strides  sh and sw The first\\nand last elements must currently be equal to 1 They may one day be used to\\nspecify a batch stride to skip some instances and a channel stride to skip some\\nof the previous layers feature maps or channels\\npadding  must be either VALID  or SAME \\nIf set to VALID  the convolutional layer does not use zero padding and may\\nignore some rows and columns at the bottom and right of the input image\\ndepending ', 'o padding and may\\nignore some rows and columns at the bottom and right of the input image\\ndepending on the stride as shown in Figure 147  for simplicity only the hor\\nizontal dimension is shown here but of course the same logic applies to the\\nvertical dimension\\nIf set to SAME  the convolutional layer uses zero padding if necessary In this\\ncase the number of output neurons is equal to the number of input neurons\\ndivided by the stride rounded up in this example 13  5  26 rounded up to\\n3 Then zeros are added as evenly as possible around the inputs\\n440  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 147 Padding optionsinput width 13 filter  width 6 stride 5\\nIn this example we manually defined the filters but in a real CNN you would nor\\nmally define filters as trainable variables so the neural net can learn which filters\\nwork best as explained earlier Instead of manually creating the variables however\\nyou can simply use the keraslayersConv2D  layer\\nconv  keraslayer', 'ally creating the variables however\\nyou can simply use the keraslayersConv2D  layer\\nconv  keraslayersConv2Dfilters32 kernelsize 3 strides1\\n                           paddingSAME activation relu\\nThis code creates a Conv2D  layer with 32 filters each 3  3 using a stride of 1 both\\nhorizontally and vertically SAME padding and applying the ReLU activation func\\ntion to its outputs As you can see convolutional layers have quite a few hyperpara\\nmeters you must choose the number of filters their height and width the strides and\\nthe padding type As always you can use crossvalidation to find the right hyperpara\\nmeter values but this is very timeconsuming We will discuss common CNN archi\\ntectures later to give you some idea of what hyperparameter values work best in \\npractice\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM This is especially true during training because the reverse pass of backpro\\npagation requires all the intermediate va', 'y true during training because the reverse pass of backpro\\npagation requires all the intermediate values computed during the forward pass\\nFor example consider a convolutional layer with 5  5 filters outputting 200 feature\\nmaps of size 150  100 with stride 1 and SAME padding If the input is a 150  100\\nConvolutional Layer  4417A fully connected layer with 150  100 neurons each connected to all 150  100  3 inputs would have 1502\\n 1002  3  675 million parameters\\n8In the international system of units SI 1 MB  1000 kB  1000  1000 bytes  1000  1000  8 bits\\nRGB image three channels then the number of parameters is 5  5  3  1  200\\n 15200 the 1 corresponds to the bias terms which is fairly small compared to a\\nfully connected layer7 However each of the 200 feature maps contains 150  100 neu\\nrons and each of these neurons needs to compute a weighted sum of its 5  5  3 \\n75 inputs thats a total of 225 million float multiplications Not as bad as a fully con\\nnected layer but still quite computationall', ' million float multiplications Not as bad as a fully con\\nnected layer but still quite computationally intensive Moreover if the feature maps\\nare represented using 32bit floats then the convolutional layers output will occupy\\n200  150  100  32  96 million bits 12 MB of RAM8 And thats just for one\\ninstance If a training batch contains 100 instances then this layer will use up 12 GB\\nof RAM\\nDuring inference ie when making a prediction for a new instance the RAM occu\\npied by one layer can be released as soon as the next layer has been computed so you\\nonly need as much RAM as required by two consecutive layers But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass so the amount of RAM needed is at least the total amount of RAM required by\\nall layers\\nIf training crashes because of an outofmemory error you can try\\nreducing the minibatch size Alternatively you can try reducing\\ndimensionality using a stride or removing a few layers Or you can\\nt', 'ternatively you can try reducing\\ndimensionality using a stride or removing a few layers Or you can\\ntry using 16bit floats instead of 32bit floats Or you could distrib\\nute the CNN across multiple devices\\nNow lets look at the second common building block of CNNs the pooling layer \\nPooling Layer\\nOnce you understand how convolutional layers work the pooling layers are quite\\neasy to grasp Their goal is to subsample  ie shrink the input image in order to\\nreduce the computational load the memory usage and the number of parameters\\nthereby limiting the risk of overfitting\\nJust like in convolutional layers each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer located within a small\\nrectangular receptive field Y ou must define its size the stride and the padding type\\njust like before However a pooling neuron has no weights all it does is aggregate the\\ninputs using an aggregation function such as the max or mean Figure 148  shows a\\nmax pool', 'regate the\\ninputs using an aggregation function such as the max or mean Figure 148  shows a\\nmax pooling layer  which is the most common type of pooling layer In this example\\n442  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks9Other kernels we discussed so far had weights but pooling kernels do not they are just stateless sliding win\\ndows\\nwe use a 2  2 pooling kernel9 with a stride of 2 and no padding Only the max\\ninput value in each receptive field makes it to the next layer while the other inputs\\nare dropped For example in the lower left receptive field in Figure 148  the input\\nvalues are 1 5 3 2 so only the max value 5 is propagated to the next layer Because\\nof the stride of 2 the output image has half the height and half the width of the input\\nimage rounded down since we use no padding\\nFigure 148 Max pooling layer 2  2 pooling kernel stride 2 no padding\\nA pooling layer typically works on every input channel independ\\nently so the output depth is the same as the i', 'layer typically works on every input channel independ\\nently so the output depth is the same as the input depth\\nOther than reducing computations memory usage and the number of parameters a\\nmax pooling layer also introduces some level of invariance  to small translations as\\nshown in Figure 149  Here we assume that the bright pixels have a lower value than\\ndark pixels and we consider 3 images A B C going through a max pooling layer\\nwith a 2  2 kernel and stride 2 Images B and C are the same as image A but shifted\\nby one and two pixels to the right As you can see the outputs of the max pooling\\nlayer for images A and B are identical This is what translation invariance means\\nHowever for image C the output is different it is shifted by one pixel to the right\\nbut there is still 75 invariance By inserting a max pooling layer every few layers in\\na CNN it is possible to get some level of translation invariance at a larger scale\\nMoreover max pooling also offers a small amount of rotational invaria', 'n invariance at a larger scale\\nMoreover max pooling also offers a small amount of rotational invariance and a\\nslight scale invariance Such invariance even if it is limited can be useful in cases\\nwhere the prediction should not depend on these details such as in classification\\ntasks\\nPooling Layer  443Figure 149 Invariance to small translations\\nBut max pooling has some downsides firstly it is obviously very destructive even\\nwith a tiny 2  2 kernel and a stride of 2 the output will be two times smaller in both\\ndirections so its area will be four times smaller simply dropping 75 of the input\\nvalues And in some applications invariance is not desirable for example for seman\\ntic segmentation  this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to obviously if the input image is translated by 1 pixel to the\\nright the output should also be translated by 1 pixel to the right The goal in this case\\nis equivariance  not invariance a small change to the ', 'by 1 pixel to the right The goal in this case\\nis equivariance  not invariance a small change to the inputs should lead to a corre\\nsponding small change in the output\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy The following code\\ncreates a max pooling layer using a 2  2 kernel The strides default to the kernel size\\nso this layer will use a stride of 2 both horizontally and vertically By default it uses\\nV ALID padding ie no padding at all\\nmaxpool   keraslayersMaxPool2D poolsize 2\\nTo create an average pooling layer  just use AvgPool2D  instead of MaxPool2D  As you\\nmight expect it works exactly like a max pooling layer except it computes the mean\\nrather than the max Average pooling layers used to be very popular but people\\n444  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksmostly use max pooling layers now as they generally perform better This may seem\\nsurprising since computing the mean generally loses less information than c', 'orm better This may seem\\nsurprising since computing the mean generally loses less information than comput\\ning the max But on the other hand max pooling preserves only the strongest feature\\ngetting rid of all the meaningless ones so the next layers get a cleaner signal to work\\nwith Moreover max pooling offers stronger translation invariance than average\\npooling\\nNote that max pooling and average pooling can be performed along the depth dimen\\nsion rather than the spatial dimensions although this is not as common This can\\nallow the CNN to learn to be invariant to various features For example it could learn\\nmultiple filters each detecting a different rotation of the same pattern such as hand\\nwritten digits see Figure 1410  and the depthwise max pooling layer would ensure\\nthat the output is the same regardless of the rotation The CNN could similarly learn\\nto be invariant to anything else thickness brightness skew color and so on\\nFigure 1410 Depthwise max pooling can help the CNN learn any in', 'ness brightness skew color and so on\\nFigure 1410 Depthwise max pooling can help the CNN learn any invariance\\nPooling Layer  445Keras does not include a depthwise max pooling layer but TensorFlows lowlevel\\nDeep Learning API does just use the tfnnmaxpool  function and specify the\\nkernel size and strides as 4tuples The first three values of each should be 1 this indi\\ncates that the kernel size and stride along the batch height and width dimensions\\nshoud be 1 The last value should be whatever kernel size and stride you want along\\nthe depth dimension for example 3 this must be a divisor of the input depth for\\nexample it will not work if the previous layer outputs 20 feature maps since 20 is not\\na multiple of 3\\noutput  tfnnmaxpool images\\n                        ksize1 1 1 3\\n                        strides1 1 1 3\\n                        paddingVALID\\nIf you want to include this as a layer in your Keras models you can simply wrap it in\\na Lambda  layer or create a custom Keras layer\\ndepthpool   ', 'r Keras models you can simply wrap it in\\na Lambda  layer or create a custom Keras layer\\ndepthpool   keraslayersLambda\\n    lambda X tfnnmaxpool X ksize1 1 1 3 strides1 1 1 3\\n                             paddingVALID\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling  layer It works very differently all it does is compute the mean\\nof each entire feature map its like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs This means that it just outputs a sin\\ngle number per feature map and per instance Although this is of course extremely\\ndestructive most of the information in the feature map is lost it can be useful as the\\noutput layer as we will see later in this chapter To create such a layer simply use the\\nkeraslayersGlobalAvgPool2D  class\\nglobalavgpool   keraslayersGlobalAvgPool2D \\nIt is actually equivalent to this simple Lamba  layer which computes the mean over the\\nspatial dimensions he', 'tually equivalent to this simple Lamba  layer which computes the mean over the\\nspatial dimensions height and width\\nglobalavgpool   keraslayersLambdalambda X tfreducemean X axis1 2\\nNow you know all the building blocks to create a convolutional neural network Lets\\nsee how to assemble them\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers each one generally fol\\nlowed by a ReLU layer then a pooling layer then another few convolutional layers\\nReLU then another pooling layer and so on The image gets smaller and smaller\\nas it progresses through the network but it also typically gets deeper and deeper ie\\nwith more feature maps thanks to the convolutional layers see Figure 1411  At the\\ntop of the stack a regular feedforward neural network is added composed of a few\\n446  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksfully connected layers ReLUs and the final layer outputs the prediction eg a\\nsoftmax layer that outputs estimated class probabilities\\n', 'he final layer outputs the prediction eg a\\nsoftmax layer that outputs estimated class probabilities\\nFigure 1411 Typical CNN architecture\\nA common mistake is to use convolution kernels that are too large\\nFor example instead of using a convolutional layer with a 5  5\\nkernel it is generally preferable to stack two layers with 3  3 ker\\nnels it will use less parameters and require less computations and\\nit will usually perform better One exception to this recommenda\\ntion is for the first convolutional layer it can typically have a large\\nkernel eg 5  5 usually with stride of 2 or more this will reduce\\nthe spatial dimension of the image without losing too much infor\\nmation and since the input image only has 3 channels in general it\\nwill not be too costly\\nHere is how you can implement a simple CNN to tackle the fashion MNIST dataset\\nintroduced in Chapter 10 \\nfrom functools  import partial\\nDefaultConv2D   partialkeraslayersConv2D\\n                        kernelsize 3 activation relu paddingSAME\\nm', 'Conv2D   partialkeraslayersConv2D\\n                        kernelsize 3 activation relu paddingSAME\\nmodel  kerasmodelsSequential \\n    DefaultConv2D filters64 kernelsize 7 inputshape 28 28 1\\n    keraslayersMaxPooling2D poolsize 2\\n    DefaultConv2D filters128\\n    DefaultConv2D filters128\\n    keraslayersMaxPooling2D poolsize 2\\n    DefaultConv2D filters256\\n    DefaultConv2D filters256\\n    keraslayersMaxPooling2D poolsize 2\\n    keraslayersFlatten\\n    keraslayersDenseunits128 activation relu\\n    keraslayersDropout05\\n    keraslayersDenseunits64 activation relu\\n    keraslayersDropout05\\n    keraslayersDenseunits10 activation softmax \\n\\nCNN Architectures  447In this code we start by using the partial  function to define a thin wrapper\\naround the Conv2D  class called DefaultConv2D  it simply avoids having to repeat\\nthe same hyperparameter values over and over again\\nThe first layer uses a large kernel size but no stride because the input images are\\nnot very large It also sets inputshape28 28 1  whic', ' size but no stride because the input images are\\nnot very large It also sets inputshape28 28 1  which means the images\\nare 28  28 pixels with a single color channel ie grayscale\\nNext we have a max pooling layer which divides each spatial dimension by a fac\\ntor of two since poolsize2 \\nThen we repeat the same structure twice two convolutional layers followed by a\\nmax pooling layer For larger images we could repeat this structure several times\\nthe number of repetitions is a hyperparameter you can tune\\nNote that the number of filters grows as we climb up the CNN towards the out\\nput layer it is initially 64 then 128 then 256 it makes sense for it to grow since\\nthe number of low level features is often fairly low eg small circles horizontal\\nlines etc but there are many different ways to combine them into higher level\\nfeatures It is a common practice to double the number of filters after each pool\\ning layer since a pooling layer divides each spatial dimension by a factor of 2 we\\ncan afford do', 'ool\\ning layer since a pooling layer divides each spatial dimension by a factor of 2 we\\ncan afford doubling the number of feature maps in the next layer without fear of\\nexploding the number of parameters memory usage or computational load\\nNext is the fully connected network composed of 2 hidden dense layers and a\\ndense output layer Note that we must flatten its inputs since a dense network\\nexpects a 1D array of features for each instance We also add two dropout layers\\nwith a dropout rate of 50 each to reduce overfitting\\nThis CNN reaches over 92 accuracy on the test set Its not the state of the art but it\\nis pretty good and clearly much better than what we achieved with dense networks in\\nChapter 10 \\nOver the years variants of this fundamental architecture have been developed lead\\ning to amazing advances in the field A good measure of this progress is the error rate\\nin competitions such as the ILSVRC ImageNet challenge  In this competition the\\ntop5 error rate for image classification fell', 'the ILSVRC ImageNet challenge  In this competition the\\ntop5 error rate for image classification fell from over 26 to less than 23 in just six\\nyears The topfive error rate is the number of test images for which the systems top 5\\npredictions did not include the correct answer The images are large 256 pixels high\\nand there are 1000 classes some of which are really subtle try distinguishing 120\\ndog breeds Looking at the evolution of the winning entries is a good way to under\\nstand how CNNs work\\nWe will first look at the classical LeNet5 architecture 1998 then three of the win\\nners of the ILSVRC challenge AlexNet 2012 GoogLeNet 2014 and ResNet\\n2015\\n448  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks10GradientBased Learning Applied to Document Recognition  Y  LeCun L Bottou Y  Bengio and P  Haffner\\n1998LeNet5\\nThe LeNet5 architecture10 is perhaps the most widely known CNN architecture As\\nmentioned earlier it was created by Y ann LeCun in 1998 and widely used for hand\\nwrit', 'rchitecture As\\nmentioned earlier it was created by Y ann LeCun in 1998 and widely used for hand\\nwritten digit recognition MNIST It is composed of the layers shown in Table 141 \\nTable 141 LeNet5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected  10   RBF\\nF6 Fully Connected  84   tanh\\nC5 Convolution 120 1  1 5  5 1 tanh\\nS4 Avg Pooling 16 5  5 2  2 2 tanh\\nC3 Convolution 16 10  10 5  5 1 tanh\\nS2 Avg Pooling 6 14  14 2  2 2 tanh\\nC1 Convolution 6 28  28 5  5 1 tanh\\nIn Input 1 32  32   \\nThere are a few extra details to be noted\\nMNIST images are 28  28 pixels but they are zeropadded to 32  32 pixels and\\nnormalized before being fed to the network The rest of the network does not use\\nany padding which is why the size keeps shrinking as the image progresses\\nthrough the network\\nThe average pooling layers are slightly more complex than usual each neuron\\ncomputes the mean of its inputs then multiplies the result by a learnable coeffi\\ncient one per map and adds a lea', 'mean of its inputs then multiplies the result by a learnable coeffi\\ncient one per map and adds a learnable bias term again one per map then\\nfinally applies the activation function\\nMost neurons in C3 maps are connected to neurons in only three or four S2\\nmaps instead of all six S2 maps See table 1 page 8 in the original paper10 for\\ndetails\\nThe output layer is a bit special instead of computing the matrix multiplication\\nof the inputs and the weight vector each neuron outputs the square of the Eucli\\ndian distance between its input vector and its weight vector Each output meas\\nures how much the image belongs to a particular digit class The cross entropy \\ncost function is now preferred as it penalizes bad predictions much more pro\\nducing larger gradients and converging faster\\nCNN Architectures  44911ImageNet Classification with Deep Convolutional Neural Networks  A Krizhevsky et al 2012Y ann LeCuns website  LENET section features great demos of LeNet5 classifying \\ndigits\\nAlexNet\\nThe AlexNet', 'LeCuns website  LENET section features great demos of LeNet5 classifying \\ndigits\\nAlexNet\\nThe AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin it achieved 17 top5 error rate while the second best achieved only\\n26 It was developed by Alex Krizhevsky hence the name Ilya Sutskever and\\nGeoffrey Hinton It is quite similar to LeNet5 only much larger and deeper and it\\nwas the first to stack convolutional layers directly on top of each other instead of\\nstacking a pooling layer on top of each convolutional layer Table 142  presents this\\narchitecture\\nTable 142 AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected  1000    Softmax\\nF9 Fully Connected  4096    ReLU\\nF8 Fully Connected  4096    ReLU\\nC7 Convolution 256 13  13 3  3 1 SAME ReLU\\nC6 Convolution 384 13  13 3  3 1 SAME ReLU\\nC5 Convolution 384 13  13 3  3 1 SAME ReLU\\nS4 Max Pooling 256 13  13 3  3 2 VALID \\nC3 Convolution 256 27  27 5  5 1 SAME ReLU\\nS2 Max Poolin', 'eLU\\nS4 Max Pooling 256 13  13 3  3 2 VALID \\nC3 Convolution 256 27  27 5  5 1 SAME ReLU\\nS2 Max Pooling 96 27  27 3  3 2 VALID \\nC1 Convolution 96 55  55 11  11 4 VALID ReLU\\nIn Input 3 RGB 227  227    \\nTo reduce overfitting the authors used two regularization techniques first they\\napplied dropout introduced in Chapter 11  with a 50 dropout rate during training\\nto the outputs of layers F8 and F9 Second they performed data augmentation  by ran\\ndomly shifting the training images by various offsets flipping them horizontally and\\nchanging the lighting conditions\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance This reduces overfitting making this\\na regularization technique The generated instances should be as realistic as possible\\n450  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksideally given an image from the augmented training set a human should not be able\\nto tell wh', 'Networksideally given an image from the augmented training set a human should not be able\\nto tell whether it was augmented or not Moreover simply adding white noise will not\\nhelp the modifications should be learnable white noise is not\\nFor example you can slightly shift rotate and resize every picture in the training set\\nby various amounts and add the resulting pictures to the training set see\\nFigure 1412  This forces the model to be more tolerant to variations in the position\\norientation and size of the objects in the pictures If you want the model to be more\\ntolerant to different lighting conditions you can similarly generate many images with\\nvarious contrasts In general you can also flip the pictures horizontally except for\\ntext and other nonsymmetrical objects By combining these transformations you\\ncan greatly increase the size of your training set\\nFigure 1412 Generating new training instances from existing ones\\nAlexNet also uses a competitive normalization step immediately after t', ' instances from existing ones\\nAlexNet also uses a competitive normalization step immediately after the ReLU step\\nof layers C1 and C3 called local response normalization  The most strongly activated\\nneurons inhibit other neurons located at the same position in neighboring feature\\nmaps such competitive activation has been observed in biological neurons This\\nencourages different feature maps to specialize pushing them apart and forcing them\\nCNN Architectures  45112Going Deeper with Convolutions  C Szegedy et al 2015\\n13In the 2010 movie Inception  the characters keep going deeper and deeper into multiple layers of dreams\\nhence the name of these modulesto explore a wider range of features ultimately improving generalization Equation\\n142  shows how to apply LRN\\nEquation 142 Local response normalization\\nbiaik\\njjlowjhigh\\naj2\\nwithjhigh min ir\\n2fn 1\\njlow max 0ir\\n2\\nbi is the normalized output of the neuron located in feature map i at some row u\\nand column v note that in this equation we consider ', 'e neuron located in feature map i at some row u\\nand column v note that in this equation we consider only neurons located at this\\nrow and column so u and v are not shown\\nai is the activation of that neuron after the ReLU step but before normalization\\nk   and r are hyperparameters k is called the bias and r is called the depth\\nradius \\nfn is the number of feature maps\\nFor example if r  2 and a neuron has a strong activation it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own\\nIn AlexNet the hyperparameters are set as follows r  2   000002   075 and k\\n 1 This step can be implemented using the tfnnlocalresponsenormaliza\\ntion  function which you can wrap in a Lambda  layer if you want to use it in a\\nKeras model\\nA variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge It is essentially AlexNet with a few tweaked \\nhyperparameters number of feature maps kernel size stride etc\\nGoo', 'tially AlexNet with a few tweaked \\nhyperparameters number of feature maps kernel size stride etc\\nGoogLeNet\\nThe GoogLeNet architecture  was developed by Christian Szegedy et al from Google\\nResearch12 and it won the ILSVRC 2014 challenge by pushing the top5 error rate\\nbelow 7 This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs see Figure 1414  This was made possible by\\nsubnetworks called inception modules 13 which allow GoogLeNet to use parameters\\n452  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksmuch more efficiently than previous architectures GoogLeNet actually has 10 times\\nfewer parameters than AlexNet roughly 6 million instead of 60 million\\nFigure 1413  shows the architecture of an inception module The notation 3  3 \\n1S means that the layer uses a 3  3 kernel stride 1 and SAME padding The input\\nsignal is first copied and fed to four different layers All convolutional layers use the\\nReLU activation function', 'st copied and fed to four different layers All convolutional layers use the\\nReLU activation function Note that the second set of convolutional layers uses differ\\nent kernel sizes 1  1 3  3 and 5  5 allowing them to capture patterns at different\\nscales Also note that every single layer uses a stride of 1 and SAME padding even\\nthe max pooling layer so their outputs all have the same height and width as their\\ninputs This makes it possible to concatenate all the outputs along the depth dimen\\nsion in the final depth concat layer  ie stack the feature maps from all four top con\\nvolutional layers This concatenation layer can be implemented in TensorFlow using\\nthe tfconcat  operation with axis3  axis 3 is the depth\\nFigure 1413 Inception module\\nY ou may wonder why inception modules have convolutional layers with 1  1 ker\\nnels Surely these layers cannot capture any features since they look at only one pixel\\nat a time In fact these layers serve three purposes\\nFirst although they cannot capture sp', ' one pixel\\nat a time In fact these layers serve three purposes\\nFirst although they cannot capture spatial patterns they can capture patterns\\nalong the depth dimension\\nSecond they are configured to output fewer feature maps than their inputs so\\nthey serve as bottleneck layers  meaning they reduce dimensionality This cuts the\\ncomputational cost and the number of parameters speeding up training and\\nimproving generalization\\nLastly each pair of convolutional layers 1  1 3  3 and 1  1 5  5 acts like\\na single powerful convolutional layer capable of capturing more complex pat\\nterns Indeed instead of sweeping a simple linear classifier across the image as a\\nCNN Architectures  453single convolutional layer does this pair of convolutional layers sweeps a two\\nlayer neural network across the image\\nIn short you can think of the whole inception module as a convolutional layer on\\nsteroids able to output feature maps that capture complex patterns at various scales\\nThe number of convolutional kernels fo', ' feature maps that capture complex patterns at various scales\\nThe number of convolutional kernels for each convolutional layer\\nis a hyperparameter Unfortunately this means that you have six\\nmore hyperparameters to tweak for every inception layer you add\\nNow lets look at the architecture of the GoogLeNet CNN see Figure 1414  The\\nnumber of feature maps output by each convolutional layer and each pooling layer is\\nshown before the kernel size The architecture is so deep that it has to be represented\\nin three columns but GoogLeNet is actually one tall stack including nine inception\\nmodules the boxes with the spinning tops The six numbers in the inception mod\\nules represent the number of feature maps output by each convolutional layer in the\\nmodule in the same order as in Figure 1413  Note that all the convolutional layers\\nuse the ReLU activation function\\n454  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 1414 GoogLeNet architecture\\nLets go through this network\\nTh', 'sing Convolutional Neural NetworksFigure 1414 GoogLeNet architecture\\nLets go through this network\\nThe first two layers divide the images height and width by 4 so its area is divided\\nby 16 to reduce the computational load The first layer uses a large kernel size\\nso that much of the information is still preserved\\nThen the local response normalization layer ensures that the previous layers learn\\na wide variety of features as discussed earlier\\nTwo convolutional layers follow where the first acts like a bottleneck layer  As\\nexplained earlier you can think of this pair as a single smarter convolutional\\nlayer\\nAgain a local response normalization layer ensures that the previous layers cap\\nture a wide variety of patterns\\nCNN Architectures  45514Very Deep Convolutional Networks for LargeScale Image Recognition  K Simonyan and A Zisserman\\n2015Next a max pooling layer reduces the image height and width by 2 again to speed\\nup computations\\nThen comes the tall stack of nine inception modules interlea', 'dth by 2 again to speed\\nup computations\\nThen comes the tall stack of nine inception modules interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net\\nNext the global average pooling layer simply outputs the mean of each feature\\nmap this drops any remaining spatial information which is fine since there was\\nnot much spatial information left at that point Indeed GoogLeNet input images\\nare typically expected to be 224  224 pixels so after 5 max pooling layers each\\ndividing the height and width by 2 the feature maps are down to 7  7 More\\nover it is a classification task not localization so it does not matter where the\\nobject is Thanks to the dimensionality reduction brought by this layer there is\\nno need to have several fully connected layers at the top of the CNN like in\\nAlexNet and this considerably reduces the number of parameters in the net\\nwork and limits the risk of overfitting\\nThe last layers are selfexplanatory dropout for regularization then a fully', ' the risk of overfitting\\nThe last layers are selfexplanatory dropout for regularization then a fully con\\nnected layer with 1000 units since there are a 1000 classes and a softmax acti\\nvation function to output estimated class probabilities\\nThis diagram is slightly simplified the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules\\nThey were both composed of one average pooling layer one convolutional layer two\\nfully connected layers and a softmax activation layer During training their loss\\nscaled down by 70 was added to the overall loss The goal was to fight the vanish\\ning gradients problem and regularize the network However it was later shown that\\ntheir effect was relatively minor\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers including Inceptionv3 and Inceptionv4 using slightly different incep\\ntion modules and reaching even better performance\\nVGGNet\\nThe runner up in th', 'lightly different incep\\ntion modules and reaching even better performance\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was  VGGNet14 developed by K Simon\\nyan and A Zisserman It had a very simple and classical architecture with 2 or 3 con\\nvolutional layers a pooling layer then again 2 or 3 convolutional layers a pooling\\nlayer and so on with a total of just 16 convolutional layers plus a final dense net\\nwork with 2 hidden layers and the output layer It used only 3  3 filters but many\\nfilters\\n456  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks15Deep Residual Learning for Image Recognition  K He 2015ResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network  or ResNet  devel\\noped by Kaiming He et al15 which delivered an astounding top5 error rate under\\n36 using an extremely deep CNN composed of 152 layers It confirmed the general\\ntrend models are getting deeper and deeper with fewer and fewer parameters The\\nkey to being able to train such a deep network is ', 'er and deeper with fewer and fewer parameters The\\nkey to being able to train such a deep network is to use skip connections  also called\\nshortcut connections  the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack Lets see why this is useful\\nWhen training a neural network the goal is to make it model a target function hx\\nIf you add the input x to the output of the network ie you add a skip connection\\nthen the network will be forced to model fx  hx  x rather than hx This is\\ncalled residual learning  see Figure 1415 \\nFigure 1415 Residual learning\\nWhen you initialize a regular neural network its weights are close to zero so the net\\nwork just outputs values close to zero If you add a skip connection the resulting net\\nwork just outputs a copy of its inputs in other words it initially models the identity\\nfunction If the target function is fairly close to the identity function which is often\\nthe case this will speed up training considerably\\nMo', 'y close to the identity function which is often\\nthe case this will speed up training considerably\\nMoreover if you add many skip connections the network can start making progress\\neven if several layers have not started learning yet see Figure 1416  Thanks to skip\\nconnections the signal can easily make its way across the whole network The deep\\nresidual network can be seen as a stack of residual units  where each residual unit is a\\nsmall neural network with a skip connection\\nCNN Architectures  457Figure 1416 Regular deep neural network left  and deep residual network right\\nNow lets look at ResNets architecture see Figure 1417  It is actually surprisingly\\nsimple It starts and ends exactly like GoogLeNet except without a dropout layer\\nand in between is just a very deep stack of simple residual units Each residual unit is\\ncomposed of two convolutional layers and no pooling layer with Batch Normaliza\\ntion BN and ReLU activation using 3  3 kernels and preserving spatial dimensions\\nstride 1 SAM', 'maliza\\ntion BN and ReLU activation using 3  3 kernels and preserving spatial dimensions\\nstride 1 SAME padding\\nFigure 1417 ResNet architecture\\nNote that the number of feature maps is doubled every few residual units at the same\\ntime as their height and width are halved using a convolutional layer with stride 2\\nWhen this happens the inputs cannot be added directly to the outputs of the residual\\nunit since they dont have the same shape for example this problem affects the skip\\n458  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks16Inceptionv4 InceptionResNet and the Impact of Residual Connections on Learning  C Szegedy et al\\n2016\\n17Xception Deep Learning with Depthwise Separable Convolutions  Franois Chollet 2016\\nconnection represented by the dashed arrow in Figure 1417  To solve this problem\\nthe inputs are passed through a 1  1 convolutional layer with stride 2 and the right\\nnumber of output feature maps see Figure 1418 \\nFigure 1418 Skip connection when changing featur', 'ight\\nnumber of output feature maps see Figure 1418 \\nFigure 1418 Skip connection when changing feature map size and depth\\nResNet34 is the ResNet with 34 layers only counting the convolutional layers and\\nthe fully connected layer containing three residual units that output 64 feature maps\\n4 RUs with 128 maps 6 RUs with 256 maps and 3 RUs with 512 maps We will imple\\nment this architecture later in this chapter\\nResNets deeper than that such as ResNet152 use slightly different residual units\\nInstead of two 3  3 convolutional layers with say 256 feature maps they use three\\nconvolutional layers first a 1  1 convolutional layer with just 64 feature maps 4\\ntimes less which acts as a bottleneck layer as discussed already then a 3  3 layer\\nwith 64 feature maps and finally another 1  1 convolutional layer with 256 feature\\nmaps 4 times 64 that restores the original depth ResNet152 contains three such\\nRUs that output 256 maps then 8 RUs with 512 maps a whopping 36 RUs with 1024\\nmaps and finally 3 RU', '\\nRUs that output 256 maps then 8 RUs with 512 maps a whopping 36 RUs with 1024\\nmaps and finally 3 RUs with 2048 maps\\nGoogles Inceptionv416 architecture merged the ideas of GoogLe\\nNet and ResNet and achieved close to 3 top5 error rate on\\nImageNet classification\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting Xception17\\nwhich stands for Extreme Inception  was proposed in 2016 by Franois Chollet the\\nCNN Architectures  45918This name can sometimes be ambiguous since spatially separable convolutions are often called separable\\nconvolutions as wellauthor of Keras and it significantly outperformed Inceptionv3 on a huge vision task\\n350 million images and 17000 classes Just like Inceptionv4 it also merges the\\nideas of GoogLeNet and ResNet but it replaces the inception modules with a special\\ntype of layer called a depthwise separable convolution  or separable convolution  for\\nshort18 These layers had been used before in some CNN architectures but they were\\nnot as centr', '  for\\nshort18 These layers had been used before in some CNN architectures but they were\\nnot as central as in the Xception architecture While a regular convolutional layer\\nuses filters that try to simultaneously capture spatial patterns eg an oval and cross\\nchannel patterns eg mouth  nose  eyes  face a separable convolutional layer\\nmakes the strong assumption that spatial patterns and crosschannel patterns can be\\nmodeled separately see Figure 1419  Thus it is composed of two parts the first part\\napplies a single spatial filter for each input feature map then the second part looks\\nexclusively for crosschannel patternsit is just a regular convolutional layer with 1 \\n1 filters\\nFigure 1419 Depthwise Separable Convolutional Layer\\nSince separable convolutional layers only have one spatial filter per input channel\\nyou should avoid using them after layers that have too few channels such as the input\\nlayer granted thats what Figure 1419  represents but it is just for illustration pur\\nposes For t', 'put\\nlayer granted thats what Figure 1419  represents but it is just for illustration pur\\nposes For this reason the Xception architecture starts with 2 regular convolutional\\nlayers but then the rest of the architecture uses only separable convolutions 34 in\\n460  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks19Crafting GBDNet for Object Detection  X Zeng et al 2016\\n20SqueezeandExcitation Networks  Jie Hu et al 2017\\nall plus a few max pooling layers and the usual final layers a global average pooling\\nlayer and a dense output layer\\nY ou might wonder why Xception is considered a variant of GoogLeNet since it con\\ntains no inception module at all Well as we discussed earlier an Inception module\\ncontains convolutional layers with 1  1 filters these look exclusively for cross\\nchannel patterns However the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and crosschannel patterns So you can\\nthink of an Inception module as ', 'ers that look both for spatial and crosschannel patterns So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer which considers spatial patterns and crosschannel patterns jointly and a sepa\\nrable convolutional layer which considers them separately In practice it seems that\\nseparable convolutions generally perform better\\nSeparable convolutions use less parameters less memory and less\\ncomputations than regular convolutional layers and in general\\nthey even perform better so you should consider using them by\\ndefault except after layers with few channels\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni\\nversity of Hong Kong They used an ensemble of many different techniques includ\\ning a sophisticated objectdetection system called GBDNet19 to achieve a top5 error\\nrate below 3 Although this result is unquestionably impressive the complexity of\\nthe solution contrasted with the simplicity of ResNets Moreover one year later\\nanoth', 'e complexity of\\nthe solution contrasted with the simplicity of ResNets Moreover one year later\\nanother fairly simple architecture performed even better as we will see now\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeezeand\\nExcitation Network  SENet20 This architecture extends existing architectures such as\\ninception networks or ResNets and boosts their performance This allowed SENet to\\nwin the competition with an astonishing 225 top5 error rate The extended ver\\nsions of inception networks and ResNet are called SEInception  and SEResNet  respec\\ntively The boost comes from the fact that a SENet adds a small neural network called\\na SE Block  to every unit in the original architecture ie every inception module or\\nevery residual unit as shown in Figure 1420 \\nCNN Architectures  461Figure 1420 SEInception Module left  and SEResNet Unit right\\nA SE Block analyzes the output of the unit it is attached to focusing exclusively on\\nthe depth dimension it does not look for ', 'tput of the unit it is attached to focusing exclusively on\\nthe depth dimension it does not look for any spatial pattern and it learns which fea\\ntures are usually most active together It then uses this information to recalibrate the\\nfeature maps as shown in Figure 1421  For example a SE Block may learn that\\nmouths noses and eyes usually appear together in pictures if you see a mouth and a\\nnose you should expect to see eyes as well So if a SE Block sees a strong activation in\\nthe mouth and nose feature maps but only mild activation in the eye feature map it\\nwill boost the eye feature map more accurately it will reduce irrelevant feature\\nmaps If the eyes were somewhat confused with something else this feature map\\nrecalibration will help resolve the ambiguity\\n462  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksFigure 1421 An SE Block Performs Feature Map Recalibration\\nA SE Block is composed of just 3 layers a global average pooling layer a hidden dense\\nlayer using the R', 'E Block is composed of just 3 layers a global average pooling layer a hidden dense\\nlayer using the ReLU activation function and a dense output layer using the sigmoid\\nactivation function see Figure 1422 \\nFigure 1422 SE Block Architecture\\nCNN Architectures  463As earlier the global average pooling layer computes the mean activation for each fea\\nture map for example if its input contains 256 feature maps it will output 256 num\\nbers representing the overall level of response for each filter The next layer is where\\nthe squeeze happens this layer has much less than 256 neurons typically 16 times\\nless than the number of feature maps eg 16 neurons so the 256 numbers get com\\npressed into a small vector eg 16 dimensional This is a lowdimensional vector\\nrepresentation ie an embedding of the distribution of feature responses This bot\\ntleneck step forces the SE Block to learn a general representation of the feature com\\nbinations we will see this principle in action again when we discuss autoencode', 'n of the feature com\\nbinations we will see this principle in action again when we discuss autoencoders\\nin  Finally the output layer takes the embedding and outputs a recalibration vec\\ntor containing one number per feature map eg 256 each between 0 and 1 The\\nfeature maps are then multiplied by this recalibration vector so irrelevant features\\nwith a low recalibration score get scaled down while relevant features with a recali\\nbration score close to 1 are left alone\\nImplementing a ResNet34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\nalthough generally you would load a pretrained network instead as we will see To\\nillustrate the process lets implement a ResNet34 from scratch using Keras First lets\\ncreate a ResidualUnit  layer\\nDefaultConv2D   partialkeraslayersConv2D kernelsize 3 strides1\\n                        paddingSAME usebias False\\nclass ResidualUnit keraslayersLayer\\n    def init self filters strides1 activation relu kwargs\\n        su', 's ResidualUnit keraslayersLayer\\n    def init self filters strides1 activation relu kwargs\\n        superinit kwargs\\n        selfactivation   kerasactivations getactivation \\n        selfmainlayers   \\n            DefaultConv2D filters stridesstrides\\n            keraslayersBatchNormalization \\n            selfactivation \\n            DefaultConv2D filters\\n            keraslayersBatchNormalization \\n        selfskiplayers   \\n        if strides  1\\n            selfskiplayers   \\n                DefaultConv2D filters kernelsize 1 stridesstrides\\n                keraslayersBatchNormalization \\n    def callself inputs\\n        Z  inputs\\n        for layer in selfmainlayers \\n            Z  layerZ\\n        skipZ  inputs\\n        for layer in selfskiplayers \\n464  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks            skipZ  layerskipZ\\n        return selfactivation Z  skipZ\\nAs you can see this code matches Figure 1418  pretty closely In the constructor we\\ncreate all the layers we will ', 'e this code matches Figure 1418  pretty closely In the constructor we\\ncreate all the layers we will need the main layers are the ones on the right side of the\\ndiagram and the skip layers are the ones on the left only needed if the stride is\\ngreater than 1 Then in the call  method we simply make the inputs go through\\nthe main layers and the skip layers if any then we add both outputs and we apply\\nthe activation function\\nNext we can build the ResNet34 simply using a Sequential  model since it is really\\njust a long sequence of layers we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit  class\\nmodel  kerasmodelsSequential \\nmodeladdDefaultConv2D 64 kernelsize 7 strides2\\n                        inputshape 224 224 3\\nmodeladdkeraslayersBatchNormalization \\nmodeladdkeraslayersActivation relu\\nmodeladdkeraslayersMaxPool2D poolsize 3 strides2 paddingSAME\\nprevfilters   64\\nfor filters in 64  3  128  4  256  6  512  3\\n    strides  1 if filters  prevfilters  else 2\\n    mo', '4\\nfor filters in 64  3  128  4  256  6  512  3\\n    strides  1 if filters  prevfilters  else 2\\n    modeladdResidualUnit filters stridesstrides\\n    prevfilters   filters\\nmodeladdkeraslayersGlobalAvgPool2D \\nmodeladdkeraslayersFlatten\\nmodeladdkeraslayersDense10 activation softmax \\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\\nto the model as explained earlier the first 3 RUs have 64 filters then the next 4 RUs\\nhave 128 filters and so on We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU or else we set it to 2 Then we add the ResidualUnit \\nand finally we update prevfilters \\nIt is quite amazing that in less than 40 lines of code we can build the model that won\\nthe ILSVRC 2015 challenge It demonstrates both the elegance of the ResNet model\\nand the expressiveness of the Keras API Implementing the other CNN architectures\\nis not much harder However Keras comes with several of these architectures built in\\nso why no', 'ctures\\nis not much harder However Keras comes with several of these architectures built in\\nso why not use them instead\\nUsing Pretrained Models From Keras\\nIn general you wont have to implement standard models like GoogLeNet or ResNet\\nmanually since pretrained networks are readily available with a single line of code in\\nthe kerasapplications  package For example\\nmodel  kerasapplications resnet50 ResNet50 weightsimagenet \\nUsing Pretrained Models From Keras  46521In the ImageNet dataset each image is associated to a word in the WordNet dataset  the class ID is just a\\nWordNet ID\\nThats all This will create a ResNet50 model and download weights pretrained on\\nthe ImageNet dataset To use it you first need to ensure that the images have the right\\nsize A ResNet50 model expects 224  224 images other models may expect other\\nsizes such as 299  299 so lets use TensorFlows tfimageresize  function to\\nresize the images we loaded earlier\\nimagesresized   tfimageresizeimages 224 224\\nThe tfimageresize  will', 'ize the images we loaded earlier\\nimagesresized   tfimageresizeimages 224 224\\nThe tfimageresize  will not preserve the aspect ratio If this is\\na problem you can try cropping the images to the appropriate\\naspect ratio before resizing Both operations can be done in one\\nshot with tfimagecropandresize \\nThe pretrained models assume that the images are preprocessed in a specific way In\\nsome cases they may expect the inputs to be scaled from 0 to 1 or 1 to 1 and so on\\nEach model provides a preprocessinput  function that you can use to preprocess\\nyour images These functions assume that the pixel values range from 0 to 255 so we\\nmust multiply them by 255 since earlier we scaled them to the 01 range\\ninputs  kerasapplications resnet50 preprocessinput imagesresized   255\\nNow we can use the pretrained model to make predictions\\nYproba  modelpredictinputs\\nAs usual the output Yproba  is a matrix with one row per image and one column per\\nclass in this case there are 1000 classes If you want to display t', ' row per image and one column per\\nclass in this case there are 1000 classes If you want to display the top K predic\\ntions including the class name and the estimated probability of each predicted class\\nyou can use the decodepredictions  function For each image it returns an array\\ncontaining the top K predictions where each prediction is represented as an array\\ncontaining the class identifier21 its name and the corresponding confidence score\\ntopK  kerasapplications resnet50 decodepredictions Yproba top3\\nfor imageindex  in rangelenimages\\n    printImage  formatimageindex \\n    for classid  name yproba in topKimageindex \\n        print    12s 2f formatclassid  name yproba  100\\n    print\\nThe output looks like this\\nImage 0\\n  n03877845  palace       4287\\n  n02825657  bellcote    4057\\n  n03781244  monastery    1456\\n466  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksImage 1\\n  n04522168  vase         4683\\n  n07930864  cup          778\\n  n11939491  daisy        487\\nThe correct c', '4522168  vase         4683\\n  n07930864  cup          778\\n  n11939491  daisy        487\\nThe correct classes monastery and daisy appear in the top 3 results for both images\\nThats pretty good considering that the model had to choose among 1000 classes\\nAs you can see it is very easy to create a pretty good image classifier using a pre\\ntrained model Other vision models are available in kerasapplications  including\\nseveral ResNet variants GoogLeNet variants like InceptionV3 and Xception\\nVGGNet variants MobileNet and MobileNetV2 lightweight models for use in\\nmobile applications and more\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet In that case you may still benefit from the pretrained models to per\\nform transfer learning\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier but you do not have enough training data\\nthen it is often a good idea to reuse the lower layers of a pretrained model as we dis\\ncussed in Ch', '\\nthen it is often a good idea to reuse the lower layers of a pretrained model as we dis\\ncussed in Chapter 11  For example lets train a model to classify pictures of flowers\\nreusing a pretrained Xception model First lets load the dataset using TensorFlow\\nDatasets see Chapter 13 \\nimport tensorflowdatasets  as tfds\\ndataset info  tfdsloadtfflowers  assupervised True withinfo True\\ndatasetsize   infosplitstrainnumexamples   3670\\nclassnames   infofeatures labelnames  dandelion daisy \\nnclasses   infofeatures labelnumclasses   5\\nNote that you can get information about the dataset by setting withinfoTrue  Here\\nwe get the dataset size and the names of the classes Unfortunately there is only a\\ntrain  dataset no test set or validation set so we need to split the training set The\\nTF Datasets project provides an API for this For example lets take the first 10 of\\nthe dataset for testing the next 15 for validation and the remaining 75 for train\\ning\\ntestsplit  validsplit  trainsplit   tfdsSplitTRAINsubs', 'validation and the remaining 75 for train\\ning\\ntestsplit  validsplit  trainsplit   tfdsSplitTRAINsubsplit 10 15 75\\ntestset   tfdsloadtfflowers  splittestsplit  assupervised True\\nvalidset   tfdsloadtfflowers  splitvalidsplit  assupervised True\\ntrainset   tfdsloadtfflowers  splittrainsplit  assupervised True\\nPretrained Models for Transfer Learning  467Next we must preprocess the images The CNN expects 224  224 images so we need\\nto resize them We also need to run the image through Xceptions prepro\\ncessinput  function\\ndef preprocess image label\\n    resizedimage   tfimageresizeimage 224 224\\n    finalimage   kerasapplications xception preprocessinput resizedimage \\n    return finalimage  label\\nLets apply this preprocessing function to all 3 datasets and lets also shuffle  repeat\\nthe training set and add batching  prefetching to all datasets\\nbatchsize   32\\ntrainset   trainset shuffle1000repeat\\ntrainset   trainset mappreprocess batchbatchsize prefetch 1\\nvalidset   validset mappreprocess batchbat', 'ainset   trainset mappreprocess batchbatchsize prefetch 1\\nvalidset   validset mappreprocess batchbatchsize prefetch 1\\ntestset   testset mappreprocess batchbatchsize prefetch 1\\nIf you want to perform some data augmentation you can just change the preprocess\\ning function for the training set adding some random transformations to the training\\nimages For example use tfimagerandomcrop  to randomly crop the images use\\ntfimagerandomflipleftright  to randomly flip the images horizontally and\\nso on see the notebook for an example\\nNext lets load an Xception model pretrained on ImageNet We exclude the top of the\\nnetwork by setting includetopFalse  this excludes the global average pooling\\nlayer and the dense output layer We then add our own global average pooling layer\\nbased on the output of the base model followed by a dense output layer with 1 unit\\nper class using the softmax activation function Finally we create the Keras Model \\nbasemodel   kerasapplications xception Xception weightsimagenet \\n ', 'inally we create the Keras Model \\nbasemodel   kerasapplications xception Xception weightsimagenet \\n                                                  includetop False\\navg  keraslayersGlobalAveragePooling2D basemodel output\\noutput  keraslayersDensenclasses  activation softmax avg\\nmodel  kerasmodelsModelinputsbasemodel input outputsoutput\\nAs explained in Chapter 11  its usually a good idea to freeze the weights of the pre\\ntrained layers at least at the beginning of training\\nfor layer in basemodel layers\\n    layertrainable   False\\nSince our model uses the base models layers directly rather than\\nthe basemodel  object itself setting basemodeltrainableFalse\\nwould have no effect\\nFinally we can compile the model and start training\\n468  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksoptimizer   kerasoptimizers SGDlr02 momentum 09 decay001\\nmodelcompilelosssparsecategoricalcrossentropy  optimizer optimizer \\n              metricsaccuracy \\nhistory  modelfittrainset \\n             ', 'ntropy  optimizer optimizer \\n              metricsaccuracy \\nhistory  modelfittrainset \\n                    stepsperepoch int075  datasetsize   batchsize \\n                    validationdata validset \\n                    validationsteps int015  datasetsize   batchsize \\n                    epochs5\\nThis will be very slow unless you have a GPU If you do not then\\nyou should run this chapters notebook in Colab using a GPU run\\ntime its free See the instructions at httpsgithubcomageron\\nhandsonml2 \\nAfter training the model for a few epochs its validation accuracy should reach about\\n7580 and stop making much progress This means that the top layers are now\\npretty well trained so we are ready to unfreeze all layers or you could try unfreezing\\njust the top ones and continue training dont forget to compile the model when you\\nfreeze or unfreeze layers This time we use a much lower learning rate to avoid dam\\naging the pretrained weights\\nfor layer in basemodel layers\\n    layertrainable   True\\noptimizer ', ' dam\\naging the pretrained weights\\nfor layer in basemodel layers\\n    layertrainable   True\\noptimizer   kerasoptimizers SGDlr001 momentum 09 decay0001\\nmodelcompile\\nhistory  modelfit\\nIt will take a while but this model should reach around 95 accuracy on the test set\\nWith that you can start training amazing image classifiers But theres more to com\\nputer vision than just classification For example what if you also want to know where\\nthe flower is in the picture Lets look at this now\\nClassification  and Localization\\nLocalizing an object in a picture can be expressed as a regression task as discussed in\\nChapter 10  to predict a bounding box around the object a common approach is to\\npredict the horizontal and vertical coordinates of the objects center as well as its\\nheight and width This means we have 4 numbers to predict It does not require much\\nchange to the model we just need to add a second dense output layer with 4 units\\ntypically on top of the global average pooling layer and it can be t', 'dense output layer with 4 units\\ntypically on top of the global average pooling layer and it can be trained using the\\nMSE loss\\nbasemodel   kerasapplications xception Xception weightsimagenet \\n                                                  includetop False\\navg  keraslayersGlobalAveragePooling2D basemodel output\\nclassoutput   keraslayersDensenclasses  activation softmax avg\\nClassification  and Localization  46922Crowdsourcing in Computer Vision  A Kovashka et al 2016\\nlocoutput   keraslayersDense4avg\\nmodel  kerasmodelsModelinputsbasemodel input\\n                           outputsclassoutput  locoutput \\nmodelcompilelosssparsecategoricalcrossentropy  mse\\n              lossweights 08 02  depends on what you care most about\\n              optimizer optimizer  metricsaccuracy \\nBut now we have a problem the flowers dataset does not have bounding boxes\\naround the flowers So we need to add them ourselves This is often one of the hard\\nest and most costly part of a Machine Learning project getting ', 'selves This is often one of the hard\\nest and most costly part of a Machine Learning project getting the labels Its a good\\nidea to spend time looking for the right tools To annotate images with bounding\\nboxes you may want to use an open source image labeling tool like VGG Image\\nAnnotator LabelImg OpenLabeler or ImgLab or perhaps a commercial tool like\\nLabelBox or Supervisely Y ou may also want to consider crowdsourcing platforms\\nsuch as Amazon Mechanical Turk or CrowdFlower if you have a very large number of\\nimages to annotate However it is quite a lot of work to setup a crowdsourcing plat\\nform prepare the form to be sent to the workers to supervise them and ensure the\\nquality of the bounding boxes they produce is good so make sure it is worth the\\neffort if there are just a few thousand images to label and you dont plan to do this\\nfrequently it may be preferable to do it yourself Adriana Kovashka et al wrote a very\\npractical paper22 about crowdsourcing in Computer Vision I recommend you', 'Kovashka et al wrote a very\\npractical paper22 about crowdsourcing in Computer Vision I recommend you check\\nit out even if you do not plan to use crowdsourcing\\nSo lets suppose you obtained the bounding boxes for every image in the flowers data\\nset for now we will assume there is a single bounding box per image you then need\\nto create a dataset whose items will be batches of preprocessed images along with\\ntheir class labels and their bounding boxes Each item should be a tuple of the form\\nimages classlabels boundingboxes  Then you are ready to train your\\nmodel\\nThe bounding boxes should be normalized so that the horizontal\\nand vertical coordinates as well as the height and width all range\\nfrom 0 to 1 Also it is common to predict the square root of the\\nheight and width rather than the height and width directly this\\nway a 10 pixel error for a large bounding box will not be penalized\\nas much as a 10 pixel error for a small bounding box\\nThe MSE often works fairly well as a cost function to tra', ' a 10 pixel error for a small bounding box\\nThe MSE often works fairly well as a cost function to train the model but it is not a\\ngreat metric to evaluate how well the model can predict bounding boxes The most\\ncommon metric for this is the Intersection over Union IoU it is the area of overlap\\nbetween the predicted bounding box and the target bounding box divided by the\\n470  Chapter 14 Deep Computer Vision Using Convolutional Neural Networksarea of their union see Figure 1423  In tfkeras it is implemented by the\\ntfkerasmetricsMeanIoU  class\\nFigure 1423 Intersection over Union IoU Metric for Bounding Boxes\\nClassifying and localizing a single object is nice but what if the images contain multi\\nple objects as is often the case in the flowers dataset\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection  Until a few years ago a common approach was to take a CNN that was\\ntrained to classify and locate a single object then slide it acro', 'pproach was to take a CNN that was\\ntrained to classify and locate a single object then slide it across the image as shown\\nin Figure 1424  In this example the image was chopped into a 6  8 grid and we\\nshow a CNN the thick black rectangle sliding across all 3  3 regions When the\\nCNN was looking at the top left of the image it detected part of the leftmost rose\\nand then it detected that same rose again when it was first shifted one step to the\\nright At the next step it started detecting part of the topmost rose and then it detec\\nted it again once it was shifted one more step to the right Y ou would then continue to\\nslide the CNN through the whole image looking at all 3  3 regions Moreover since\\nobjects can have varying sizes you would also slide the CNN across regions of differ\\nent sizes For example once you are done with the 3  3 regions you might want to\\nslide the CNN across all 4  4 regions as well\\nObject Detection  471Figure 1424 Detecting Multiple Objects by Sliding a CNN Across the ', 'ons as well\\nObject Detection  471Figure 1424 Detecting Multiple Objects by Sliding a CNN Across the Image\\nThis technique is fairly straightforward but as you can see it will detect the same\\nobject multiple times at slightly different positions Some postprocessing will then\\nbe needed to get rid of all the unnecessary bounding boxes A common approach for\\nthis is called nonmax suppression \\nFirst you need to add an extra objectness  output to your CNN to estimate the\\nprobability that a flower is indeed present in the image alternatively you could\\nadd a noflower class but this usually does not work as well It must use the\\nsigmoid activation function and you can train it using the binarycrossen\\ntropy  loss Then just get rid of all the bounding boxes for which the objectness\\nscore is below some threshold this will drop all the bounding boxes that dont\\nactually contain a flower\\nSecond find the bounding box with the highest objectness score and get rid of\\nall the other bounding boxes that overl', 'ounding box with the highest objectness score and get rid of\\nall the other bounding boxes that overlap a lot with it eg with an IoU greater\\nthan 60 For example in Figure 1424  the bounding box with the max object\\nness score is the thick bounding box over the topmost rose the objectness score\\nis represented by the thickness of the bounding boxes The other bounding box\\nover that same rose overlaps a lot with the max bounding box so we will get rid\\nof it\\n472  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks23Fully Convolutional Networks for Semantic Segmentation  J Long E Shelhamer T Darrell 2015\\n24There is one small exception a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size\\nThird repeat step two until there are no more bounding boxes to get rid of\\nThis simple approach to object detection works pretty well but it requires running\\nthe CNN many times so it is quite slow Fortunately there is a much faster way to\\nsli', 'equires running\\nthe CNN many times so it is quite slow Fortunately there is a much faster way to\\nslide a CNN across an image using a Fully Convolutional Network \\nFully Convolutional Networks FCNs\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al for\\nsemantic segmentation the task of classifying every pixel in an image according to\\nthe class of the object it belongs to They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers To understand this lets look\\nat an example suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps each of size 7  7 this is the feature map size not\\nthe kernel size Each neuron will compute a weighted sum of all 100  7  7 activa\\ntions from the convolutional layer plus a bias term Now lets see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters each 7  7 and with\\nV ALID padding This layer will output 200 feature', 'olution layer using 200 filters each 7  7 and with\\nV ALID padding This layer will output 200 feature maps each 1  1 since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding In\\nother words it will output 200 numbers just like the dense layer did and if you look\\nclosely at the computations performed by a convolutional layer you will notice that\\nthese numbers will be precisely the same as the dense layer produced The only differ\\nence is that the dense layers output was a tensor of shape batch size 200 while the\\nconvolutional layer will output a tensor of shape batch size 1 1 200\\nTo convert a dense layer to a convolutional layer the number of fil\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer the filter size must be equal to the size of the\\ninput feature maps and you must use V ALID padding The stride\\nmay be set to 1 or more as we will see shortly\\nWhy is this important Well while a dense layer expects a specific', 'o 1 or more as we will see shortly\\nWhy is this important Well while a dense layer expects a specific input size since it\\nhas one weight per input feature a convolutional layer will happily process images of\\nany size24 however it does expect its inputs to have a specific number of channels\\nsince each kernel contains a different set of weights for each input channel Since an\\nFCN contains only convolutional layers and pooling layers which have the same\\nproperty it can be trained and executed on images of any size\\nObject Detection  47325This assumes we used only SAME padding in the network indeed V ALID padding would reduce the size of\\nthe feature maps Moreover 448 can be neatly divided by 2 several times until we reach 7 without any round\\ning error If any layer uses a different stride than 1 or 2 then there may be some rounding error so again the\\nfeature maps may end up being smallerFor example suppose we already trained a CNN for flower classification and localiza\\ntion It was trained on ', 'mple suppose we already trained a CNN for flower classification and localiza\\ntion It was trained on 224  224 images and it outputs 10 numbers outputs 0 to 4 are\\nsent through the softmax activation function and this gives the class probabilities\\none per class output 5 is sent through the logistic activation function and this gives\\nthe objectness score outputs 6 to 9 do not use any activation function and they rep\\nresent the bounding boxs center coordinates and its height and width We can now\\nconvert its dense layers to convolutional layers In fact we dont even need to retrain\\nit we can just copy the weights from the dense layers to the convolutional layers\\nAlternatively we could have converted the CNN into an FCN before training\\nNow suppose the last convolutional layer before the output layer also called the bot\\ntleneck layer outputs 7  7 feature maps when the network is fed a 224  224 image\\nsee the left side of Figure 1425  If we feed the FCN a 448  448 image see the right\\nside of Figu', 'age\\nsee the left side of Figure 1425  If we feed the FCN a 448  448 image see the right\\nside of Figure 1425  the bottleneck layer will now output 14  14 feature maps25\\nSince the dense output layer was replaced by a convolutional layer using 10 filters of\\nsize 7  7 V ALID padding and stride 1 the output will be composed of 10 features\\nmaps each of size 8  8 since 14  7  1  8 In other words the FCN will process\\nthe whole image only once and it will output an 8  8 grid where each cell contains 10\\nnumbers 5 class probabilities 1 objectness score and 4 bounding box coordinates\\nIts exactly like taking the original CNN and sliding it across the image using 8 steps\\nper row and 8 steps per column to visualize this imagine chopping the original\\nimage into a 14  14 grid then sliding a 7  7 window across this grid there will be 8\\n 8  64 possible locations for the window hence 8  8 predictions However the\\nFCN approach is much  more efficient since the network only looks at the image\\nonce In fact Yo', 'r the\\nFCN approach is much  more efficient since the network only looks at the image\\nonce In fact You Only Look Once  YOLO is the name of a very popular object detec\\ntion architecture\\n474  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks26Y ou Only Look Once Unified RealTime Object Detection  J Redmon S Divvala R Girshick A Farhadi\\n2015\\n27YOLO9000 Better Faster Stronger  J Redmon A Farhadi 2016\\n28YOLOv3 An Incremental Improvement  J Redmon A Farhadi 2018\\nFigure 1425 A Fully Convolutional Network Processing a Small Image left  and a\\nLarge One right\\nYou Only Look Once YOLO\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al in a 2015 paper26 and subsequently improved in 201627\\nYOLOv2 and in 201828 YOLOv3 It is so fast that it can run in realtime on a video\\ncheck out this nice demo \\nYOLOv3s architecture is quite similar to the one we just discussed but with a few\\nimportant differences\\nObject Detection  475First it outputs', 'the one we just discussed but with a few\\nimportant differences\\nObject Detection  475First it outputs 5 bounding boxes for each grid cell instead of just 1 and each\\nbounding box comes with an objectness score It also outputs 20 class probabili\\nties per grid cell as it was trained on the PASCAL VOC dataset which contains\\n20 classes Thats a total of 45 numbers per grid cell 5  4 bounding box coordi\\nnates plus 5 objectness scores plus 20 class probabilities\\nSecond instead of predicting the absolute coordinates of the bounding box cen\\nters YOLOv3 predicts an offset relative to the coordinates of the grid cell where\\n0 0 means the top left of that cell and 1 1 means the bottom right For each\\ngrid cell YOLOv3 is trained to predict only bounding boxes whose center lies in\\nthat cell but the bounding box itself generally extends well beyond the grid cell\\nYOLOv3 applies the logistic activation function to the bounding box coordinates\\nto ensure they remain in the 0 to 1 range\\nThird before training ', 'ion to the bounding box coordinates\\nto ensure they remain in the 0 to 1 range\\nThird before training the neural net YOLOv3 finds 5 representative bounding\\nbox dimensions called anchor boxes  or bounding box priors  it does this by\\napplying the KMeans algorithm see  to the height and width of the training\\nset bounding boxes For example if the training images contain many pedes\\ntrians then one of the anchor boxes will likely have the dimensions of a typical\\npedestrian Then when the neural net predicts 5 bounding boxes per grid cell it\\nactually predicts how much to rescale each of the anchor boxes For example\\nsuppose one anchor box is 100 pixels tall and 50 pixels wide and the network\\npredicts say a vertical rescaling factor of 15 and a horizontal rescaling of 09 for\\none of the grid cells this will result in a predicted bounding box of size 150  45\\npixels To be more precise for each grid cell and each anchor box the network\\npredicts the log of the vertical and horizontal rescaling factors ', 'l and each anchor box the network\\npredicts the log of the vertical and horizontal rescaling factors Having these pri\\nors makes the network more likely to predict bounding boxes of the appropriate\\ndimensions and it also speeds up training since it will more quickly learn what\\nreasonable bounding boxes look like\\nFourth the network is trained using images of different scales every few batches\\nduring training the network randomly chooses a new image dimension from\\n330  330 to 608  608 pixels This allows the network to learn to detect objects\\nat different scales Moreover it makes it possible to use YOLOv3 at different\\nscales the smaller scale will be less accurate but faster than the larger scale so\\nyou can choose the right tradeoff for your use case\\nThere are a few more innovations you might be interested in such as the use of skip\\nconnections to recover some of the spatial resolution that is lost in the CNN we will\\ndiscuss this shortly when we look at semantic segmentation Moreover in the', 's lost in the CNN we will\\ndiscuss this shortly when we look at semantic segmentation Moreover in the 2016\\npaper the authors introduce the YOLO9000 model that uses hierarchical classifica\\ntion the model predicts a probability for each node in a visual hierarchy called Word\\nTree This makes it possible for the network to predict with high confidence that an\\nimage represents say a dog even though it is unsure what specific type of dog it is\\n476  Chapter 14 Deep Computer Vision Using Convolutional Neural NetworksSo I encourage you to go ahead and read all three papers they are quite pleasant to\\nread and it is an excellent example of how Deep Learning systems can be incremen\\ntally improved\\nMean Average Precision mAP\\nA very common metric used in object detection tasks is the mean Average Precision\\nmAP Mean Average sounds a bit redundant doesnt it To understand this met\\nric lets go back to two classification metrics we discussed in Chapter 3  precision and\\nrecall Remember the tradeoff the high', 'lassification metrics we discussed in Chapter 3  precision and\\nrecall Remember the tradeoff the higher the recall the lower the precision Y ou can\\nvisualize this in a PrecisionRecall curve see Figure 35  To summarize this curve\\ninto a single number we could compute its Area Under the Curve AUC But note\\nthat the PrecisionRecall curve may contain a few sections where precision actually\\ngoes up when recall increases especially at low recall values you can see this at the\\ntop left of Figure 35  This is one of the motivations for the mAP metric\\nSuppose the classifier has a 90 precision at 10 recall but a 96 precision at 20\\nrecall theres really no tradeoff here it simply makes more sense to use the classifier\\nat 20 recall rather than at 10 recall as you will get both higher recall and higher\\nprecision So instead of looking at the precision at 10 recall we should really be\\nlooking at the maximum  precision that the classifier can offer with at least  10 recall\\nIt would be 96 not 90 So one way', 'm  precision that the classifier can offer with at least  10 recall\\nIt would be 96 not 90 So one way to get a fair idea of the models performance is\\nto compute the maximum precision you can get with at least 0 recall then 10\\nrecall 20 and so on up to 100 and then calculate the mean of these maximum\\nprecisions This is called the Average Precision  AP metric Now when there are more\\nthan 2 classes we can compute the AP for each class and then compute the mean AP\\nmAP Thats it\\nHowever in an object detection systems there is an additional level of complexity\\nwhat if the system detected the correct class but at the wrong location ie the\\nbounding box is completely off Surely we should not count this as a positive predic\\ntion So one approach is to define an IOU threshold for example we may consider\\nthat a prediction is correct only if the IOU is greater than say 05 and the predicted\\nclass is correct The corresponding mAP is generally noted mAP05 or mAP50\\nor sometimes just AP50 In some competiti', 'ect The corresponding mAP is generally noted mAP05 or mAP50\\nor sometimes just AP50 In some competitions such as the Pascal VOC challenge\\nthis is what is done In others such as the COCO competition the mAP is computed\\nfor different IOU thresholds 050 055 060  095 and the final metric is the\\nmean of all these mAPs noted AP5095 or AP5000595 Y es thats a mean\\nmean average\\nSeveral YOLO implementations built using TensorFlow are available on github some\\nwith pretrained weights At the time of writing they are based on TensorFlow 1 but\\nby the time you read this TF 2 implementations will certainly be available Moreover\\nother object detection models are available in the TensorFlow Models project many\\nObject Detection  47729SSD Single Shot MultiBox Detector  Wei Liu et al 2015\\n30Faster RCNN Towards RealTime Object Detection with Region Proposal Networks  Shaoqing Ren et al\\n2015with pretrained weights and some have even been ported to TF Hub making them\\nextremely easy to use such as SSD29 and Fast', 'ts and some have even been ported to TF Hub making them\\nextremely easy to use such as SSD29 and FasterRCNN 30 which are both quite popu\\nlar SSD is also a single shot detection model quite similar to YOLO while Faster R\\nCNN is more complex the image first goes through a CNN and the output is passed\\nto a Region Proposal Network RPN which proposes bounding boxes that are most\\nlikely to contain an object and a classifier is run for each bounding box based on the\\ncropped output of the CNN\\nThe choice of detection system depends on many factors speed accuracy available\\npretrained models training time complexity etc The papers contain tables of met\\nrics but there is quite a lot of variability in the testing environments and the technol\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months\\nGreat So we can locate objects by drawing bounding boxes around them But per\\nhaps you might want to be a bit more', 'an locate objects by drawing bounding boxes around them But per\\nhaps you might want to be a bit more precise Lets see how to go down to the pixel\\nlevel\\nSemantic Segmentation\\nIn semantic segmentation  each pixel is classified according to the class of the object it\\nbelongs to eg road car pedestrian building etc as shown in Figure 1426  Note\\nthat different objects of the same class are not distinguished For example all the bicy\\ncles on the right side of the segmented image end up as one big lump of pixels The\\nmain difficulty in this task is that when images go through a regular CNN they grad\\nually lose their spatial resolution due to the layers with strides greater than 1 so a\\nregular CNN may end up knowing that theres a person in the image somewhere in\\nthe bottom left of the image but it will not be much more precise than that\\n478  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks31This type of layer is sometimes referred to as a deconvolution layer  but it does not pe', 'l Networks31This type of layer is sometimes referred to as a deconvolution layer  but it does not perform what mathemati\\ncians call a deconvolution so this name should be avoided\\nFigure 1426 Semantic segmentation\\nJust like for object detection there are many different approaches to tackle this prob\\nlem some quite complex However a fairly simple solution was proposed in the 2015\\npaper by Jonathan Long et al we discussed earlier They start by taking a pretrained\\nCNN and turning into an FCN as discussed earlier The CNN applies a stride of 32 to\\nthe input image overall ie if you add up all the strides greater than 1 meaning the\\nlast layer outputs feature maps that are 32 times smaller than the input image This is\\nclearly too coarse so they add a single upsampling layer  that multiplies the resolution\\nby 32 There are several solutions available for upsampling increasing the size of an\\nimage such as bilinear interpolation but it only works reasonably well up to 4 or\\n8 Instead they used a tra', 'uch as bilinear interpolation but it only works reasonably well up to 4 or\\n8 Instead they used a transposed convolutional layer 31 it is equivalent to first\\nstretching the image by inserting empty rows and columns full of zeros then per\\nforming a regular convolution see Figure 1427  Alternatively some people prefer to\\nthink of it as a regular convolutional layer that uses fractional strides eg 12 in\\nFigure 1427  The transposed convolutional layer  can be initialized to perform some\\nthing close to linear interpolation but since it is a trainable layer it will learn to do\\nbetter during training\\nSemantic Segmentation  479Figure 1427 Upsampling Using a Transpose Convolutional Layer\\nIn a transposed convolution layer the stride defines how much the\\ninput will be stretched not the size of the filter steps so the larger\\nthe stride the larger the output unlike for convolutional layers or\\npooling layers\\nTensorFlow Convolution Operations\\nTensorFlow also offers a few other kinds of convolutional l', 'layers\\nTensorFlow Convolution Operations\\nTensorFlow also offers a few other kinds of convolutional layers\\nkeraslayersConv1D  creates a convolutional layer for 1D inputs such as time\\nseries or text sequences of letters or words as we will see in \\nkeraslayersConv3D  creates a convolutional layer for 3D inputs such as 3D\\nPET scan\\nSetting the dilationrate  hyperparameter of any convolutional layer to a value\\nof 2 or more creates an trous convolutional layer   trous is French for with\\nholes This is equivalent to using a regular convolutional layer with a filter dila\\nted by inserting rows and columns of zeros ie holes For example a 1  3 filter\\nequal to 123  may be dilated with a dilation rate  of 4 resulting in a dilated\\nfilter  1 0 0 0 2 0 0 0 3  This allows the convolutional layer to\\n480  Chapter 14 Deep Computer Vision Using Convolutional Neural Networkshave a larger receptive field at no computational price and using no extra param\\neters\\ntfnndepthwiseconv2d  can be used to create a depth', 'omputational price and using no extra param\\neters\\ntfnndepthwiseconv2d  can be used to create a depthwise convolutional layer\\nbut you need to create the variables yourself It applies every filter to every\\nindividual input channel independently Thus if there are fn filters and fn input\\nchannels then this will output fn  fn feature maps\\nThis solution is okay but still too imprecise To do better the authors added skip con\\nnections from lower layers for example they upsampled the output image by a factor\\nof 2 instead of 32 and they added the output of a lower layer that had this double\\nresolution Then they upsampled the result by a factor of 16 leading to a total upsam\\npling factor of 32 see Figure 1428  This recovered some of the spatial resolution\\nthat was lost in earlier pooling layers In their best architecture they used a second\\nsimilar skip connection to recover even finer details from an even lower layer in\\nshort the output of the original CNN goes through the following extra steps u', 'an even lower layer in\\nshort the output of the original CNN goes through the following extra steps upscale\\n2 add the output of a lower layer of the appropriate scale upscale 2 add the out\\nput of an even lower layer and finally upscale 8 It is even possible to scale up\\nbeyond the size of the original image this can be used to increase the resolution of an\\nimage which is a technique called superresolution \\nFigure 1428 Skip layers recover some spatial resolution from lower layers\\nOnce again many github repositories provide TensorFlow implementations of\\nsemantic segmentation TensorFlow 1 for now and you will even find a pretrained\\ninstance segmentation  model in the TensorFlow Models project Instance segmenta\\ntion is similar to semantic segmentation but instead of merging all objects of the\\nsame class into one big lump each object is distinguished from the others eg it\\nidentifies each individual bicycle At the present they provide multiple implementa\\ntions of the Mask RCNN  architecture wh', 'dual bicycle At the present they provide multiple implementa\\ntions of the Mask RCNN  architecture which was proposed in a 2017 paper  it\\nextends the Faster RCNN model by additionally producing a pixelmask for each\\nbounding box So not only do you get a bounding box around each object with a set\\nof estimated class probabilities you also get a pixel mask that locates pixels in the\\nbounding box that belong to the object\\nSemantic Segmentation  48132Matrix Capsules with EM Routing  G Hinton S Sabour N Frosst 2018As you can see the field of Deep Computer Vision is vast and moving fast with all\\nsorts of architectures popping out every year all based on Convolutional Neural Net\\nworks The progress made in just a few years has been astounding and researchers\\nare now focusing on harder and harder problems such as adversarial learning  which\\nattempts to make the network more resistant to images designed to fool it explaina\\nbility understanding why the network makes a specific classification realist', 'ned to fool it explaina\\nbility understanding why the network makes a specific classification realistic image\\ngeneration  which we will come back to in  singleshot learning  a system that can\\nrecognize an object after it has seen it just once and much more Some even explore\\ncompletely novel architectures such as Geoffrey Hintons capsule networks32 I pre\\nsented them in a couple videos  with the corresponding code in a notebook Now on\\nto the next chapter where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks\\nExercises\\n1What are the advantages of a CNN over a fully connected DNN for image classi\\nfication\\n2Consider a CNN composed of three convolutional layers each with 3  3 kernels\\na stride of 2 and SAME padding The lowest layer outputs 100 feature maps the\\nmiddle one outputs 200 and the top one outputs 400 The input images are RGB\\nimages of 200  300 pixels What is the total number of parameters in the CNN\\nI', 'e input images are RGB\\nimages of 200  300 pixels What is the total number of parameters in the CNN\\nIf we are using 32bit floats at least how much RAM will this network require\\nwhen making a prediction for a single instance What about when training on a\\nminibatch of 50 images\\n3If your GPU runs out of memory while training a CNN what are five things you\\ncould try to solve the problem\\n4Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride\\n5When would you want to add a local response normalization  layer\\n6Can you name the main innovations in AlexNet compared to LeNet5 What\\nabout the main innovations in GoogLeNet ResNet SENet and Xception\\n7What is a Fully Convolutional Network How can you convert a dense layer into\\na convolutional layer\\n8What is the main technical difficulty of semantic segmentation\\n9Build your own CNN from scratch and try to achieve the highest possible accu\\nracy on MNIST\\n482  Chapter 14 Deep Computer Vision Using Convolutiona', 'ieve the highest possible accu\\nracy on MNIST\\n482  Chapter 14 Deep Computer Vision Using Convolutional Neural Networks10Use transfer learning for large image classification\\naCreate a training set containing at least 100 images per class For example you\\ncould classify your own pictures based on the location beach mountain city\\netc or alternatively you can just use an existing dataset eg from Tensor\\nFlow Datasets\\nbSplit it into a training set a validation set and a test set\\ncBuild the input pipeline including the appropriate preprocessing operations\\nand optionally add data augmentation\\ndFinetune a pretrained model on this dataset\\n11Go through TensorFlows DeepDream tutorial  It is a fun way to familiarize your\\nself with various ways of visualizing the patterns learned by a CNN and to gener\\nate art using Deep Learning\\nSolutions to these exercises are available in \\nExercises  483About the Author\\nAurlien Gron  is a Machine Learning consultant A former Googler he led the Y ou\\nTube video classi', 'or\\nAurlien Gron  is a Machine Learning consultant A former Googler he led the Y ou\\nTube video classification team from 2013 to 2016 He was also a founder and CTO of\\nWifirst from 2002 to 2012 a leading Wireless ISP in France and a founder and CTO\\nof Polyconseil in 2001 the firm that now manages the electric car sharing service\\nAutolib \\nBefore this he worked as an engineer in a variety of domains finance JP Morgan and\\nSocit Gnrale defense Canadas DOD and healthcare blood transfusion He\\npublished a few technical books on C WiFi and internet architectures and was\\na Computer Science lecturer in a French engineering school\\nA few fun facts he taught his three children to count in binary with their fingers up\\nto 1023 he studied microbiology and evolutionary genetics before going into soft\\nware engineering and his parachute didnt open on the second jump\\nColophon\\nThe animal on the cover of HandsOn Machine Learning with ScikitLearn and Ten\\nsorFlow  is the fire salamander  Salamandra salamandra  a', ' Machine Learning with ScikitLearn and Ten\\nsorFlow  is the fire salamander  Salamandra salamandra  an amphibian found across\\nmost of Europe Its black glossy skin features large yellow spots on the head and\\nback signaling the presence of alkaloid toxins This is a possible source of this\\namphibians common name contact with these toxins which they can also spray\\nshort distances causes convulsions and hyperventilation Either the painful poisons\\nor the moistness of the salamanders skin or both led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well\\nFire salamanders live in shaded forests hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding Though they spend\\nmost of their life on land they give birth to their young in water They subsist mostly\\non a diet of insects spiders slugs and worms Fire salamanders can grow up to a foot\\nin length and in captivity may live as lo', 'ers slugs and worms Fire salamanders can grow up to a foot\\nin length and in captivity may live as long as 50 years\\nThe fire salamanders numbers have been reduced by destruction of their forest habi\\ntat and capture for the pet trade but the greatest threat is the susceptibility of their\\nmoisturepermeable skin to pollutants and microbes Since 2014 they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus\\nMany of the animals on OReilly covers are endangered all of them are important to\\nthe world To learn more about how you can help go to animalsoreillycom \\nThe cover image is from Woods Illustrated Natural History  The cover fonts are URW\\nTypewriter and Guardian Sans The text font is Adobe Minion Pro the heading font\\nis Adobe Myriad Condensed and the code font is Dalton Maags Ubuntu Mono']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load embedder llm bert\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"bert-base-uncased\"  # You can choose a different model if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576,
          "referenced_widgets": [
            "c6dddc7676314387aa52aa7cd5025358",
            "104d0fa848b84711bf120318a61683e0",
            "15246c3d280443c78f2c36be6b2a1f28",
            "c8a9f9d1bb074efcab99914591ab80c4",
            "2fa7ee4ed1ca4a4ba500f9a6c9301423",
            "5c2265caa74d4db88932f69ed3699268",
            "055fc54c2c674deb859f7373407d6e82",
            "5a9e04879c374cb888ffd57efe5f8c56",
            "15682f76630942c7abd2d28b08b20ece",
            "56bce7aa62da4562a67d3816bac80e2e",
            "b17c8075539c49e4b57790caa9b6aee5",
            "7de8a5d6229e45f38d6da27396e700db",
            "8b65cefeb4db43399f96341d06591137",
            "73882d55571946b296b1f5641d70331a",
            "88dbff06d73d4e108804aee6fcaa2366",
            "e78936dd25954d59aedf419e3092f39d",
            "e5afe2623dc3452fba3c424c220e77c0",
            "736beb7ffb2b4914a9ed3f9bf5c939e7",
            "5efe96e7dcca47a18eb22495521eb1ad",
            "3b879793a2d94d628a5027e5b10cd82c",
            "54bc5bf458334b888f587e67d4f9773d",
            "2d0a65a50e1b4e5a9898e71fc63d1b75",
            "12d4a730e826493b96285a859b361529",
            "a65cdc9641034a04b807e48276655918",
            "ef252531af0640f3bf277fecc52f430d",
            "5c338a1fd90c411a9a1513958948b61f",
            "3da0b639df384e66ac4b1c52fd48eee8",
            "47bc132f360e4e37b4af1e3c28ec3089",
            "1ce40e0749034d1db9826536eb84a8bd",
            "b5993b5ebcf440879c50d517ef7f8539",
            "ce1fa5ebd4644966af6f45a5607337f3",
            "a0fdd0d961ed460f827e597646e85fb7",
            "1571014f9cf740f78fea960dd0ffaa9e",
            "b895216a560e430797cb020f3c2f40d5",
            "27ade560f8a145f58b608756ec35794b",
            "cdf6adf5d9214d6ea71d0ebc47729178",
            "dd3ed42d9f2a49a294172aa22457d8f8",
            "4f28930e930847a282496af9dcc09e19",
            "59706b5e7cb340b2aaa0c97ddd94159b",
            "438df17a1d824292a9b80f831293b2c8",
            "2fba7865e88546dd94e5a2900eb0c2fa",
            "26ddf46a0b0d40f98e306ed9e4c0f2aa",
            "f26d215612c84818aab14bffb5c3ca2a",
            "cd7fe51a4ccb4b84bb66bc49e10221ce",
            "e3198d01d8e44d27b5f5061a826c23e0",
            "bc3cff2301ed44328dd9bfe99b9ba739",
            "79b4b3c27e834f4f98f503f5ecf935aa",
            "f15d9ce711e24b2b9b02d0545359115b",
            "7376aec80d494915938ade3b70560f9c",
            "2a563e45aff54c81bcd9d1bc2902ee60",
            "03743e5c3d2f49c0a88b96171cc5e39f",
            "0da625a7abd6493687794883541ba5fa",
            "fe11454a24a74fa4b5bedb7d9b3c7bd8",
            "93acdac3337d4064872e259646e959cc",
            "0fb0800995644cd69011f3a4fc46919c"
          ]
        },
        "id": "jJqXeUpfoLck",
        "outputId": "0b9f0c4d-7bcb-4dda-a486-4a7b4f8b0a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6dddc7676314387aa52aa7cd5025358"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7de8a5d6229e45f38d6da27396e700db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12d4a730e826493b96285a859b361529"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b895216a560e430797cb020f3c2f40d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3198d01d8e44d27b5f5061a826c23e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: do embedding on text\n",
        "\n",
        "import torch\n",
        "\n",
        "def embed_text(text):\n",
        "  \"\"\"\n",
        "  Embeds the given text using the specified model.\n",
        "\n",
        "  Args:\n",
        "    text: The text to embed.\n",
        "\n",
        "  Returns:\n",
        "    A tensor containing the embeddings.\n",
        "  \"\"\"\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "  outputs = model(**inputs)\n",
        "  embeddings = outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token embedding\n",
        "  return embeddings\n",
        "\n",
        "# Example usage:\n",
        "embeddings = embed_text(chunks[0])\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZWluplgof7z",
        "outputId": "cb2b2fb9-64f7-499b-a3c0-f35a4509c60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-6.3629e-01, -1.9240e-02,  3.9091e-01,  1.0270e-01, -1.1747e-01,\n",
            "         -4.2392e-01, -1.6528e-01,  2.0605e-01, -1.1368e-02, -3.2024e-01,\n",
            "         -2.3666e-01, -1.9673e-01, -2.1077e-01,  1.3237e-01, -5.2791e-02,\n",
            "          3.3195e-01, -9.8500e-02,  6.7056e-01, -1.8390e-01,  2.5867e-01,\n",
            "          9.3409e-02, -7.5081e-01,  2.1375e-01, -2.0334e-01,  3.5591e-02,\n",
            "         -9.7601e-02, -1.4072e-01, -5.5314e-01, -1.4088e-01,  1.4356e-01,\n",
            "          2.5173e-01,  8.6991e-02,  2.3601e-01, -2.4474e-01,  4.5427e-01,\n",
            "         -1.7684e-01,  3.4473e-01, -2.4307e-01,  3.6959e-01,  1.3631e-01,\n",
            "         -1.0674e-01,  3.2495e-01,  4.0054e-01, -1.7346e-02,  3.6394e-01,\n",
            "          1.6227e-01, -2.8608e+00, -2.8698e-01, -1.6715e-01, -2.5160e-01,\n",
            "          2.8487e-01, -7.9746e-01,  4.7690e-01, -7.6341e-02,  3.3729e-01,\n",
            "          3.2816e-01, -9.0467e-01, -2.6900e-02,  2.2785e-01,  2.4180e-02,\n",
            "          1.8279e-01,  6.5056e-02, -1.8003e-01, -1.7512e-01,  2.8989e-03,\n",
            "          9.5459e-02, -2.2817e-01,  1.1963e-01, -3.5638e-01,  3.7457e-01,\n",
            "         -2.2853e-01,  2.1822e-01,  3.4314e-01, -2.8007e-01,  1.2717e-01,\n",
            "          1.9917e-01, -8.6698e-03,  4.2260e-01, -1.8010e-01, -1.2333e-01,\n",
            "          3.3459e-01,  3.8269e-01,  1.2877e-01, -5.9997e-01,  3.6142e-01,\n",
            "          7.3340e-01, -8.3800e-02, -6.1581e-01,  2.7399e-01,  4.7332e-01,\n",
            "         -2.3019e-01,  9.1053e-02, -1.7173e-01,  6.5530e-02,  7.2396e-01,\n",
            "         -4.0687e-01,  5.4188e-02,  2.0189e-01,  1.6586e-01,  4.8059e-01,\n",
            "          2.5768e-01,  6.3877e-01,  2.2395e-01, -4.9491e-01,  9.7665e-02,\n",
            "         -1.7720e-01,  3.3476e-01, -9.4002e-02,  5.7056e-01, -2.2818e+00,\n",
            "          5.0779e-01,  7.4912e-01,  8.3464e-02, -1.8929e-01, -3.2489e-01,\n",
            "          1.9631e-01,  5.8778e-01,  5.7858e-02,  1.8357e-01,  3.1189e-01,\n",
            "         -1.8636e-01,  6.0572e-01, -1.8085e-01, -1.6643e-01, -3.2822e-02,\n",
            "          6.0856e-01, -1.7997e-01, -5.5016e-01,  4.3850e-01,  2.4782e-01,\n",
            "          1.2136e-01,  4.7106e-02, -4.1999e-01, -3.7317e-01, -4.0006e-01,\n",
            "          6.2764e-01, -2.1968e-02, -2.3734e-01, -4.6332e-02,  5.1343e-02,\n",
            "         -1.3281e-01,  1.8448e-01, -3.3469e+00,  3.6541e-01,  6.9966e-02,\n",
            "          1.3397e-01, -3.3804e-01,  1.4148e-01, -8.6756e-01,  2.1418e-01,\n",
            "          2.7735e-01, -3.6565e-01,  1.1061e-01,  2.0231e-02, -3.6106e-01,\n",
            "         -6.1938e-02,  1.9025e-01, -1.0886e-01, -2.4377e-01,  3.5322e-02,\n",
            "          3.6070e-01,  6.5643e-01, -2.8265e-01,  1.9042e-01,  2.4000e-01,\n",
            "          3.9578e-02,  5.5716e-02,  4.4485e-01,  4.6739e-01, -1.5832e-01,\n",
            "         -2.8762e-01,  7.2973e-02,  5.5086e-01,  1.8251e-01,  3.7567e-01,\n",
            "          1.2726e-01, -2.4463e-01,  1.0264e+00,  1.7965e-01,  2.1782e-01,\n",
            "         -2.4317e-01, -1.8523e-01, -3.3120e-01,  4.0435e-01, -1.5291e-01,\n",
            "          3.5578e-02,  4.7368e-01,  1.0555e-01,  1.7371e-01,  8.2387e-02,\n",
            "         -3.5975e-02,  7.9970e-02,  4.2127e-01,  5.1549e-01,  5.4111e-01,\n",
            "          3.2791e-01, -1.6986e-01, -2.5733e-01,  2.4678e-01,  6.5137e-02,\n",
            "          1.7376e-01,  1.5062e-01,  3.2031e-01,  2.5059e-01, -8.5924e-01,\n",
            "          3.2484e+00,  5.5602e-02,  3.9375e-02,  3.0267e-01,  4.7463e-01,\n",
            "          1.2238e-01, -1.1574e-01, -1.2234e-01, -1.6661e-01,  4.4612e-01,\n",
            "         -1.0928e-01, -1.3582e-01,  4.8685e-01, -7.3563e-04,  9.6622e-02,\n",
            "         -5.9799e-02,  7.4531e-01,  2.9352e-01,  1.2011e-01,  1.7927e-02,\n",
            "          1.6530e-01, -1.3054e-01, -1.3740e-01,  2.0236e-01, -1.2621e+00,\n",
            "          3.2370e-01, -2.2062e-01, -5.8991e-01,  7.7499e-02,  2.1647e-01,\n",
            "         -1.9105e-01,  1.8479e-01, -2.5067e-01,  2.9737e-01,  2.9385e-01,\n",
            "          1.4652e-01,  7.2468e-01, -3.6690e-01,  4.5591e-02, -2.5546e-01,\n",
            "         -1.4013e-01,  3.3290e-01, -8.0904e-02,  7.4600e-01, -2.3328e-01,\n",
            "          1.2464e-01,  1.6406e-01,  3.7556e-02, -2.2617e-01,  5.6850e-01,\n",
            "          1.9440e-01, -5.8911e-01, -3.6059e-01, -6.3356e-01, -1.8189e-01,\n",
            "         -4.8064e-01, -3.7536e-02, -1.9072e-01, -2.5542e-02,  1.6993e-03,\n",
            "         -8.4495e-01,  2.9072e-01, -5.3340e-01, -2.6000e-01, -2.6504e-01,\n",
            "          5.9877e-02,  9.5588e-02, -5.8529e-01, -3.5505e+00,  5.4389e-01,\n",
            "          4.1205e-02,  6.5752e-01,  2.1623e-01,  1.8584e-01, -3.6929e-01,\n",
            "         -1.6202e-02, -3.3726e-02, -4.8221e-02,  3.3283e-01, -1.8058e-02,\n",
            "         -4.7776e-01,  2.4178e-01, -2.4327e-01,  1.7979e-01,  8.1602e-01,\n",
            "         -3.1634e-01, -3.2340e-01,  3.7109e-02, -3.6961e-01,  2.8155e-02,\n",
            "         -7.8493e-02,  6.0349e-02,  7.4578e-02, -2.6444e-02, -1.8211e-01,\n",
            "         -4.8289e-01,  1.6623e-01, -2.9159e-01,  3.2837e-01,  2.1023e-02,\n",
            "          6.2369e-01, -3.6901e-01, -7.6413e-01, -3.1592e+00,  5.6338e-01,\n",
            "         -5.0506e-01, -3.0985e-01, -5.4714e-01, -8.1213e-02,  3.2517e-01,\n",
            "          2.6391e-01, -1.9478e-01,  9.8094e-02,  4.8090e-01,  2.5978e-01,\n",
            "          4.5742e-01, -4.4481e-02,  3.8286e-01,  5.6904e-01,  2.8794e-01,\n",
            "          5.4962e-02, -2.3438e-01,  8.6542e-02, -3.4000e-01, -1.2261e-01,\n",
            "          2.1577e-01, -5.6775e-01,  1.8541e-01,  1.6126e-02, -4.8229e-01,\n",
            "          4.9810e-02, -2.6610e-01,  1.6790e-01,  2.0091e-01, -2.8703e-01,\n",
            "         -2.6945e-01, -6.9774e-02, -5.1287e-01, -4.4756e-01, -9.7352e-02,\n",
            "          5.4638e-01,  3.3255e-01, -1.8180e-01,  6.8315e-02,  6.3446e-01,\n",
            "          1.3417e-01,  2.4456e-01,  5.2019e-01,  1.3533e-02, -1.0988e-01,\n",
            "         -1.4677e-01, -1.1035e-01,  4.9809e-01, -1.1091e-01, -4.4569e-01,\n",
            "          1.1255e+00, -1.7048e-01, -9.3849e-02,  2.3850e-01,  7.6729e-01,\n",
            "         -1.6004e-01,  2.5365e-01,  1.4087e-02,  6.8414e-01,  4.3162e-02,\n",
            "          2.1482e-01,  5.1488e-01,  1.0394e-01, -3.1186e-01,  1.1315e-01,\n",
            "         -2.7829e-01, -3.2310e-01,  1.7712e-01, -4.5948e-01,  5.9810e-01,\n",
            "         -1.1877e-01, -1.0116e+00, -1.8067e-01, -3.8016e-01, -3.6617e-02,\n",
            "          2.1817e-01,  1.7900e-01,  2.7581e-01, -1.0282e-02, -2.7579e-02,\n",
            "          1.0778e-01,  7.8372e-01, -2.4121e-02,  2.6211e-01, -1.5868e-01,\n",
            "          3.2780e-01, -6.7119e-01, -7.4355e-02, -3.0176e-01,  1.4333e-01,\n",
            "          2.2118e-01,  2.6733e-01,  9.1554e-02, -5.0159e-01,  9.0470e-01,\n",
            "         -8.8047e-01,  1.7599e-01, -6.7364e-01, -4.7320e-02,  1.9097e-01,\n",
            "          3.8012e-01, -1.9674e-01, -4.1830e-01, -1.2687e-01, -4.9604e-01,\n",
            "          1.5975e-01,  6.5381e-02, -5.6910e-02, -3.2745e-01,  4.4249e-02,\n",
            "         -6.6524e-03, -1.1228e-01,  7.7341e-01, -2.5852e-01, -1.5956e-02,\n",
            "          5.4944e-01,  6.6526e-02,  5.9755e-02,  1.1668e-02, -1.2251e-01,\n",
            "          9.4067e-02, -5.7606e-02, -4.2434e-01,  8.5567e-02, -7.3278e-02,\n",
            "         -4.5703e-01,  2.7361e-01,  1.7445e-02, -4.2470e-01,  2.6892e-02,\n",
            "         -4.5342e-01, -4.1612e-01, -1.3517e-01, -2.7911e-01,  1.3589e-01,\n",
            "         -1.0697e-01,  2.0055e-01, -1.4004e-01,  6.6527e-02, -2.2713e-01,\n",
            "         -2.9649e-01,  1.1272e-01, -1.6660e-01,  3.3085e-01,  3.2366e-01,\n",
            "          7.7440e-03, -3.2233e-01,  3.8589e-01,  1.9735e-02, -2.5762e-02,\n",
            "          3.6447e-01, -3.5853e-01,  1.0338e-01,  2.4755e-01, -3.8925e-01,\n",
            "          2.2933e-01, -4.4436e-01, -7.7713e-02, -9.9624e-02,  5.5032e-01,\n",
            "         -1.4823e+00,  2.6861e-01,  7.3583e-01,  1.0746e-01,  7.1074e-02,\n",
            "         -7.7805e-01, -2.9899e-01,  4.0444e-01,  1.3616e-01,  2.9760e-01,\n",
            "         -2.3679e-01, -3.9214e-01,  3.1386e-01,  1.0632e-01,  4.1001e-01,\n",
            "         -2.5379e-02, -1.7708e-01,  1.6863e-01, -1.4833e-01,  2.1531e-02,\n",
            "         -1.8397e-01,  8.9822e-01, -3.3740e-01,  4.7920e-02, -4.1798e-02,\n",
            "         -8.8456e-02, -2.6450e-01,  3.8546e-01, -1.6465e-02,  7.4225e-02,\n",
            "         -7.1247e-02, -2.5230e-01, -5.9883e-01,  3.6505e-01,  1.0679e-02,\n",
            "          1.6914e-01, -7.4113e-02, -3.3363e-01,  4.4915e-01,  5.6120e-01,\n",
            "         -6.0455e-01,  5.1678e-01, -1.0959e-01,  2.3805e-01,  1.4447e-01,\n",
            "          4.7366e-02, -3.2012e-01,  2.1897e-01,  3.1158e-01,  9.6873e-02,\n",
            "          7.9475e-02,  5.0891e-02, -3.4329e-01, -8.1689e-02, -6.6519e-02,\n",
            "          4.3819e-01, -1.5038e-01,  2.3683e-01, -1.7379e-01, -1.2388e-01,\n",
            "          6.3837e-01, -2.7722e-01,  5.4084e-02,  1.2244e-01, -4.3553e-01,\n",
            "         -8.9895e-01,  5.2682e-01, -1.6476e-01,  3.2194e-01,  4.6177e-01,\n",
            "          6.0910e-03, -2.2548e-01, -6.7713e-01,  4.0503e-01,  1.6557e-02,\n",
            "         -1.4868e-01,  1.8504e-01, -1.0513e-01,  1.8690e-01, -3.0359e-01,\n",
            "          6.6307e-01, -5.3184e-01, -5.1019e-01,  1.6428e-01, -6.6434e-01,\n",
            "          4.6351e-01, -2.1993e-01, -1.1022e-01,  5.0656e-02, -1.3383e-01,\n",
            "         -2.3886e-01, -6.9021e-01,  3.9816e-01,  2.6473e-01,  1.1513e-01,\n",
            "          5.2715e-01, -1.7283e-01, -1.7293e-01,  1.1256e-01,  2.1467e-01,\n",
            "         -2.5120e-01,  7.1889e-01,  6.3878e-02,  2.1529e-01,  3.0709e-01,\n",
            "         -2.2834e-01,  4.1752e-01, -3.4906e-01, -3.7665e-01,  3.5984e-01,\n",
            "         -2.3980e-01,  4.5121e-01, -1.8621e-01, -6.9834e-02,  4.8936e-02,\n",
            "         -1.2517e-01,  4.6962e-02, -2.8874e-01,  2.1120e+00,  3.4863e-01,\n",
            "          2.6537e-01,  4.6112e-02,  6.7544e-02,  2.0978e-01,  2.7995e-02,\n",
            "          7.0807e-02, -1.0116e-01,  4.3956e-01, -2.7925e-01, -3.5648e-01,\n",
            "         -3.5124e-02, -3.1364e-01,  6.6008e-01,  7.6586e-02,  9.5816e-02,\n",
            "         -6.1964e-01, -2.0400e-01,  9.9307e-02, -4.2274e-01,  4.0601e-01,\n",
            "          6.7325e-01,  1.6169e-01,  3.6210e-01,  2.5471e-01, -3.8002e-01,\n",
            "         -3.6373e-01, -2.7154e-01,  2.6823e-01, -3.8551e-01, -4.8510e-01,\n",
            "          4.5209e-01,  8.7964e-01, -2.8353e-02,  1.6354e-02, -2.6274e-01,\n",
            "         -4.1546e-01, -1.6931e-02, -3.0210e-01, -2.8675e-01,  1.1577e-01,\n",
            "          5.9925e-01, -3.7211e-01, -2.7350e-01,  1.2775e-02, -1.5930e-01,\n",
            "         -2.6136e-01,  3.1032e-01, -5.4627e-01,  8.8343e-02, -2.8656e-01,\n",
            "         -6.8788e-03,  2.8493e-01, -2.5059e-01,  5.9433e-02, -3.8549e-01,\n",
            "         -5.6644e-01,  6.2366e-02,  1.4930e-01, -2.6111e-01,  1.1849e-01,\n",
            "          5.7001e-01, -4.3349e-01,  1.6195e-02, -2.7858e-01, -4.8976e-01,\n",
            "          2.9991e-01, -1.8321e-01, -2.5330e-02, -7.3854e-02,  4.6711e-01,\n",
            "          4.6780e-01,  5.9563e-02, -6.0921e-02,  6.4540e-01,  5.4018e-01,\n",
            "         -9.3723e-02,  5.5560e-02, -2.7027e+00,  1.1593e-01,  6.0438e-01,\n",
            "          3.4213e-01,  4.7427e-02,  1.2593e-01,  1.7241e-01,  2.9735e-01,\n",
            "          6.6792e-02, -1.0675e-01,  1.5511e-01,  3.7448e-01,  5.3976e-02,\n",
            "         -2.8699e-01, -2.2683e-01, -4.4152e-02,  8.1257e-01,  1.3147e-01,\n",
            "         -6.9813e-01,  1.5424e-01,  3.0725e-01, -3.0814e-01, -4.1348e-01,\n",
            "         -2.1274e-01, -4.2065e-01,  5.4279e-01,  8.2522e-02, -3.1287e-01,\n",
            "          7.4620e-02,  5.1440e-01, -3.5708e-01,  3.8472e-01, -2.5619e-01,\n",
            "          6.8657e-01,  3.5761e-01, -4.1594e-01, -3.4392e-01,  1.9921e-02,\n",
            "          3.8177e-02, -2.2119e-02, -5.2911e-01,  3.6846e-01, -2.1510e-01,\n",
            "         -2.5164e-01,  4.2313e-01, -6.3036e-02,  6.0341e-01, -3.8588e-01,\n",
            "          9.1752e-01, -4.1326e-01, -2.8810e-01, -2.6920e-01,  4.0047e-01,\n",
            "         -2.8172e-01,  7.4606e-03, -2.6643e-01, -1.7455e-01, -2.3758e-02,\n",
            "          1.6540e-02, -3.2303e-01, -4.3318e-02, -1.5793e-01,  1.3008e-01,\n",
            "          8.7436e-02,  3.6625e-01, -8.1577e-04, -1.2916e-01,  1.3692e-03,\n",
            "         -3.7778e-01, -3.2104e-01, -5.0839e-02, -2.8816e-01, -2.8673e-02,\n",
            "         -1.7688e-01,  3.8400e-01, -3.4909e-01,  4.4282e-01,  5.6042e-01,\n",
            "          6.7600e-01,  1.1907e-01,  5.9152e-03, -4.3979e-01,  2.4983e-01,\n",
            "          3.3093e-01,  9.0346e-02, -7.0752e+00, -4.9647e-01, -6.1260e-02,\n",
            "         -4.0169e-01, -1.7965e-01, -1.1380e-01, -2.6891e-01, -6.2826e-01,\n",
            "         -3.9006e-01, -1.4199e-01,  4.6716e-01,  2.7692e-01, -2.2780e-01,\n",
            "         -2.1638e-01,  5.5597e-02, -5.1339e-02]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: take quary then emedding it\n",
        "\n",
        "query = \"knn?\"  #@param {type:\"string\"}\n",
        "\n",
        "# Embed the query\n",
        "query_embedding = embed_text(query)\n",
        "print(query_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9GpUr8-ooo7",
        "outputId": "8cdc1d6d-3f09-4407-95a3-8b25afeea79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-5.6151e-01, -3.2058e-01, -1.2626e-01, -2.6654e-01, -4.0018e-01,\n",
            "          3.3182e-01,  2.3096e-01,  1.3104e-01, -5.0104e-01,  1.7477e-01,\n",
            "         -1.7215e-01,  5.0396e-02, -1.5489e-01,  6.8974e-01,  3.7790e-01,\n",
            "         -2.2389e-02, -2.8357e-01,  5.7076e-01,  1.9262e-01, -3.1256e-01,\n",
            "          1.1833e-01, -2.2326e-01, -4.3730e-01, -5.3822e-01,  9.4787e-02,\n",
            "          7.1371e-02, -4.2946e-02, -4.7955e-01, -1.5128e-01,  3.4595e-02,\n",
            "         -2.1352e-01,  4.6899e-01, -6.3208e-01, -1.3796e-01, -4.1321e-01,\n",
            "         -8.9864e-02, -5.7842e-02, -9.2536e-02,  7.7594e-02,  3.7511e-01,\n",
            "         -2.4966e-01, -1.7581e-01,  3.0140e-01,  9.0716e-02, -4.5079e-01,\n",
            "         -5.8414e-01, -2.7419e+00, -1.3977e-01, -5.5407e-01, -4.4342e-01,\n",
            "         -9.6318e-02,  2.2290e-01,  2.1571e-01,  5.5755e-01, -2.1557e-01,\n",
            "          7.9121e-01, -2.3667e-01,  8.9659e-02,  2.3776e-01, -4.8213e-01,\n",
            "          2.6376e-01, -3.9789e-02,  1.3783e-01, -2.4949e-02, -9.9624e-02,\n",
            "          3.5074e-01, -1.0864e-01,  5.3463e-01, -4.5571e-01,  3.9290e-01,\n",
            "         -5.0898e-01,  1.4877e-01,  5.1078e-01,  3.0811e-02,  1.1645e-01,\n",
            "         -2.7071e-01, -2.6619e-01,  3.8896e-01, -2.4306e-01, -2.6163e-01,\n",
            "         -9.6432e-02,  2.8722e-01,  6.1216e-02,  9.3730e-03,  3.8254e-01,\n",
            "          6.4143e-01, -4.2246e-01, -5.0722e-01,  2.0732e-01,  7.6619e-01,\n",
            "          8.9706e-02,  4.7929e-02,  9.8003e-02,  5.4934e-01,  4.8363e-01,\n",
            "          1.6270e-01,  8.5451e-02,  1.3549e-01, -1.3089e-01,  3.6351e-01,\n",
            "          6.1132e-02, -7.0507e-02,  7.4319e-02, -2.3504e-01, -3.2270e-01,\n",
            "          1.8638e-01, -2.6794e-01,  9.3085e-02, -3.5424e-02, -2.0816e+00,\n",
            "          6.2373e-02,  1.7141e-01,  1.6042e-01, -2.6053e-01,  2.1987e-01,\n",
            "          3.1100e-01,  3.9729e-01, -4.3565e-01, -1.6360e-01,  4.3567e-01,\n",
            "          1.1770e-01,  2.2503e-01, -8.2920e-02,  9.5190e-02,  3.9349e-01,\n",
            "          6.5557e-01,  3.3573e-01, -3.7731e-01,  2.6756e-01,  5.5217e-01,\n",
            "          9.1353e-02,  8.3844e-01,  2.1323e-01, -4.3335e-01,  1.3223e-02,\n",
            "         -1.9413e-01,  3.5653e-01,  1.7318e-01, -5.6724e-04,  9.6101e-02,\n",
            "         -8.3521e-01, -4.4841e-01, -3.0125e+00,  1.6207e-01,  7.8966e-01,\n",
            "         -2.8847e-02, -4.1210e-01, -1.6857e-01,  5.3717e-01,  3.0147e-01,\n",
            "         -1.2053e-01, -1.0773e-01, -2.3894e-02, -1.8766e-01, -2.9094e-01,\n",
            "          7.0212e-02, -4.8383e-01, -6.7146e-02,  2.0147e-01,  6.2113e-01,\n",
            "          1.0188e-01, -1.3844e-01, -1.3653e-01, -2.5874e-01, -3.8861e-01,\n",
            "         -3.0227e-01, -7.1524e-02,  4.4596e-01, -1.8685e-01, -5.4292e-03,\n",
            "         -1.5435e-01, -3.6491e-01,  5.4590e-01,  3.8516e-01,  2.0904e-01,\n",
            "         -4.6923e-01,  1.3360e-01,  3.5315e-01,  1.8174e-01, -9.1525e-02,\n",
            "         -1.0409e-01,  5.5175e-01,  7.9102e-02, -6.8740e-02,  1.8673e-02,\n",
            "          1.8652e-01,  6.2637e-01, -4.2248e-01, -1.4360e-01,  1.3235e-01,\n",
            "         -6.0788e-01,  2.9386e-02,  2.4942e-01,  7.8572e-02,  3.3917e-01,\n",
            "         -1.8324e-01,  2.1851e-01, -6.6888e-01,  3.9605e-01,  4.9372e-01,\n",
            "          2.9708e-01,  1.4583e-01,  5.6938e-02,  7.9583e-02,  1.0970e-01,\n",
            "          4.1265e+00,  3.8609e-01, -2.5173e-01, -9.0475e-03,  4.7903e-02,\n",
            "         -1.9460e-01,  6.2408e-01, -1.1340e-01, -1.1718e-01, -4.2469e-01,\n",
            "         -3.9907e-01,  8.8199e-01,  1.2847e-01,  6.9828e-03, -2.5709e-01,\n",
            "          5.2405e-01,  5.6502e-02,  1.4748e-01,  1.4220e-01, -6.2087e-01,\n",
            "          5.1212e-01,  6.7900e-02,  4.7983e-01, -7.9515e-02, -1.2567e+00,\n",
            "         -1.1745e-01,  2.7472e-01, -2.6249e-01,  3.8242e-01, -4.1842e-01,\n",
            "          4.0563e-02, -3.4617e-03, -3.7926e-01,  1.6614e-01, -2.3078e-01,\n",
            "          8.0786e-02,  3.7953e-01,  1.8488e-01,  5.7577e-03, -4.5693e-01,\n",
            "          1.1444e-01,  6.5614e-01, -3.5326e-01,  4.1606e-01, -5.8649e-02,\n",
            "          2.5909e-01,  2.0189e-01, -4.4874e-01, -2.7303e-01, -2.4273e-01,\n",
            "         -1.1812e-01, -1.9185e-01,  4.9653e-01, -4.0240e-01, -3.1244e-01,\n",
            "          1.5346e-02, -1.8613e-01,  6.7140e-01, -4.7073e-03, -2.2697e-01,\n",
            "         -4.4573e-01,  2.1859e-02, -5.1429e-02, -2.0276e-01, -2.8130e-01,\n",
            "          5.4028e-02,  7.5007e-02,  4.3229e-02, -3.3419e+00,  1.8309e-01,\n",
            "          3.8282e-01,  3.2928e-01, -1.5077e-01, -6.4448e-01,  3.4089e-03,\n",
            "          2.4349e-01,  1.9778e-01, -5.4208e-01,  1.4922e-01,  6.1602e-01,\n",
            "         -2.0480e-02,  4.6897e-01, -2.7791e-01,  6.5115e-01, -2.5108e-02,\n",
            "         -3.5829e-01, -2.5256e-01, -5.2474e-01, -2.8476e-01,  4.0026e-01,\n",
            "         -5.6354e-01, -3.6001e-01,  5.1011e-01, -3.1799e-02, -4.3420e-02,\n",
            "          6.7572e-02,  2.2055e-01, -5.7861e-01,  1.9902e-03, -2.6692e-01,\n",
            "          2.0264e-01, -1.1841e-01, -5.1951e-02, -3.1722e+00, -4.9894e-03,\n",
            "         -1.4607e-01, -3.7302e-01,  2.7863e-01,  7.9381e-02,  4.2227e-01,\n",
            "          7.1250e-02, -2.5637e-01,  1.8465e-02,  4.8520e-01, -3.5465e-01,\n",
            "          1.0855e-01,  4.3627e-01, -2.0241e-01,  1.2378e-01,  3.6984e-01,\n",
            "          3.6938e-01,  1.6029e-01, -2.6999e-01,  5.0088e-02, -5.1637e-02,\n",
            "          3.3807e-01,  6.4487e-02,  2.4778e-01,  1.1370e+00, -6.0882e-02,\n",
            "         -5.3772e-01, -1.9968e-02, -2.9312e-01, -8.9816e-02, -5.3951e-01,\n",
            "         -4.9300e-02,  4.3369e-01, -3.3465e-01, -1.4090e-01,  5.7696e-01,\n",
            "          3.4496e-02,  2.9868e-01,  1.6069e-01,  1.3736e-01,  4.6792e-01,\n",
            "          1.9891e-01,  4.6263e-01,  6.7660e-01, -1.6089e-02, -1.3846e-01,\n",
            "         -1.6642e-01,  6.3883e-02, -6.5090e-01, -6.5721e-01,  9.4054e-02,\n",
            "          1.3651e+00,  1.2186e-01,  6.3953e-01,  1.1080e-01,  3.5480e-01,\n",
            "          6.3136e-02,  4.0310e-01, -3.5626e-02,  1.4088e+00,  2.1943e-02,\n",
            "         -1.7526e-01, -1.9597e-01,  9.9514e-03, -7.0905e-01,  5.0498e-02,\n",
            "         -2.7830e-01,  2.1390e-01, -2.7213e-01, -5.5156e-01,  3.6792e-02,\n",
            "          3.1364e-01, -1.0007e+00, -1.7702e-01,  3.4899e-01, -8.2718e-02,\n",
            "          3.5934e-02,  7.4894e-03, -3.5104e-01, -4.4063e-01,  1.7130e-01,\n",
            "          3.2913e-01,  5.9244e-01, -2.0895e-01, -9.5080e-02, -1.1536e-01,\n",
            "          2.0223e-01, -3.9453e-01,  4.0047e-01, -4.0062e-01,  4.7593e-01,\n",
            "          1.0444e-01,  2.3441e-01, -2.4781e-01,  4.4358e-01,  4.1164e-01,\n",
            "         -9.2204e-01,  3.1070e-01, -1.8456e-01, -4.6487e-01, -3.6691e-01,\n",
            "         -2.2214e-01,  4.2967e-02, -5.7353e-01, -5.1477e-02, -6.8543e-01,\n",
            "         -1.8808e-01,  1.0703e-01,  6.9607e-02, -1.1574e-01, -1.0892e-01,\n",
            "         -3.4936e-01,  3.5777e-01,  7.0305e-01, -5.8761e-02, -1.9097e-01,\n",
            "          6.1781e-01, -4.5729e-02,  4.3847e-01, -1.4617e-01, -5.2655e-03,\n",
            "          1.6794e-01, -4.4806e-02,  2.1930e-01, -1.7007e-02,  4.7126e-01,\n",
            "         -9.1108e-02, -4.5993e-01, -3.9638e-01, -2.6534e-01, -6.1594e-01,\n",
            "         -3.4465e-01, -4.8732e-01, -1.4152e-01,  1.0548e-01, -4.6758e-01,\n",
            "          4.5868e-01,  3.1392e-01,  1.1186e-01,  4.4405e-01, -2.8393e-01,\n",
            "         -2.2288e-01,  7.3557e-01, -2.6699e-02,  5.1319e-01,  9.4445e-02,\n",
            "         -6.9762e-04, -3.1043e-02,  9.3386e-01,  1.4142e-01, -8.6754e-01,\n",
            "          6.2830e-01, -6.3182e-02,  1.5049e-02, -3.3617e-01,  8.2280e-02,\n",
            "         -2.3542e-01, -2.8205e-01,  3.2943e-01,  5.0491e-01,  1.6535e-01,\n",
            "         -1.3098e+00,  6.3610e-01,  1.9996e-01, -2.1654e-01,  5.0883e-01,\n",
            "          1.0570e-01, -4.9496e-01,  5.3092e-01, -8.9875e-02, -9.9856e-03,\n",
            "         -3.4689e-01, -1.2236e-02,  3.5526e-02,  2.6927e-01,  1.4492e-01,\n",
            "          8.8914e-02,  1.6468e-01, -3.2651e-01,  6.5758e-02,  1.6966e-01,\n",
            "         -9.8482e-02,  2.1579e-01,  7.7242e-02, -3.9706e-01,  2.5351e-01,\n",
            "         -1.5836e-01,  1.5235e-01,  1.9513e-01,  3.3876e-02, -1.3556e-01,\n",
            "          4.5621e-01, -5.8790e-01, -3.7673e-01, -3.2557e-01,  4.4647e-01,\n",
            "          2.9179e-01,  3.3065e-01,  3.1229e-01,  6.2103e-01,  4.1210e-01,\n",
            "          2.6559e-01,  3.1868e-03, -1.9162e-01, -2.5104e-01,  2.3526e-01,\n",
            "          4.0385e-01, -2.2053e-01,  2.8954e-01,  3.5157e-01, -3.4590e-01,\n",
            "         -4.8121e-02,  4.3432e-01, -1.9261e-02,  2.6385e-01, -9.3625e-03,\n",
            "         -1.1088e-01,  5.3643e-01,  4.1123e-02, -5.9925e-01, -3.0211e-01,\n",
            "          8.1326e-02, -1.0789e-01, -3.7795e-01,  8.3687e-02, -3.7004e-01,\n",
            "         -4.0158e-01,  4.5957e-01, -1.3250e-01,  2.1509e-03,  3.9097e-02,\n",
            "          1.8115e-01,  9.7399e-02, -2.8338e-01,  1.1869e-01, -5.2537e-01,\n",
            "          2.8517e-01, -9.7789e-02,  6.0798e-02,  3.9123e-01, -6.6898e-01,\n",
            "          3.5246e-01, -6.1930e-01, -5.5777e-01,  6.1452e-01, -2.2156e-01,\n",
            "         -2.0568e-01,  4.1644e-01,  1.0125e-01, -7.8274e-02,  5.0432e-02,\n",
            "         -8.3738e-01, -2.0577e-01,  2.5258e-01, -2.8571e-01,  5.0317e-01,\n",
            "         -2.3774e-01,  1.8828e-01, -1.9650e-01, -3.3316e-01, -1.6421e-03,\n",
            "          2.5575e-01,  3.1397e-01,  8.0995e-02,  1.1112e-02,  2.5667e-01,\n",
            "          1.3036e-01,  5.8711e-01,  2.6201e-01, -6.3808e-01, -3.0740e-01,\n",
            "          3.7819e-01,  2.1268e-01, -1.7473e-01, -3.6585e-01, -2.0224e-02,\n",
            "         -1.2741e-01, -1.6511e-01, -4.9262e-01,  2.0508e+00,  7.6136e-01,\n",
            "          1.3272e-02,  4.5417e-01,  6.5888e-01,  2.3082e-01, -2.5755e-01,\n",
            "         -5.0485e-01,  2.2517e-02,  3.3868e-01, -5.4067e-01,  5.0356e-02,\n",
            "         -2.2661e-01,  6.8568e-01,  1.9020e-01,  2.4426e-01, -2.8326e-01,\n",
            "         -6.1603e-01, -5.6667e-01,  1.8961e-01, -2.6690e-01, -6.8379e-02,\n",
            "         -4.9340e-02,  1.6450e-02,  1.9390e-01,  3.4853e-01, -3.0310e-01,\n",
            "         -4.9502e-01, -4.5743e-02, -7.2717e-02, -3.4949e-01,  2.3433e-01,\n",
            "         -1.0259e-01, -2.3697e-02,  3.0982e-02,  6.6481e-02, -2.9303e-02,\n",
            "         -7.8852e-01, -7.2020e-01, -3.0364e-01,  3.5196e-02,  1.5410e-01,\n",
            "          7.0760e-01,  5.5817e-01,  6.8882e-01,  2.8598e-01,  2.0728e-01,\n",
            "         -4.0278e-01,  2.8054e-01,  2.3659e-01, -7.8001e-02, -2.1400e-01,\n",
            "         -5.7912e-01,  7.4849e-02, -5.9331e-01, -4.5823e-01,  1.0500e-01,\n",
            "         -1.2565e-01, -2.6780e-01,  3.9150e-01, -2.4827e-01,  5.5962e-01,\n",
            "          4.6685e-01, -9.4180e-01, -1.6965e-01,  6.3654e-02,  1.6112e-01,\n",
            "         -1.9306e-01,  8.4100e-02, -2.2519e-01,  3.1610e-01,  3.9857e-01,\n",
            "          3.6328e-01,  1.1258e-01,  4.1813e-01, -9.8192e-02, -3.9787e-01,\n",
            "         -2.0732e-01,  2.0560e-01, -2.7156e+00,  1.7047e-01, -1.2437e-01,\n",
            "          6.5807e-01,  1.3455e-02,  2.7398e-02, -1.8638e-01, -2.4376e-01,\n",
            "          1.1443e-01, -5.3671e-02,  1.8314e-01,  4.8584e-01,  7.4932e-02,\n",
            "          7.9318e-02, -5.0058e-02,  1.1753e-01,  5.2260e-01, -1.7135e-01,\n",
            "         -3.7261e-01, -4.1592e-01, -5.4961e-02, -1.8168e-01, -1.4106e-01,\n",
            "         -3.2177e-01, -4.1313e-01,  6.0747e-01, -8.0926e-02, -2.5872e-01,\n",
            "          5.2614e-01,  1.6472e-01, -1.1174e-01,  8.8342e-01, -9.8058e-02,\n",
            "          2.6543e-01,  1.3860e-01, -6.7327e-02,  4.9814e-02,  1.7479e-01,\n",
            "         -6.6563e-02,  2.6595e-01,  1.3393e-01,  5.5140e-01,  5.1871e-01,\n",
            "          1.3962e-01, -4.2824e-01, -1.2924e-01,  1.1642e-01, -3.6984e-01,\n",
            "          5.9681e-01, -3.2006e-01, -1.4310e-01, -4.7576e-02, -3.1400e-01,\n",
            "         -3.2158e-03,  6.3473e-01,  7.1052e-02, -5.6371e-02,  1.7529e-01,\n",
            "          1.4862e-01, -5.3984e-02, -2.9379e-02,  2.0421e-02, -2.9763e-01,\n",
            "         -2.2265e-01, -1.9157e-01, -2.9250e-01,  4.3747e-01,  4.5813e-02,\n",
            "         -2.2039e-01,  4.2328e-02, -1.6585e-01, -2.8073e-01,  1.0167e+00,\n",
            "          1.9157e-01, -8.9004e-02,  3.3261e-01,  3.3411e-01,  2.5007e-01,\n",
            "         -1.2860e-02, -1.3583e-01, -4.0414e-01, -5.1644e-02, -1.7302e-01,\n",
            "          3.3714e-01,  9.3343e-02, -6.8412e+00, -3.5039e-01, -4.6243e-01,\n",
            "         -5.7408e-01,  3.3361e-01, -7.8582e-01,  5.8671e-02, -2.2779e-02,\n",
            "         -1.4671e-02, -3.2646e-01, -3.9188e-02, -1.1055e-01, -2.4131e-01,\n",
            "         -6.0955e-01,  3.6106e-01,  6.3634e-01]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calculate similarty\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate cosine similarity between query embedding and document embeddings\n",
        "similarities = []\n",
        "for chunk in chunks:\n",
        "  chunk_embedding = embed_text(chunk)\n",
        "  similarity = cosine_similarity(query_embedding.detach().numpy(), chunk_embedding.detach().numpy())\n",
        "  similarities.append(similarity[0][0])\n",
        "\n",
        "# Find the most similar chunk\n",
        "most_similar_index = similarities.index(max(similarities))\n",
        "most_similar_chunk = chunks[most_similar_index]\n",
        "\n",
        "print(f\"Most similar chunk: {most_similar_chunk}\")\n",
        "print(f\"Similarity score: {max(similarities)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOWbDc4ho3Xp",
        "outputId": "fd69d54c-d276-4b30-eb4f-66521a4b51cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar chunk: g Pretrained Models From Keras                                                                      465\n",
            "Pretrained Models for Transfer Learning                                                                 467\n",
            "Classification and Localization                                                                                  469\n",
            "Object Detection                                                                                                          471\n",
            "Fully Convolutional Networks FCNs                                                                 473\n",
            "Y ou Only Look Once YOLO                                                                                475\n",
            "Semantic Segmentation                                                                                               478\n",
            "Exercises                                                                                                                        482\n",
            "x  Table of Contents1Available on Hintons home page at httpwwwcstorontoeduhinton \n",
            "\n",
            "Similarity score: 0.7802160978317261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHTeWczesB32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}